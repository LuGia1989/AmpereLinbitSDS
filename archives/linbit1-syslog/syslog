Mar 12 00:00:02 linbit1 systemd[1]: logrotate.service: Deactivated successfully.
Mar 12 00:00:02 linbit1 systemd[1]: Finished Rotate log files.
Mar 12 00:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:00:40.334623    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:00:40.344822    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:03:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.RHTsXi.mount: Deactivated successfully.
Mar 12 00:03:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.fGleRV.mount: Deactivated successfully.
Mar 12 00:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:05:40.334719    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:05:40.345293    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:10:02 linbit1 rsyslogd: [origin software="rsyslogd" swVersion="8.2112.0" x-pid="2357" x-info="https://www.rsyslog.com"] rsyslogd was HUPed
Mar 12 00:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:10:40.334726    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:10:40.345472    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:15:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.wAOgNz.mount: Deactivated successfully.
Mar 12 00:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:15:40.335121    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:15:40.345410    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:17:01 linbit1 CRON[696969]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 00:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:20:40.333960    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:20:40.345472    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:23:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.W89JWu.mount: Deactivated successfully.
Mar 12 00:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:25:40.334784    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:25:40.345781    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:30:40.334659    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:30:40.344917    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:32:12 linbit1 systemd[1]: Starting Download data for packages that failed at package install time...
Mar 12 00:32:12 linbit1 systemd[1]: update-notifier-download.service: Deactivated successfully.
Mar 12 00:32:12 linbit1 systemd[1]: Finished Download data for packages that failed at package install time.
Mar 12 00:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:35:40.335612    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:35:40.345089    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:38:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.IumDBl.mount: Deactivated successfully.
Mar 12 00:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:40:40.334842    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:40:40.345491    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:40:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Zf3qgg.mount: Deactivated successfully.
Mar 12 00:41:56 linbit1 systemd[1]: Starting Cleanup of Temporary Directories...
Mar 12 00:41:56 linbit1 systemd[1]: systemd-tmpfiles-clean.service: Deactivated successfully.
Mar 12 00:41:56 linbit1 systemd[1]: Finished Cleanup of Temporary Directories.
Mar 12 00:45:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.jrxjQx.mount: Deactivated successfully.
Mar 12 00:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:45:40.335179    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:45:40.347981    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:45:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MPlwBW.mount: Deactivated successfully.
Mar 12 00:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:50:40.335175    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:50:40.345558    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 00:52:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.xuKVzS.mount: Deactivated successfully.
Mar 12 00:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 00:55:40.335517    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 00:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 00:55:40.345702    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:00:40.335666    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:00:40.346677    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:01:01 linbit1 systemd[1]: Starting Ubuntu Advantage Timer for running repeated jobs...
Mar 12 01:01:02 linbit1 systemd[1]: ua-timer.service: Deactivated successfully.
Mar 12 01:01:02 linbit1 systemd[1]: Finished Ubuntu Advantage Timer for running repeated jobs.
Mar 12 01:01:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.36Lcxm.mount: Deactivated successfully.
Mar 12 01:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:05:40.335034    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:05:40.345085    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:08:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.PfH6z9.mount: Deactivated successfully.
Mar 12 01:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:10:40.334972    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:10:40.344559    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:15:40.335004    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:15:40.345293    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:17:01 linbit1 CRON[736448]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 01:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:20:40.335393    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:20:40.346436    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:21:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FxEUkr.mount: Deactivated successfully.
Mar 12 01:24:37 linbit1 snapd[2460]: storehelpers.go:769: cannot refresh: snap has no updates available: "core18", "core20", "lxd", "microk8s", "snapd"
Mar 12 01:24:37 linbit1 snapd[2460]: autorefresh.go:551: auto-refresh: all snaps are up-to-date
Mar 12 01:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:25:40.334847    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:25:40.347398    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:30:40.335297    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:30:40.346573    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:35:40.335229    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:35:40.345527    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:37:31 linbit1 systemd[1]: Starting Daily apt download activities...
Mar 12 01:38:01 linbit1 systemd-networkd-wait-online[749943]: Timeout occurred while waiting for network connectivity.
Mar 12 01:38:01 linbit1 apt-helper[749941]: E: Sub-process /lib/systemd/systemd-networkd-wait-online returned an error code (1)
Mar 12 01:38:10 linbit1 systemd[1]: apt-daily.service: Deactivated successfully.
Mar 12 01:38:10 linbit1 systemd[1]: Finished Daily apt download activities.
Mar 12 01:38:10 linbit1 systemd[1]: apt-daily.service: Consumed 8.127s CPU time.
Mar 12 01:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:40:40.335289    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:40:40.345753    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:41:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5F9EyI.mount: Deactivated successfully.
Mar 12 01:42:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.43gASO.mount: Deactivated successfully.
Mar 12 01:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:45:40.335683    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:45:40.347108    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:45:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.BEdtQg.mount: Deactivated successfully.
Mar 12 01:45:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UhfHCz.mount: Deactivated successfully.
Mar 12 01:50:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.nREr3l.mount: Deactivated successfully.
Mar 12 01:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:50:40.335416    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:50:40.348892    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:51:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XTpS94.mount: Deactivated successfully.
Mar 12 01:51:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.AyJTSV.mount: Deactivated successfully.
Mar 12 01:51:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JorUro.mount: Deactivated successfully.
Mar 12 01:52:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.AUWjNZ.mount: Deactivated successfully.
Mar 12 01:52:50 linbit1 systemd[1]: Starting Message of the Day...
Mar 12 01:52:52 linbit1 50-motd-news[760414]:  * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
Mar 12 01:52:52 linbit1 50-motd-news[760414]:    just raised the bar for easy, resilient and secure K8s cluster deployment.
Mar 12 01:52:52 linbit1 50-motd-news[760414]:    https://ubuntu.com/engage/secure-kubernetes-at-the-edge
Mar 12 01:52:52 linbit1 systemd[1]: motd-news.service: Deactivated successfully.
Mar 12 01:52:52 linbit1 systemd[1]: Finished Message of the Day.
Mar 12 01:53:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.gPLEjS.mount: Deactivated successfully.
Mar 12 01:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 01:55:40.334647    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 01:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 01:55:40.346466    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 01:58:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.cpDPUH.mount: Deactivated successfully.
Mar 12 02:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:00:40.334731    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:00:40.345173    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:05:40.334936    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:05:40.346749    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:10:40.334702    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:10:40.345659    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:15:40.335042    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:15:40.345048    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:15:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.qyq0vK.mount: Deactivated successfully.
Mar 12 02:17:01 linbit1 CRON[776221]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 02:18:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.kwPtOm.mount: Deactivated successfully.
Mar 12 02:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:20:40.334479    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:20:40.348172    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:21:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.E3QO5E.mount: Deactivated successfully.
Mar 12 02:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:25:40.335165    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:25:40.347589    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:27:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5Kyr1G.mount: Deactivated successfully.
Mar 12 02:27:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.4Sa1W7.mount: Deactivated successfully.
Mar 12 02:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:30:40.335088    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:30:40.346007    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:35:40.334973    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:35:40.344642    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:37:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VnL48b.mount: Deactivated successfully.
Mar 12 02:37:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LCXMHq.mount: Deactivated successfully.
Mar 12 02:38:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6AhyjF.mount: Deactivated successfully.
Mar 12 02:39:30 linbit1 systemd[1]: Starting Refresh fwupd metadata and update motd...
Mar 12 02:39:30 linbit1 systemd[1]: fwupd-refresh.service: Main process exited, code=exited, status=1/FAILURE
Mar 12 02:39:30 linbit1 systemd[1]: fwupd-refresh.service: Failed with result 'exit-code'.
Mar 12 02:39:30 linbit1 systemd[1]: Failed to start Refresh fwupd metadata and update motd.
Mar 12 02:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:40:40.334759    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:40:40.344682    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:41:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LBedqN.mount: Deactivated successfully.
Mar 12 02:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:45:40.335406    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:45:40.346242    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:46:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.N8cwwG.mount: Deactivated successfully.
Mar 12 02:48:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.wBBxKD.mount: Deactivated successfully.
Mar 12 02:50:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.P3O0US.mount: Deactivated successfully.
Mar 12 02:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:50:40.334569    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:50:40.345493    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 02:55:40.335109    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 02:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 02:55:40.344703    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 02:55:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.vot5Rx.mount: Deactivated successfully.
Mar 12 03:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:00:40.334573    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:00:40.345220    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:05:40.335418    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:05:40.346462    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:10:01 linbit1 CRON[811053]: (root) CMD (test -e /run/systemd/system || SERVICE_MODE=1 /sbin/e2scrub_all -A -r)
Mar 12 03:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:10:40.335357    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:10:40.346807    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:10:55 linbit1 systemd[1]: Starting Online ext4 Metadata Check for All Filesystems...
Mar 12 03:10:55 linbit1 systemd[1]: e2scrub_all.service: Deactivated successfully.
Mar 12 03:10:55 linbit1 systemd[1]: Finished Online ext4 Metadata Check for All Filesystems.
Mar 12 03:14:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.oRak28.mount: Deactivated successfully.
Mar 12 03:15:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.aWGaar.mount: Deactivated successfully.
Mar 12 03:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:15:40.334749    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:15:40.347658    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:17:01 linbit1 CRON[815677]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 03:19:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.fWGa77.mount: Deactivated successfully.
Mar 12 03:19:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.xxAi4j.mount: Deactivated successfully.
Mar 12 03:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:20:40.335382    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:20:40.346833    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:25:40.335152    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:25:40.344581    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:30:01 linbit1 CRON[824176]: (root) CMD (test -e /run/systemd/system || SERVICE_MODE=1 /usr/lib/aarch64-linux-gnu/e2fsprogs/e2scrub_all_cron)
Mar 12 03:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:30:40.334774    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:30:40.345303    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:35:40.334845    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:35:40.345400    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:40:40.334968    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:40:40.344499    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:45:40.335060    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:45:40.345638    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:50:40.335119    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:50:40.344621    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 03:55:40.335027    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 03:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 03:55:40.345606    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 03:56:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.uNpJOF.mount: Deactivated successfully.
Mar 12 03:58:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.zZXBJW.mount: Deactivated successfully.
Mar 12 04:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:00:40.335244    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:00:40.345074    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:02:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.H6A6IR.mount: Deactivated successfully.
Mar 12 04:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:05:40.335302    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:05:40.348113    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:10:40.334427    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:10:40.346059    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:11:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.8NubwC.mount: Deactivated successfully.
Mar 12 04:11:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Ou1goJ.mount: Deactivated successfully.
Mar 12 04:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:15:40.334909    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:15:40.346334    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:17:01 linbit1 CRON[855035]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 04:18:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ZybKRo.mount: Deactivated successfully.
Mar 12 04:18:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.7Boghy.mount: Deactivated successfully.
Mar 12 04:19:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JlGuTy.mount: Deactivated successfully.
Mar 12 04:20:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MQ1n0Q.mount: Deactivated successfully.
Mar 12 04:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:20:40.334630    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:20:40.344426    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:23:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.4yawMC.mount: Deactivated successfully.
Mar 12 04:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:25:40.335182    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:25:40.345963    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:26:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ItkOdn.mount: Deactivated successfully.
Mar 12 04:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:30:40.335444    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:30:40.346764    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:31:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Xa9n7b.mount: Deactivated successfully.
Mar 12 04:34:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.S8MqCK.mount: Deactivated successfully.
Mar 12 04:35:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3BzBM3.mount: Deactivated successfully.
Mar 12 04:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:35:40.335118    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:35:40.345090    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:40:40.335334    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:40:40.345522    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:41:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Fve3z6.mount: Deactivated successfully.
Mar 12 04:44:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.AWixEK.mount: Deactivated successfully.
Mar 12 04:45:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.AuqsWf.mount: Deactivated successfully.
Mar 12 04:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:45:40.335484    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:45:40.346089    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:46:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.cgn0Zh.mount: Deactivated successfully.
Mar 12 04:50:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Q57Sne.mount: Deactivated successfully.
Mar 12 04:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:50:40.335241    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:50:40.345392    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 04:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 04:55:40.334953    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 04:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 04:55:40.344718    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:00:40.335295    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:00:40.345277    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:05:40.334830    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:05:40.345160    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:10:40.335241    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:10:40.345174    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:12:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Lu404U.mount: Deactivated successfully.
Mar 12 05:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:15:40.334827    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:15:40.345869    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:17:01 linbit1 CRON[894324]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 05:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:20:40.334566    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:20:40.348600    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:25:40.335328    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:25:40.345492    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:30:40.334888    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:30:40.345393    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:35:40.334454    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:35:40.351219    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:40:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VnW3wZ.mount: Deactivated successfully.
Mar 12 05:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:40:40.334734    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:40:40.346228    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:41:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.cquevd.mount: Deactivated successfully.
Mar 12 05:44:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MySuCu.mount: Deactivated successfully.
Mar 12 05:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:45:40.334785    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:45:40.345313    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:50:40.334594    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:50:40.347066    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 05:55:40.335640    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 05:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 05:55:40.345833    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 05:56:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.gpyiRr.mount: Deactivated successfully.
Mar 12 05:58:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.rMxA1L.mount: Deactivated successfully.
Mar 12 06:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:00:40.334784    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:00:40.345984    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:05:40.334920    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:05:40.344980    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:10:40.335558    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:10:40.347919    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:14:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.jt8br6.mount: Deactivated successfully.
Mar 12 06:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:15:40.334567    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:15:40.345542    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:17:01 linbit1 CRON[933643]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 06:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:20:40.335495    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:20:40.346246    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:24:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.O32YbF.mount: Deactivated successfully.
Mar 12 06:25:01 linbit1 CRON[938865]: (root) CMD (test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.daily ))
Mar 12 06:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:25:40.334968    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:25:40.345680    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:30:40.334334    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:30:40.343818    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:32:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.pHj7Er.mount: Deactivated successfully.
Mar 12 06:32:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.rHoUtv.mount: Deactivated successfully.
Mar 12 06:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:35:40.335246    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:35:40.344559    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:40:40.334894    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:40:40.344765    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:45:40.335211    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:45:40.344733    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:46:43 linbit1 systemd[1]: Starting Daily apt upgrade and clean activities...
Mar 12 06:47:01 linbit1 CRON[953290]: (root) CMD (test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.weekly ))
Mar 12 06:47:13 linbit1 systemd-networkd-wait-online[953145]: Timeout occurred while waiting for network connectivity.
Mar 12 06:47:13 linbit1 apt-helper[953142]: E: Sub-process /lib/systemd/systemd-networkd-wait-online returned an error code (1)
Mar 12 06:47:16 linbit1 systemd[1]: apt-daily-upgrade.service: Deactivated successfully.
Mar 12 06:47:16 linbit1 systemd[1]: Finished Daily apt upgrade and clean activities.
Mar 12 06:47:16 linbit1 systemd[1]: apt-daily-upgrade.service: Consumed 2.453s CPU time.
Mar 12 06:48:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6gLEJs.mount: Deactivated successfully.
Mar 12 06:49:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.GxPktE.mount: Deactivated successfully.
Mar 12 06:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:50:40.334997    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:50:40.344486    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 06:50:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6TDFv4.mount: Deactivated successfully.
Mar 12 06:51:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.p93W9t.mount: Deactivated successfully.
Mar 12 06:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 06:55:40.335186    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 06:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 06:55:40.345802    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:00:40.334104    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:00:40.344476    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:01:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.YN2sWc.mount: Deactivated successfully.
Mar 12 07:03:21 linbit1 systemd[1]: Starting Ubuntu Advantage Timer for running repeated jobs...
Mar 12 07:03:21 linbit1 systemd[1]: ua-timer.service: Deactivated successfully.
Mar 12 07:03:21 linbit1 systemd[1]: Finished Ubuntu Advantage Timer for running repeated jobs.
Mar 12 07:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:05:40.335273    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:05:40.345408    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:09:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XCIzmT.mount: Deactivated successfully.
Mar 12 07:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:10:40.335769    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:10:40.346885    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:15:40.335595    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:15:40.345747    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:17:02 linbit1 CRON[973015]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 07:18:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.b8dmVz.mount: Deactivated successfully.
Mar 12 07:19:37 linbit1 snapd[2460]: storehelpers.go:769: cannot refresh: snap has no updates available: "core18", "core20", "lxd", "microk8s", "snapd"
Mar 12 07:19:37 linbit1 snapd[2460]: autorefresh.go:551: auto-refresh: all snaps are up-to-date
Mar 12 07:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:20:40.334982    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:20:40.345039    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:21:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.wtY4eS.mount: Deactivated successfully.
Mar 12 07:23:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.eLgERV.mount: Deactivated successfully.
Mar 12 07:24:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.YAhYFZ.mount: Deactivated successfully.
Mar 12 07:25:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.V3fHRD.mount: Deactivated successfully.
Mar 12 07:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:25:40.334946    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:25:40.345495    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:30:40.334922    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:30:40.344714    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:35:40.335251    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:35:40.345509    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:40:40.334930    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:40:40.346326    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:45:40.334890    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:45:40.346081    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:50:40.335595    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:50:40.346909    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:54:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6iSbam.mount: Deactivated successfully.
Mar 12 07:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 07:55:40.334651    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 07:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 07:55:40.344533    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 07:56:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.gTwqKP.mount: Deactivated successfully.
Mar 12 08:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:00:40.335135    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:00:40.348346    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:02:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.vMwmLV.mount: Deactivated successfully.
Mar 12 08:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:05:40.335590    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:05:40.345444    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:08:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.uZgHJL.mount: Deactivated successfully.
Mar 12 08:09:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.yqclC8.mount: Deactivated successfully.
Mar 12 08:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:10:40.335020    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:10:40.345415    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:11:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.DlXV9Y.mount: Deactivated successfully.
Mar 12 08:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:15:40.334724    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:15:40.345292    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:17:01 linbit1 CRON[1012340]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 08:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:20:40.334693    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:20:40.345318    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:25:40.335591    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:25:40.345896    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:28:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Ovd8Ep.mount: Deactivated successfully.
Mar 12 08:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:30:40.335086    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:30:40.344765    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:31:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.9JKHU4.mount: Deactivated successfully.
Mar 12 08:33:37 linbit1 systemd[1]: Starting Daily man-db regeneration...
Mar 12 08:33:37 linbit1 systemd[1]: man-db.service: Deactivated successfully.
Mar 12 08:33:37 linbit1 systemd[1]: Finished Daily man-db regeneration.
Mar 12 08:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:35:40.334666    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:35:40.346637    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:40:40.334994    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:40:40.346155    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:45:40.334639    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:45:40.344627    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:50:40.334361    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:50:40.344609    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:51:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.def9WM.mount: Deactivated successfully.
Mar 12 08:53:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.nyx6m6.mount: Deactivated successfully.
Mar 12 08:53:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FdI2HN.mount: Deactivated successfully.
Mar 12 08:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 08:55:40.335127    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 08:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 08:55:40.346497    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 08:56:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.AFtXM1.mount: Deactivated successfully.
Mar 12 08:57:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.gov60w.mount: Deactivated successfully.
Mar 12 09:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:00:40.335014    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:00:40.345315    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:01:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.i6RdAI.mount: Deactivated successfully.
Mar 12 09:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:05:40.335297    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:05:40.344992    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:08:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Jar5eY.mount: Deactivated successfully.
Mar 12 09:08:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.GZMHny.mount: Deactivated successfully.
Mar 12 09:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:10:40.334408    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:10:40.343944    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:15:40.335329    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:15:40.345917    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:17:01 linbit1 CRON[1051690]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 09:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:20:40.334320    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:20:40.345262    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:20:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.F5Ray9.mount: Deactivated successfully.
Mar 12 09:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:25:40.335148    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:25:40.345048    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:30:40.335293    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:30:40.345406    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:35:40.335390    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:35:40.345564    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:40:40.334864    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:40:40.346494    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:41:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.yfYYiX.mount: Deactivated successfully.
Mar 12 09:41:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XJFWbp.mount: Deactivated successfully.
Mar 12 09:41:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6bnlCo.mount: Deactivated successfully.
Mar 12 09:42:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.4KcP6d.mount: Deactivated successfully.
Mar 12 09:44:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.smkM9X.mount: Deactivated successfully.
Mar 12 09:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:45:40.334923    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:45:40.344546    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:50:40.334675    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:50:40.346836    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 09:55:40.335421    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 09:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 09:55:40.345742    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 09:56:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.tPj2Fs.mount: Deactivated successfully.
Mar 12 10:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:00:40.334640    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:00:40.345272    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:05:40.335404    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:05:40.345360    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:10:40.335245    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:10:40.345148    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:13:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.wC9Fta.mount: Deactivated successfully.
Mar 12 10:14:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.QtM4sL.mount: Deactivated successfully.
Mar 12 10:15:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.pCowoY.mount: Deactivated successfully.
Mar 12 10:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:15:40.334915    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:15:40.344937    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:15:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Rf2R81.mount: Deactivated successfully.
Mar 12 10:17:01 linbit1 CRON[1091135]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 10:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:20:40.334698    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:20:40.344822    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:24:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.PJgWWx.mount: Deactivated successfully.
Mar 12 10:25:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.kG86bd.mount: Deactivated successfully.
Mar 12 10:25:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.RKPfX9.mount: Deactivated successfully.
Mar 12 10:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:25:40.335047    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:25:40.348386    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:28:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TThN5q.mount: Deactivated successfully.
Mar 12 10:28:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.iPQOEe.mount: Deactivated successfully.
Mar 12 10:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:30:40.335349    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:30:40.348304    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:32:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.cdsfYA.mount: Deactivated successfully.
Mar 12 10:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:35:40.335086    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:35:40.344924    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:35:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XazUIk.mount: Deactivated successfully.
Mar 12 10:39:57 linbit1 systemd[1]: Starting Refresh fwupd metadata and update motd...
Mar 12 10:39:57 linbit1 systemd[1]: fwupd-refresh.service: Main process exited, code=exited, status=1/FAILURE
Mar 12 10:39:57 linbit1 systemd[1]: fwupd-refresh.service: Failed with result 'exit-code'.
Mar 12 10:39:57 linbit1 systemd[1]: Failed to start Refresh fwupd metadata and update motd.
Mar 12 10:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:40:40.335274    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:40:40.345013    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:41:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.hzZTF1.mount: Deactivated successfully.
Mar 12 10:41:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.rsSsZN.mount: Deactivated successfully.
Mar 12 10:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:45:40.335462    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:45:40.346496    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:50:40.334963    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:50:40.346447    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:55:40.334891    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 10:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 10:55:40.345285    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 10:58:15 linbit1 microk8s.daemon-kubelite[2448]: E0312 10:58:15.556673    2448 repair.go:310] the cluster IP 10.152.183.223 may have leaked: flagging for later clean up
Mar 12 11:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:00:40.335398    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:00:40.346484    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:05:40.335597    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:05:40.345596    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:07:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.lU5xeP.mount: Deactivated successfully.
Mar 12 11:10:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JEhYkg.mount: Deactivated successfully.
Mar 12 11:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:10:40.335113    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:10:40.345182    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:15:40.335222    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:15:40.345167    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:17:01 linbit1 CRON[1130453]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 11:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:20:40.334761    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:20:40.350921    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:25:40.334212    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:25:40.345341    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:30:40.335053    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:30:40.346816    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:35:40.334850    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:35:40.344483    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:38:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TlvabB.mount: Deactivated successfully.
Mar 12 11:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:40:40.335228    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:40:40.346363    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:45:40.334672    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:45:40.345916    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:50:40.334805    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:50:40.344339    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 11:55:40.334248    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 11:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 11:55:40.343803    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 11:59:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.qOf2xj.mount: Deactivated successfully.
Mar 12 12:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:00:40.335255    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:00:40.344770    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:05:40.335050    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:05:40.345410    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:10:40.335047    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:10:40.345847    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:12:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.eKgRlt.mount: Deactivated successfully.
Mar 12 12:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:15:40.336037    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:15:40.345617    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:17:01 linbit1 CRON[1169793]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 12:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:20:40.335186    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:20:40.347667    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:25:40.335442    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:25:40.348476    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:30:40.335355    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:30:40.345353    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:35:40.335540    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:35:40.345535    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:39:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Ufz1uM.mount: Deactivated successfully.
Mar 12 12:39:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ydt6X9.mount: Deactivated successfully.
Mar 12 12:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:40:40.334709    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:40:40.347885    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:45:40.334890    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:45:40.346514    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:50:40.334900    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:50:40.344415    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 12:55:40.335254    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 12:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 12:55:40.345177    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 12:57:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.eaNtyK.mount: Deactivated successfully.
Mar 12 12:58:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.o5BFd1.mount: Deactivated successfully.
Mar 12 13:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:00:40.335183    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:00:40.345106    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:01:08 linbit1 microk8s.daemon-apiserver-kicker[1198719]: chmod: cannot access '/var/snap/microk8s/4567/var/kubernetes/backend/tmp-snapshot-80-54708144-390856922': No such file or directory
Mar 12 13:02:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.qw8IsE.mount: Deactivated successfully.
Mar 12 13:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:05:40.334811    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:05:40.345517    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:06:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.b0yP8R.mount: Deactivated successfully.
Mar 12 13:07:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.NXDa1k.mount: Deactivated successfully.
Mar 12 13:07:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.8iRGnp.mount: Deactivated successfully.
Mar 12 13:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:10:40.335293    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:10:40.348480    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:15:40.335431    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:15:40.348131    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:17:01 linbit1 CRON[1209077]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 13:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:20:40.335241    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:20:40.346657    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:22:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.hOcBSq.mount: Deactivated successfully.
Mar 12 13:25:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Iv9NJl.mount: Deactivated successfully.
Mar 12 13:25:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.GAPrB5.mount: Deactivated successfully.
Mar 12 13:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:25:40.335497    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:25:40.345562    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:25:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.jxjU6D.mount: Deactivated successfully.
Mar 12 13:26:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Drc6FR.mount: Deactivated successfully.
Mar 12 13:26:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.82a6Vk.mount: Deactivated successfully.
Mar 12 13:28:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.rpFfIv.mount: Deactivated successfully.
Mar 12 13:29:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UnTafM.mount: Deactivated successfully.
Mar 12 13:30:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.oHO527.mount: Deactivated successfully.
Mar 12 13:30:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.8qIhG5.mount: Deactivated successfully.
Mar 12 13:30:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.g4qT9r.mount: Deactivated successfully.
Mar 12 13:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:30:40.334406    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:30:40.345336    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:34:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.QkKqce.mount: Deactivated successfully.
Mar 12 13:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:35:40.335390    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:35:40.345948    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:39:55 linbit1 systemd[1]: Starting Ubuntu Advantage Timer for running repeated jobs...
Mar 12 13:39:55 linbit1 systemd[1]: ua-timer.service: Deactivated successfully.
Mar 12 13:39:55 linbit1 systemd[1]: Finished Ubuntu Advantage Timer for running repeated jobs.
Mar 12 13:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:40:40.335332    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:40:40.348772    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:41:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.fUXcgi.mount: Deactivated successfully.
Mar 12 13:42:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.YkG6Fm.mount: Deactivated successfully.
Mar 12 13:43:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TdQApx.mount: Deactivated successfully.
Mar 12 13:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:45:40.334957    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:45:40.346556    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:46:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.0SplXP.mount: Deactivated successfully.
Mar 12 13:47:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UOLoup.mount: Deactivated successfully.
Mar 12 13:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:50:40.334442    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:50:40.347207    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:51:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.bUwgX0.mount: Deactivated successfully.
Mar 12 13:51:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.A3kmmA.mount: Deactivated successfully.
Mar 12 13:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 13:55:40.335372    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 13:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 13:55:40.345068    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 13:55:45 linbit1 systemd[1]: Starting Daily apt download activities...
Mar 12 13:56:15 linbit1 systemd-networkd-wait-online[1234678]: Timeout occurred while waiting for network connectivity.
Mar 12 13:56:16 linbit1 apt-helper[1234675]: E: Sub-process /lib/systemd/systemd-networkd-wait-online returned an error code (1)
Mar 12 13:56:16 linbit1 systemd[1]: apt-daily.service: Deactivated successfully.
Mar 12 13:56:16 linbit1 systemd[1]: Finished Daily apt download activities.
Mar 12 14:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:00:40.335168    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:00:40.348378    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:01:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5AeYOh.mount: Deactivated successfully.
Mar 12 14:01:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FtcyD0.mount: Deactivated successfully.
Mar 12 14:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:05:40.334864    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:05:40.346516    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:08:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FYjbBY.mount: Deactivated successfully.
Mar 12 14:09:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.aFPQXD.mount: Deactivated successfully.
Mar 12 14:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:10:40.334827    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:10:40.346516    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:15:40.335467    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:15:40.346694    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:17:01 linbit1 CRON[1248580]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 14:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:20:40.335378    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:20:40.347595    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:23:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FtmmwL.mount: Deactivated successfully.
Mar 12 14:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:25:40.335229    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:25:40.345805    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:30:40.335405    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:30:40.346814    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:35:40.335073    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:35:40.347294    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:40:40.334575    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:40:40.345195    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:45:40.334750    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:45:40.345066    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:50:40.334732    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:50:40.345004    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 14:51:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.cA1N2B.mount: Deactivated successfully.
Mar 12 14:51:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.CULNTe.mount: Deactivated successfully.
Mar 12 14:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 14:55:40.334948    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 14:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 14:55:40.345472    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:00:40.334253    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:00:40.345182    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:05:40.334755    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:05:40.344699    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:08:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.koBCR4.mount: Deactivated successfully.
Mar 12 15:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:10:40.335209    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:10:40.344973    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:15:40.334888    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:15:40.345374    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:17:01 linbit1 CRON[1287959]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 15:17:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.uU50yJ.mount: Deactivated successfully.
Mar 12 15:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:20:40.335615    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:20:40.345569    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:25:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.eWO1zv.mount: Deactivated successfully.
Mar 12 15:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:25:40.335126    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:25:40.345333    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:26:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.iBKvKK.mount: Deactivated successfully.
Mar 12 15:26:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.nadhL6.mount: Deactivated successfully.
Mar 12 15:29:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XWmAkI.mount: Deactivated successfully.
Mar 12 15:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:30:40.334985    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:30:40.346827    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:30:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3KxkVZ.mount: Deactivated successfully.
Mar 12 15:32:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.b6TdZZ.mount: Deactivated successfully.
Mar 12 15:32:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Zso5P5.mount: Deactivated successfully.
Mar 12 15:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:35:40.334855    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:35:40.344398    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:40:40.335161    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:40:40.346830    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:43:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6xfv3m.mount: Deactivated successfully.
Mar 12 15:43:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XJFDuo.mount: Deactivated successfully.
Mar 12 15:44:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UO8TrD.mount: Deactivated successfully.
Mar 12 15:44:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ZSAjk5.mount: Deactivated successfully.
Mar 12 15:45:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.kD8Z7M.mount: Deactivated successfully.
Mar 12 15:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:45:40.335071    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:45:40.347284    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:50:40.335584    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:50:40.346993    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 15:53:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.amz5dP.mount: Deactivated successfully.
Mar 12 15:54:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3fgz00.mount: Deactivated successfully.
Mar 12 15:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 15:55:40.335375    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 15:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 15:55:40.346539    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:00:40.334695    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:00:40.345661    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:05:40.335419    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:05:40.346141    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:10:40.334966    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:10:40.345866    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:15:40.335106    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:15:40.345444    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:16:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.arPY1m.mount: Deactivated successfully.
Mar 12 16:17:01 linbit1 CRON[1327291]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 16:18:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.bNXUWq.mount: Deactivated successfully.
Mar 12 16:20:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.t6UuVO.mount: Deactivated successfully.
Mar 12 16:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:20:40.334990    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:20:40.344655    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:25:40.334726    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:25:40.345064    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:26:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.IWk1ey.mount: Deactivated successfully.
Mar 12 16:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:30:40.334759    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:30:40.344318    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:33:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JL62Mi.mount: Deactivated successfully.
Mar 12 16:34:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.lFjz9s.mount: Deactivated successfully.
Mar 12 16:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:35:40.335270    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:35:40.345598    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:40:40.335537    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:40:40.345741    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:41:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.iRQgLu.mount: Deactivated successfully.
Mar 12 16:41:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.4aqUxM.mount: Deactivated successfully.
Mar 12 16:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:45:40.335045    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:45:40.348168    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:45:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.yv0i65.mount: Deactivated successfully.
Mar 12 16:47:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ylbSxP.mount: Deactivated successfully.
Mar 12 16:48:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.pN90Zu.mount: Deactivated successfully.
Mar 12 16:48:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.CNVOu1.mount: Deactivated successfully.
Mar 12 16:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:50:40.334627    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:50:40.345273    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 16:51:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.QFJww8.mount: Deactivated successfully.
Mar 12 16:52:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.HcMQrW.mount: Deactivated successfully.
Mar 12 16:52:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.X1kmVX.mount: Deactivated successfully.
Mar 12 16:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 16:55:40.335111    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 16:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 16:55:40.345723    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:00:40.336248    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:00:40.348914    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:05:40.335396    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:05:40.345876    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:06:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LHaiKU.mount: Deactivated successfully.
Mar 12 17:09:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Fqc96q.mount: Deactivated successfully.
Mar 12 17:09:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.0oPcBG.mount: Deactivated successfully.
Mar 12 17:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:10:40.334603    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:10:40.344977    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:15:40.334767    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:15:40.347345    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:17:01 linbit1 CRON[1366727]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 17:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:20:40.334725    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:20:40.344278    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:21:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.hcJxuh.mount: Deactivated successfully.
Mar 12 17:24:37 linbit1 snapd[2460]: storehelpers.go:769: cannot refresh: snap has no updates available: "core18", "core20", "lxd", "microk8s", "snapd"
Mar 12 17:24:37 linbit1 snapd[2460]: autorefresh.go:551: auto-refresh: all snaps are up-to-date
Mar 12 17:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:25:40.334558    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:25:40.344838    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:30:40.334959    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:30:40.345961    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:35:40.335097    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:35:40.344601    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:40:20 linbit1 microk8s.daemon-apiserver-kicker[1382102]: chgrp: changing group of '/var/snap/microk8s/4567/var/kubernetes/backend/open-13830': No such file or directory
Mar 12 17:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:40:40.335035    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:40:40.347149    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:45:40.335269    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:45:40.345205    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:50:40.335513    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:50:40.348948    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:50:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.KGPTXI.mount: Deactivated successfully.
Mar 12 17:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 17:55:40.335321    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 17:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 17:55:40.345964    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 17:55:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.9gZrZY.mount: Deactivated successfully.
Mar 12 18:00:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.xp4MMR.mount: Deactivated successfully.
Mar 12 18:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:00:40.334671    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:00:40.345039    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:05:40.334764    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:05:40.345069    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:08:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XOZVuv.mount: Deactivated successfully.
Mar 12 18:09:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.8uzebm.mount: Deactivated successfully.
Mar 12 18:09:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.RlcxC4.mount: Deactivated successfully.
Mar 12 18:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:10:40.334497    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:10:40.345284    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:15:40.334777    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:15:40.345310    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:16:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.4TLQ3k.mount: Deactivated successfully.
Mar 12 18:17:01 linbit1 CRON[1406103]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 18:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:20:40.334662    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:20:40.344806    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:24:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.YOLq76.mount: Deactivated successfully.
Mar 12 18:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:25:40.334695    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:25:40.348322    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:30:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ILOFPq.mount: Deactivated successfully.
Mar 12 18:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:30:40.334737    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:30:40.345896    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:33:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.EaNxXp.mount: Deactivated successfully.
Mar 12 18:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:35:40.335017    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:35:40.344532    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:40:40.335060    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:40:40.345196    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:45:40.335018    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:45:40.348405    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:49:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Rj9rzN.mount: Deactivated successfully.
Mar 12 18:50:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MqEb6e.mount: Deactivated successfully.
Mar 12 18:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:50:40.334956    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:50:40.344706    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:50:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ARuBym.mount: Deactivated successfully.
Mar 12 18:51:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.2ICHdX.mount: Deactivated successfully.
Mar 12 18:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 18:55:40.335041    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 18:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 18:55:40.345159    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 18:55:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.hzsnVN.mount: Deactivated successfully.
Mar 12 18:56:19 linbit1 systemd[1]: Starting Message of the Day...
Mar 12 18:56:20 linbit1 50-motd-news[1431924]:  * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
Mar 12 18:56:20 linbit1 50-motd-news[1431924]:    just raised the bar for easy, resilient and secure K8s cluster deployment.
Mar 12 18:56:20 linbit1 50-motd-news[1431924]:    https://ubuntu.com/engage/secure-kubernetes-at-the-edge
Mar 12 18:56:20 linbit1 systemd[1]: motd-news.service: Deactivated successfully.
Mar 12 18:56:20 linbit1 systemd[1]: Finished Message of the Day.
Mar 12 18:57:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5z1eZO.mount: Deactivated successfully.
Mar 12 18:57:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XVXvE7.mount: Deactivated successfully.
Mar 12 18:58:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ZAmlvy.mount: Deactivated successfully.
Mar 12 19:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:00:40.334595    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:00:40.345188    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:02:48 linbit1 systemd[1]: Starting Refresh fwupd metadata and update motd...
Mar 12 19:02:48 linbit1 systemd[1]: fwupd-refresh.service: Main process exited, code=exited, status=1/FAILURE
Mar 12 19:02:48 linbit1 systemd[1]: fwupd-refresh.service: Failed with result 'exit-code'.
Mar 12 19:02:48 linbit1 systemd[1]: Failed to start Refresh fwupd metadata and update motd.
Mar 12 19:03:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.WRmK4L.mount: Deactivated successfully.
Mar 12 19:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:05:40.334524    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:05:40.345716    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:10:40.335362    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:10:40.345632    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:13:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.xrnc14.mount: Deactivated successfully.
Mar 12 19:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:15:40.335080    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:15:40.346150    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:17:01 linbit1 CRON[1445448]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 19:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:20:40.335543    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:20:40.345324    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:25:40.334861    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:25:40.347150    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:26:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.m26hS9.mount: Deactivated successfully.
Mar 12 19:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:30:40.335059    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:30:40.345731    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:35:40.334455    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:35:40.345019    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:40:40.334984    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:40:40.345001    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:45:40.334312    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:45:40.344099    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:50:40.335443    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:50:40.345344    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 19:55:40.335507    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 19:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 19:55:40.348193    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 19:55:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.sOvKS3.mount: Deactivated successfully.
Mar 12 19:56:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.haiBwc.mount: Deactivated successfully.
Mar 12 19:56:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.eHRViP.mount: Deactivated successfully.
Mar 12 19:57:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.GDSTxB.mount: Deactivated successfully.
Mar 12 19:58:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SnpxhA.mount: Deactivated successfully.
Mar 12 20:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:00:40.335425    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:00:40.345502    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:04:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.lewIbY.mount: Deactivated successfully.
Mar 12 20:04:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.afYgQb.mount: Deactivated successfully.
Mar 12 20:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:05:40.335369    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:05:40.346073    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:10:40.334565    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:10:40.345589    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:15:40.335245    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:15:40.344772    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:17:01 linbit1 CRON[1484810]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 20:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:20:40.334853    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:20:40.344710    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:25:40.334945    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:25:40.346502    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:30:40.334356    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:30:40.347308    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:35:40.334837    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:35:40.344693    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:36:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.nO0p9y.mount: Deactivated successfully.
Mar 12 20:38:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.WJUhV7.mount: Deactivated successfully.
Mar 12 20:38:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.4Gli0t.mount: Deactivated successfully.
Mar 12 20:38:28 linbit1 systemd[1]: Starting Ubuntu Advantage Timer for running repeated jobs...
Mar 12 20:38:29 linbit1 systemd[1]: ua-timer.service: Deactivated successfully.
Mar 12 20:38:29 linbit1 systemd[1]: Finished Ubuntu Advantage Timer for running repeated jobs.
Mar 12 20:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:40:40.335536    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:40:40.347015    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:45:40.335130    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:45:40.345449    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:46:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ATfg4J.mount: Deactivated successfully.
Mar 12 20:46:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.z39ZVN.mount: Deactivated successfully.
Mar 12 20:46:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.dFcsHj.mount: Deactivated successfully.
Mar 12 20:50:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SUDCrC.mount: Deactivated successfully.
Mar 12 20:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:50:40.334963    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:50:40.344321    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:51:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MNK9z4.mount: Deactivated successfully.
Mar 12 20:51:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3h48mu.mount: Deactivated successfully.
Mar 12 20:52:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.8E5vmw.mount: Deactivated successfully.
Mar 12 20:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 20:55:40.334867    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 20:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 20:55:40.344867    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 20:57:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TiYAcM.mount: Deactivated successfully.
Mar 12 21:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:00:40.334805    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:00:40.345615    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:02:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.iTObXE.mount: Deactivated successfully.
Mar 12 21:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:05:40.334283    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:05:40.346207    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:09:37 linbit1 snapd[2460]: storehelpers.go:769: cannot refresh: snap has no updates available: "core18", "core20", "lxd", "microk8s", "snapd"
Mar 12 21:09:37 linbit1 snapd[2460]: autorefresh.go:551: auto-refresh: all snaps are up-to-date
Mar 12 21:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:10:40.335150    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:10:40.346736    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:14:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.OoprCn.mount: Deactivated successfully.
Mar 12 21:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:15:40.334728    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:15:40.345441    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:17:01 linbit1 CRON[1524151]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 21:19:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.projX9.mount: Deactivated successfully.
Mar 12 21:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:20:40.334897    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:20:40.346340    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:23:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LrvZ7i.mount: Deactivated successfully.
Mar 12 21:24:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JqNTW5.mount: Deactivated successfully.
Mar 12 21:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:25:40.334786    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:25:40.346014    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:27:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Pnozpj.mount: Deactivated successfully.
Mar 12 21:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:30:40.335785    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:30:40.346418    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:31:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.OYclF6.mount: Deactivated successfully.
Mar 12 21:32:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.2IoHUK.mount: Deactivated successfully.
Mar 12 21:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:35:40.334962    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:35:40.345779    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:38:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.4oIxYX.mount: Deactivated successfully.
Mar 12 21:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:40:40.334773    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:40:40.344414    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:40:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.PFluJf.mount: Deactivated successfully.
Mar 12 21:42:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MAdjOQ.mount: Deactivated successfully.
Mar 12 21:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:45:40.334731    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:45:40.345015    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:49:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.f4FLSV.mount: Deactivated successfully.
Mar 12 21:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:50:40.334363    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:50:40.344900    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 21:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 21:55:40.334761    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 21:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 21:55:40.345038    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:00:40.335107    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:00:40.345334    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:03:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SP8OZi.mount: Deactivated successfully.
Mar 12 22:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:05:40.335499    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:05:40.345417    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:07:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.uEAD2a.mount: Deactivated successfully.
Mar 12 22:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:10:40.335259    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:10:40.346044    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:11:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.fMYJyf.mount: Deactivated successfully.
Mar 12 22:13:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ATvS7s.mount: Deactivated successfully.
Mar 12 22:15:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.owBPOs.mount: Deactivated successfully.
Mar 12 22:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:15:40.335275    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:15:40.346493    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:17:01 linbit1 CRON[1563578]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 22:17:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XSk6c2.mount: Deactivated successfully.
Mar 12 22:17:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.z6OUWm.mount: Deactivated successfully.
Mar 12 22:18:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.uK1wl0.mount: Deactivated successfully.
Mar 12 22:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:20:40.335378    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:20:40.345370    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ vgic(9) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ ptimer(10) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ vtimer(11) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arch_timer(12) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arch_mem_timer(15) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ pcc-mbox(18) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ pcc-mbox(30) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ APMC0D0F:00(31) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ APMC0D0F:01(32) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ ACPI:Ged(33) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ ACPI:Ged(34) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ ACPI:Ged(35) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ uart-pl011(36) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_0(38) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_1(39) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_2(40) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_3(41) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_4(42) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_5(43) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_6(44) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_7(45) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_8(46) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_9(47) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_10(48) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_11(49) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_12(50) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_13(51) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_14(52) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_15(53) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_16(54) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_17(55) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_18(56) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_19(57) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_20(58) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_21(59) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_22(60) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_23(61) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_24(62) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_25(63) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_26(64) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_27(65) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_28(66) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_29(67) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_30(68) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_31(69) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_32(70) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_33(71) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_34(72) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_35(73) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_36(74) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_37(75) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_38(76) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_39(77) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_40(78) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_41(79) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_42(80) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_43(81) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_44(82) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_45(83) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_46(84) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_47(85) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_48(86) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_49(87) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_50(88) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_51(89) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_52(90) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_53(91) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_54(92) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_55(93) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_56(94) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_57(95) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_58(96) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_59(97) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_60(98) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_61(99) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_62(100) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_63(101) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm_dsu_64(102) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ ARMHC600:00(103) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ dmc620-pmu(104) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ dmc620-pmu(105) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ dmc620-pmu(106) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ dmc620-pmu(107) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ dmc620-pmu(108) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ dmc620-pmu(109) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ dmc620-pmu(110) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ dmc620-pmu(111) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-pmu(113) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-evtq(115) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-gerror(116) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-priq(117) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-evtq(118) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-gerror(119) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-priq(120) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-evtq(121) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-gerror(122) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-priq(123) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-evtq(124) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-gerror(125) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-priq(126) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-evtq(127) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-gerror(128) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-priq(129) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-evtq(130) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-gerror(131) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ arm-smmu-v3-priq(132) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ aerdrv(133) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ aerdrv(134) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ aerdrv(135) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ aerdrv(136) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ aerdrv(137) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ aerdrv(138) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ xhci_hcd(139) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ xhci_hcd(140) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ xhci_hcd(141) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ xhci_hcd(142) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ xhci_hcd(143) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ xhci_hcd(144) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ xhci_hcd(145) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ xhci_hcd(146) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0(147) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0-TxRx-0(148) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0-TxRx-1(149) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0-TxRx-2(150) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0-TxRx-3(151) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0-TxRx-4(152) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0-TxRx-5(153) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0-TxRx-6(154) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f0-TxRx-7(155) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q0(156) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q1(157) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q2(158) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q3(159) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q4(160) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q5(161) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q6(162) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q7(163) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q8(164) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q9(165) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q10(166) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q11(167) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q12(168) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q13(169) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q14(170) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q15(171) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q16(172) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q17(173) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q18(174) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q19(175) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q20(176) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q21(177) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q22(178) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q23(179) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q24(180) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q25(181) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q26(182) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q27(183) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q28(184) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q29(185) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q30(186) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q31(187) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q32(188) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q33(189) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q34(190) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q35(191) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q36(192) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q37(193) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q38(194) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q39(195) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q40(196) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q41(197) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q42(198) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q43(199) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q44(200) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q45(201) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q46(202) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q47(203) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q48(204) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q49(205) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q50(206) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q51(207) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q52(208) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q53(209) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q54(210) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q55(211) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q56(212) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q57(213) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q58(214) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q59(215) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q60(216) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q61(217) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q62(218) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme0q63(219) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1(221) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1-TxRx-0(222) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1-TxRx-1(223) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1-TxRx-2(224) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1-TxRx-3(225) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1-TxRx-4(226) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1-TxRx-5(227) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1-TxRx-6(228) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP2p4s0f1-TxRx-7(229) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q0(230) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f0np0-TxR(231) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f0np0-TxR(232) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f0np0-TxR(233) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f0np0-TxR(234) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f0np0-TxR(235) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f0np0-TxR(236) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f0np0-TxR(237) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f0np0-TxR(238) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q1(240) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q2(241) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q3(242) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q4(243) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q5(244) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q6(245) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q7(246) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q8(247) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q9(248) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q10(249) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q11(250) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q12(251) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q13(252) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q14(253) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q15(254) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q16(255) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q17(256) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q18(257) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q19(258) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q20(259) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q21(260) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q22(261) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q23(262) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q24(263) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q25(264) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q26(265) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q27(266) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q28(267) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q29(268) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q30(269) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q31(270) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q32(271) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q33(272) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q34(273) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q35(274) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q36(275) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q37(276) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q38(277) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q39(278) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q40(279) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q41(280) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q42(281) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q43(282) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q44(283) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q45(284) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q46(285) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q47(286) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q48(287) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q49(288) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q50(289) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q51(290) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q52(291) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q53(292) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q54(293) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q55(294) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q56(295) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q57(296) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q58(297) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q59(298) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q60(299) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q61(300) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q62(301) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme1q63(302) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q0(303) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q1(304) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q2(305) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q3(306) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q4(307) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q5(308) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q6(309) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q7(310) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q8(311) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q9(312) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q10(313) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q11(314) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q12(315) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q13(316) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q14(317) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q15(318) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q16(319) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q17(320) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q18(321) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q19(322) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q20(323) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q21(324) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q22(325) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q23(326) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q24(327) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q25(328) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q26(329) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q27(330) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q28(331) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q29(332) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q30(333) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q31(334) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme2q32(335) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q0(336) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q1(337) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q2(338) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q3(339) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q4(340) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q5(341) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q6(342) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q7(343) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q8(344) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q9(345) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q10(346) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q11(347) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q12(348) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q13(349) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q14(350) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q15(351) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q16(352) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q17(353) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q18(354) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q19(355) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q20(356) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q21(357) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q22(358) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q23(359) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q24(360) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q25(361) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q26(362) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q27(363) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q28(364) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q29(365) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q30(366) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q31(367) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q32(368) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q33(369) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q34(370) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q35(371) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q36(372) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q37(373) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q38(374) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q39(375) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q40(376) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q41(377) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q42(378) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q43(379) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q44(380) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q45(381) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q46(382) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q47(383) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q48(384) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q49(385) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q50(386) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q51(387) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q52(388) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q53(389) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q54(390) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q55(391) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q56(392) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q57(393) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q58(394) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q59(395) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q60(396) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q61(397) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q62(398) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme3q63(399) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q0(400) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q1(401) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q2(402) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q3(403) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q4(404) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q5(405) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q6(406) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q7(407) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q8(408) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q9(409) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q10(410) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q11(411) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q12(412) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q13(413) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q14(414) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q15(415) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q16(416) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q17(417) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q18(418) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q19(419) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q20(420) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q21(421) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q22(422) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q23(423) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q24(424) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q25(425) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q26(426) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q27(427) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q28(428) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q29(429) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q30(430) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q31(431) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q32(432) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q33(433) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q34(434) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q35(435) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q36(436) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q37(437) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q38(438) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q39(439) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q40(440) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q41(441) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q42(442) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q43(443) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q44(444) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q45(445) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q46(446) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q47(447) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q48(448) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q49(449) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q50(450) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q51(451) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q52(452) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q53(453) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q54(454) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q55(455) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q56(456) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q57(457) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q58(458) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q59(459) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q60(460) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q61(461) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q62(462) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme4q63(463) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q0(464) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f1np1-TxR(465) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f1np1-TxR(466) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f1np1-TxR(467) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f1np1-TxR(468) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f1np1-TxR(469) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f1np1-TxR(470) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f1np1-TxR(471) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f1np1-TxR(472) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q1(473) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q2(474) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q3(475) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q4(476) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q5(477) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q6(478) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q7(479) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q8(480) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q9(481) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q10(482) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q11(483) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q12(484) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q13(485) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q14(486) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q15(487) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q16(488) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q17(489) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q18(490) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q19(491) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q20(492) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q21(493) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q22(494) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q23(495) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q24(496) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q25(497) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q26(498) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q27(499) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q28(500) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q29(501) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q30(502) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q31(503) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q32(504) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q33(505) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q34(506) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q35(507) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q36(508) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q37(509) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q38(510) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q39(511) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q40(512) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q41(513) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q42(514) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q43(515) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q44(516) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q45(517) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q46(518) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q47(519) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q48(520) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q49(521) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q50(522) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q51(523) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q52(524) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q53(525) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q54(526) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q55(527) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q56(528) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q57(529) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q58(530) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q59(531) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q60(532) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q61(533) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q62(534) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme5q63(535) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q0(536) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q0(537) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q1(538) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q2(539) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q3(540) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q4(541) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q5(542) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q6(543) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q7(544) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q8(545) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q9(546) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q10(547) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q11(548) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q12(549) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q13(550) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q14(551) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q15(552) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q16(553) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q17(554) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q18(555) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q19(556) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q20(557) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q21(558) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q22(559) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q23(560) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q24(561) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q25(562) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q26(563) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q27(564) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q28(565) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q29(566) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q30(567) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q31(568) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q32(569) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q33(570) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q34(571) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q35(572) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q36(573) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q37(574) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q38(575) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q39(576) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q40(577) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q41(578) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q42(579) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q43(580) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q44(581) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q45(582) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q46(583) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q47(584) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q48(585) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q49(586) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q50(587) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q51(588) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q52(589) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q53(590) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q54(591) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q55(592) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q56(593) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q57(594) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q58(595) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q59(596) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q60(597) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q61(598) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q62(599) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme6q63(600) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q0(601) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f2np2-TxR(602) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f2np2-TxR(603) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f2np2-TxR(604) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f2np2-TxR(605) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f2np2-TxR(606) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f2np2-TxR(607) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f2np2-TxR(608) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f2np2-TxR(609) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q1(610) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q2(611) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q3(612) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q4(613) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q5(614) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q6(615) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q7(616) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q8(617) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q9(618) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q10(619) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q11(620) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q12(621) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q13(622) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q14(623) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q15(624) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q16(625) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q17(626) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q18(627) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q19(628) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q20(629) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q21(630) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q22(631) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q23(632) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q24(633) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q25(634) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q26(635) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q27(636) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q28(637) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q29(638) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q30(639) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q31(640) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q32(641) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q33(642) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q34(643) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q35(644) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q36(645) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q37(646) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q38(647) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q39(648) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q40(649) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q41(650) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q42(651) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q43(652) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q44(653) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q45(654) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q46(655) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q47(656) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q48(657) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q49(658) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q50(659) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q51(660) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q52(661) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q53(662) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q54(663) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q55(664) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q56(665) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q57(666) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q58(667) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q59(668) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q60(669) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q61(670) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q62(671) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme7q63(672) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q1(673) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q2(674) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q3(675) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q4(676) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q5(677) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q6(678) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q7(679) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q8(680) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q9(681) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q10(682) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q11(683) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q12(684) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q13(685) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q14(686) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q15(687) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q16(688) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q17(689) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q18(690) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q19(691) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q20(692) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q21(693) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q22(694) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q23(695) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q24(696) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q25(697) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q26(698) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q27(699) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q28(700) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q29(701) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q30(702) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q31(703) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q32(704) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q33(705) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q34(706) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q35(707) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q36(708) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q37(709) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q38(710) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q39(711) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q40(712) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q41(713) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q42(714) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q43(715) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q44(716) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q45(717) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q46(718) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q47(719) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q48(720) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q49(721) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q50(722) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q51(723) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q52(724) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q53(725) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q54(726) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q55(727) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q56(728) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q57(729) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q58(730) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q59(731) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q60(732) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q61(733) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q62(734) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ nvme8q63(735) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f3np3-TxR(736) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f3np3-TxR(737) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f3np3-TxR(738) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f3np3-TxR(739) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f3np3-TxR(740) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f3np3-TxR(741) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f3np3-TxR(742) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ enP3p1s0f3np3-TxR(743) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_async0@pci:0000:01:00.0(744) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp0@pci:0000:01:00.0(745) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp1@pci:0000:01:00.0(746) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp2@pci:0000:01:00.0(747) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp3@pci:0000:01:00.0(748) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp4@pci:0000:01:00.0(749) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp5@pci:0000:01:00.0(750) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp6@pci:0000:01:00.0(751) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp7@pci:0000:01:00.0(752) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp8@pci:0000:01:00.0(753) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp9@pci:0000:01:00.0(754) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp10@pci:0000:01:00.0(755) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp11@pci:0000:01:00.0(756) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp12@pci:0000:01:00.0(757) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp13@pci:0000:01:00.0(758) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp14@pci:0000:01:00.0(759) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp15@pci:0000:01:00.0(760) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp16@pci:0000:01:00.0(761) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp17@pci:0000:01:00.0(762) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp18@pci:0000:01:00.0(763) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp19@pci:0000:01:00.0(764) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp20@pci:0000:01:00.0(765) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp21@pci:0000:01:00.0(766) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp22@pci:0000:01:00.0(767) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp23@pci:0000:01:00.0(768) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp24@pci:0000:01:00.0(769) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp25@pci:0000:01:00.0(770) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp26@pci:0000:01:00.0(771) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp27@pci:0000:01:00.0(772) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp28@pci:0000:01:00.0(773) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp29@pci:0000:01:00.0(774) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp30@pci:0000:01:00.0(775) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp31@pci:0000:01:00.0(776) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp32@pci:0000:01:00.0(777) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp33@pci:0000:01:00.0(778) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp34@pci:0000:01:00.0(779) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp35@pci:0000:01:00.0(780) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp36@pci:0000:01:00.0(781) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp37@pci:0000:01:00.0(782) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp38@pci:0000:01:00.0(783) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp39@pci:0000:01:00.0(784) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp40@pci:0000:01:00.0(785) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp41@pci:0000:01:00.0(786) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp42@pci:0000:01:00.0(787) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp43@pci:0000:01:00.0(788) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp44@pci:0000:01:00.0(789) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp45@pci:0000:01:00.0(790) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp46@pci:0000:01:00.0(791) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp47@pci:0000:01:00.0(792) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp48@pci:0000:01:00.0(793) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp49@pci:0000:01:00.0(794) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp50@pci:0000:01:00.0(795) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp51@pci:0000:01:00.0(796) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp52@pci:0000:01:00.0(797) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp53@pci:0000:01:00.0(798) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp54@pci:0000:01:00.0(799) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp55@pci:0000:01:00.0(800) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp56@pci:0000:01:00.0(801) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp57@pci:0000:01:00.0(802) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp58@pci:0000:01:00.0(803) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp59@pci:0000:01:00.0(804) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp60@pci:0000:01:00.0(805) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp61@pci:0000:01:00.0(806) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp62@pci:0000:01:00.0(807) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_async0@pci:0001:01:00.0(808) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp0@pci:0001:01:00.0(809) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp1@pci:0001:01:00.0(810) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp2@pci:0001:01:00.0(811) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp3@pci:0001:01:00.0(812) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp4@pci:0001:01:00.0(813) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp5@pci:0001:01:00.0(814) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp6@pci:0001:01:00.0(815) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp7@pci:0001:01:00.0(816) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp8@pci:0001:01:00.0(817) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp9@pci:0001:01:00.0(818) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp10@pci:0001:01:00.0(819) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp11@pci:0001:01:00.0(820) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp12@pci:0001:01:00.0(821) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp13@pci:0001:01:00.0(822) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp14@pci:0001:01:00.0(823) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp15@pci:0001:01:00.0(824) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp16@pci:0001:01:00.0(825) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp17@pci:0001:01:00.0(826) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp18@pci:0001:01:00.0(827) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp19@pci:0001:01:00.0(828) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp20@pci:0001:01:00.0(829) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp21@pci:0001:01:00.0(830) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp22@pci:0001:01:00.0(831) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp23@pci:0001:01:00.0(832) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp24@pci:0001:01:00.0(833) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp25@pci:0001:01:00.0(834) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp26@pci:0001:01:00.0(835) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp27@pci:0001:01:00.0(836) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp28@pci:0001:01:00.0(837) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp29@pci:0001:01:00.0(838) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp30@pci:0001:01:00.0(839) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp31@pci:0001:01:00.0(840) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp32@pci:0001:01:00.0(841) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp33@pci:0001:01:00.0(842) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp34@pci:0001:01:00.0(843) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp35@pci:0001:01:00.0(844) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp36@pci:0001:01:00.0(845) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp37@pci:0001:01:00.0(846) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp38@pci:0001:01:00.0(847) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp39@pci:0001:01:00.0(848) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp40@pci:0001:01:00.0(849) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp41@pci:0001:01:00.0(850) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp42@pci:0001:01:00.0(851) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp43@pci:0001:01:00.0(852) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp44@pci:0001:01:00.0(853) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp45@pci:0001:01:00.0(854) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp46@pci:0001:01:00.0(855) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp47@pci:0001:01:00.0(856) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp48@pci:0001:01:00.0(857) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp49@pci:0001:01:00.0(858) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp50@pci:0001:01:00.0(859) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp51@pci:0001:01:00.0(860) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp52@pci:0001:01:00.0(861) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp53@pci:0001:01:00.0(862) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp54@pci:0001:01:00.0(863) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp55@pci:0001:01:00.0(864) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp56@pci:0001:01:00.0(865) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp57@pci:0001:01:00.0(866) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp58@pci:0001:01:00.0(867) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp59@pci:0001:01:00.0(868) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp60@pci:0001:01:00.0(869) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp61@pci:0001:01:00.0(870) guessed as class 0
Mar 12 22:23:09 linbit1 /usr/sbin/irqbalance: IRQ mlx5_comp62@pci:0001:01:00.0(871) guessed as class 0
Mar 12 22:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:25:40.335162    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:25:40.344820    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:27:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.C0djQt.mount: Deactivated successfully.
Mar 12 22:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:30:40.335320    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:30:40.346615    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:32:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.iURCDJ.mount: Deactivated successfully.
Mar 12 22:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:35:40.334894    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:35:40.345446    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:37:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.0N5R2Y.mount: Deactivated successfully.
Mar 12 22:40:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VT7S1x.mount: Deactivated successfully.
Mar 12 22:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:40:40.334898    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:40:40.345402    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:45:40.335561    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:45:40.345214    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:50:40.336296    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:50:40.349114    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 22:53:29 linbit1 systemd[1]: Starting Daily apt download activities...
Mar 12 22:53:59 linbit1 systemd-networkd-wait-online[1587582]: Timeout occurred while waiting for network connectivity.
Mar 12 22:53:59 linbit1 apt-helper[1587579]: E: Sub-process /lib/systemd/systemd-networkd-wait-online returned an error code (1)
Mar 12 22:54:00 linbit1 systemd[1]: apt-daily.service: Deactivated successfully.
Mar 12 22:54:00 linbit1 systemd[1]: Finished Daily apt download activities.
Mar 12 22:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 22:55:40.335411    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 22:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 22:55:40.346472    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:00:40.335401    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:00:40.346495    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:05:40.335387    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:05:40.346364    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:06:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.CElBHD.mount: Deactivated successfully.
Mar 12 23:07:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FjfVC6.mount: Deactivated successfully.
Mar 12 23:08:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.EAtoXK.mount: Deactivated successfully.
Mar 12 23:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:10:40.335439    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:10:40.346767    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:15:40.334470    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:15:40.344882    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:15:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.t4bMl5.mount: Deactivated successfully.
Mar 12 23:17:01 linbit1 CRON[1603083]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 12 23:17:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SUNMLu.mount: Deactivated successfully.
Mar 12 23:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:20:40.335069    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:20:40.345604    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:25:40.335040    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:25:40.350409    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:26:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TWogfj.mount: Deactivated successfully.
Mar 12 23:26:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SPrh4G.mount: Deactivated successfully.
Mar 12 23:27:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.nOLOgx.mount: Deactivated successfully.
Mar 12 23:28:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.rIQH6S.mount: Deactivated successfully.
Mar 12 23:28:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3C0RJN.mount: Deactivated successfully.
Mar 12 23:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:30:40.334689    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:30:40.345148    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:31:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.exYFtJ.mount: Deactivated successfully.
Mar 12 23:33:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.g8zdAW.mount: Deactivated successfully.
Mar 12 23:33:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.OqfkwK.mount: Deactivated successfully.
Mar 12 23:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:35:40.334612    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:35:40.346177    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:37:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.8HWxzm.mount: Deactivated successfully.
Mar 12 23:39:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.0l9CjV.mount: Deactivated successfully.
Mar 12 23:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:40:40.335445    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:40:40.346667    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:41:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.V4raQQ.mount: Deactivated successfully.
Mar 12 23:41:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ofYjWF.mount: Deactivated successfully.
Mar 12 23:42:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.yh3PsA.mount: Deactivated successfully.
Mar 12 23:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:45:40.334478    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:45:40.346190    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:47:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TmE9bB.mount: Deactivated successfully.
Mar 12 23:49:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FPB33V.mount: Deactivated successfully.
Mar 12 23:49:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.wqT2X4.mount: Deactivated successfully.
Mar 12 23:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:50:40.335531    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:50:40.346487    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:53:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UkYVmD.mount: Deactivated successfully.
Mar 12 23:53:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VeICYt.mount: Deactivated successfully.
Mar 12 23:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0312 23:55:40.335011    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 12 23:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0312 23:55:40.345187    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 12 23:57:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UXtiVT.mount: Deactivated successfully.
Mar 13 00:00:01 linbit1 systemd[1]: Starting Daily dpkg database backup service...
Mar 13 00:00:01 linbit1 systemd[1]: Starting Rotate log files...
Mar 13 00:00:01 linbit1 systemd[1]: dpkg-db-backup.service: Deactivated successfully.
Mar 13 00:00:01 linbit1 systemd[1]: Finished Daily dpkg database backup service.
Mar 13 00:00:01 linbit1 systemd[1]: logrotate.service: Deactivated successfully.
Mar 13 00:00:01 linbit1 systemd[1]: Finished Rotate log files.
Mar 13 00:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:00:40.334798    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:00:40.344450    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:05:40.334425    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:05:40.345238    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:10:40.335394    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:10:40.345282    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:11:12 linbit1 systemd[1]: Starting Discard unused blocks on filesystems from /etc/fstab...
Mar 13 00:11:17 linbit1 fstrim[1638618]: /boot/efi: 1 GiB (1119653888 bytes) trimmed on /dev/disk/by-uuid/219C-5A79
Mar 13 00:11:17 linbit1 fstrim[1638618]: /: 853.7 GiB (916686807040 bytes) trimmed on /dev/disk/by-uuid/e8a4f461-93e7-4613-973a-5f4c8460a7ef
Mar 13 00:11:17 linbit1 systemd[1]: fstrim.service: Deactivated successfully.
Mar 13 00:11:17 linbit1 systemd[1]: Finished Discard unused blocks on filesystems from /etc/fstab.
Mar 13 00:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:15:40.334894    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:15:40.345351    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:16:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.HLF9AN.mount: Deactivated successfully.
Mar 13 00:17:01 linbit1 CRON[1642430]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 00:17:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.BvfMKm.mount: Deactivated successfully.
Mar 13 00:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:20:40.334116    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:20:40.346223    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:25:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.PZ0EPm.mount: Deactivated successfully.
Mar 13 00:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:25:40.334609    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:25:40.345314    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:30:40.334857    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:30:40.345606    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:31:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.hcbGBT.mount: Deactivated successfully.
Mar 13 00:32:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TS7hOS.mount: Deactivated successfully.
Mar 13 00:32:14 linbit1 systemd[1]: Starting Download data for packages that failed at package install time...
Mar 13 00:32:14 linbit1 systemd[1]: update-notifier-download.service: Deactivated successfully.
Mar 13 00:32:14 linbit1 systemd[1]: Finished Download data for packages that failed at package install time.
Mar 13 00:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:35:40.335345    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:35:40.345338    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:38:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.QXNom8.mount: Deactivated successfully.
Mar 13 00:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:40:40.334717    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:40:40.345409    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:41:57 linbit1 systemd[1]: Starting Cleanup of Temporary Directories...
Mar 13 00:41:57 linbit1 systemd[1]: systemd-tmpfiles-clean.service: Deactivated successfully.
Mar 13 00:41:57 linbit1 systemd[1]: Finished Cleanup of Temporary Directories.
Mar 13 00:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:45:40.335499    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:45:40.344995    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:50:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.qoXDhy.mount: Deactivated successfully.
Mar 13 00:50:06 linbit1 systemd[1]: Starting Daily man-db regeneration...
Mar 13 00:50:06 linbit1 systemd[1]: man-db.service: Deactivated successfully.
Mar 13 00:50:06 linbit1 systemd[1]: Finished Daily man-db regeneration.
Mar 13 00:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:50:40.335579    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:50:40.346590    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 00:55:40.334441    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 00:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 00:55:40.344952    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 00:59:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Ui4sgf.mount: Deactivated successfully.
Mar 13 01:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:00:40.335960    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:00:40.346372    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:05:40.334614    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:05:40.346358    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:06:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.AckQPZ.mount: Deactivated successfully.
Mar 13 01:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:10:40.334409    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:10:40.345546    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:15:40.334780    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:15:40.345637    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:16:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ta1hty.mount: Deactivated successfully.
Mar 13 01:17:01 linbit1 CRON[1681785]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 01:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:20:40.335099    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:20:40.347597    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:21:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.jUC67n.mount: Deactivated successfully.
Mar 13 01:21:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6VyybS.mount: Deactivated successfully.
Mar 13 01:22:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.hodXTe.mount: Deactivated successfully.
Mar 13 01:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:25:40.334910    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:25:40.347703    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:26:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.1Vf1Jg.mount: Deactivated successfully.
Mar 13 01:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:30:40.334586    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:30:40.345118    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:35:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.nYuM4l.mount: Deactivated successfully.
Mar 13 01:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:35:40.335001    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:35:40.345473    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:39:37 linbit1 snapd[2460]: storehelpers.go:769: cannot refresh: snap has no updates available: "core18", "core20", "lxd", "microk8s", "snapd"
Mar 13 01:39:37 linbit1 snapd[2460]: autorefresh.go:551: auto-refresh: all snaps are up-to-date
Mar 13 01:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:40:40.334991    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:40:40.350286    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:45:40.335472    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:45:40.345764    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:50:40.335477    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:50:40.346682    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 01:50:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.xtwAmc.mount: Deactivated successfully.
Mar 13 01:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 01:55:40.335035    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 01:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 01:55:40.345239    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:00:40.334920    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:00:40.345287    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:05:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.e6OYul.mount: Deactivated successfully.
Mar 13 02:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:05:40.334446    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:05:40.344007    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:10:40.335666    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:10:40.346069    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:12:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.CnQ44o.mount: Deactivated successfully.
Mar 13 02:13:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.jtbZgg.mount: Deactivated successfully.
Mar 13 02:13:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.iUvJM7.mount: Deactivated successfully.
Mar 13 02:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:15:40.335520    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:15:40.347676    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:16:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.0NRd4L.mount: Deactivated successfully.
Mar 13 02:17:01 linbit1 CRON[1721196]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 02:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:20:40.335334    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:20:40.344069    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:24:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.lBuJSt.mount: Deactivated successfully.
Mar 13 02:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:25:40.334660    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:25:40.345196    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:29:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.pHIuwD.mount: Deactivated successfully.
Mar 13 02:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:30:40.335265    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:30:40.346756    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:35:40.335337    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:35:40.345778    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:40:40.335274    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:40:40.345530    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:42:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.H8MIWa.mount: Deactivated successfully.
Mar 13 02:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:45:40.335799    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:45:40.348926    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:48:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.7POfYN.mount: Deactivated successfully.
Mar 13 02:49:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.wHD1sq.mount: Deactivated successfully.
Mar 13 02:50:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.KmGvtU.mount: Deactivated successfully.
Mar 13 02:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:50:40.335117    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:50:40.348540    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 02:50:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JjPce5.mount: Deactivated successfully.
Mar 13 02:50:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5hWI5b.mount: Deactivated successfully.
Mar 13 02:51:21 linbit1 systemd[1]: Starting Ubuntu Advantage Timer for running repeated jobs...
Mar 13 02:51:21 linbit1 systemd[1]: ua-timer.service: Deactivated successfully.
Mar 13 02:51:21 linbit1 systemd[1]: Finished Ubuntu Advantage Timer for running repeated jobs.
Mar 13 02:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 02:55:40.335817    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 02:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 02:55:40.345448    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:00:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.XsrR5s.mount: Deactivated successfully.
Mar 13 03:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:00:40.335133    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:00:40.345555    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:03:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ZQ5yB7.mount: Deactivated successfully.
Mar 13 03:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:05:40.334583    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:05:40.344813    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:07:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VOAx5c.mount: Deactivated successfully.
Mar 13 03:10:01 linbit1 CRON[1755987]: (root) CMD (test -e /run/systemd/system || SERVICE_MODE=1 /sbin/e2scrub_all -A -r)
Mar 13 03:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:10:40.335588    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:10:40.345105    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:10:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.fuCSNH.mount: Deactivated successfully.
Mar 13 03:12:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.vwyleT.mount: Deactivated successfully.
Mar 13 03:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:15:40.335230    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:15:40.345136    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:17:01 linbit1 CRON[1760551]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 03:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:20:40.335486    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:20:40.347516    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:23:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.1AgFCD.mount: Deactivated successfully.
Mar 13 03:24:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.HsWzv5.mount: Deactivated successfully.
Mar 13 03:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:25:40.335420    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:25:40.345270    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:27:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.qIMc7U.mount: Deactivated successfully.
Mar 13 03:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:30:40.334936    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:30:40.346280    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:35:40.335288    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:35:40.346989    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:36:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ebu33b.mount: Deactivated successfully.
Mar 13 03:38:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.vF5T9j.mount: Deactivated successfully.
Mar 13 03:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:40:40.334062    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:40:40.344014    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:45:40.335353    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:45:40.346190    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:50:40.334844    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:50:40.344439    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 03:52:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.at52Fz.mount: Deactivated successfully.
Mar 13 03:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 03:55:40.334891    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 03:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 03:55:40.345477    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:00:40.334908    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:00:40.344527    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:05:40.334749    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:05:40.348730    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:07:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.EQUt7W.mount: Deactivated successfully.
Mar 13 04:07:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.vd84na.mount: Deactivated successfully.
Mar 13 04:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:10:40.334800    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:10:40.344292    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:11:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.X64WML.mount: Deactivated successfully.
Mar 13 04:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:15:40.335176    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:15:40.346410    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:17:01 linbit1 CRON[1799876]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 04:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:20:40.334963    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:20:40.345254    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:21:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.oRtdzw.mount: Deactivated successfully.
Mar 13 04:24:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.T2WV5b.mount: Deactivated successfully.
Mar 13 04:25:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.qFlJ0b.mount: Deactivated successfully.
Mar 13 04:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:25:40.334554    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:25:40.348145    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:30:40.335294    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:30:40.345249    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:35:40.334709    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:35:40.345202    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:40:40.334703    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:40:40.346641    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:45:40.335024    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:45:40.345053    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:50:40.335258    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:50:40.348560    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:52:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.OCLS2p.mount: Deactivated successfully.
Mar 13 04:53:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MTk32p.mount: Deactivated successfully.
Mar 13 04:53:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.KoqLN6.mount: Deactivated successfully.
Mar 13 04:54:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.J3DCke.mount: Deactivated successfully.
Mar 13 04:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 04:55:40.335504    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 04:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 04:55:40.346091    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 04:58:00 linbit1 systemd[1]: Starting Message of the Day...
Mar 13 04:58:01 linbit1 50-motd-news[1826796]:  * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
Mar 13 04:58:01 linbit1 50-motd-news[1826796]:    just raised the bar for easy, resilient and secure K8s cluster deployment.
Mar 13 04:58:01 linbit1 50-motd-news[1826796]:    https://ubuntu.com/engage/secure-kubernetes-at-the-edge
Mar 13 04:58:01 linbit1 systemd[1]: motd-news.service: Deactivated successfully.
Mar 13 04:58:01 linbit1 systemd[1]: Finished Message of the Day.
Mar 13 05:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:00:40.335265    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:00:40.346228    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:05:40.334934    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:05:40.344720    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:10:40.334539    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:10:40.344335    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:15:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.E5ccLr.mount: Deactivated successfully.
Mar 13 05:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:15:40.335623    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:15:40.345989    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:16:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.iyvVfg.mount: Deactivated successfully.
Mar 13 05:17:01 linbit1 CRON[1839195]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 05:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:20:40.334550    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:20:40.344082    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:22:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JF74WS.mount: Deactivated successfully.
Mar 13 05:25:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.F9Q5OR.mount: Deactivated successfully.
Mar 13 05:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:25:40.335111    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:25:40.348226    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:30:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.1iTci4.mount: Deactivated successfully.
Mar 13 05:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:30:40.335050    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:30:40.347045    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:35:40.334999    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:35:40.346562    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:40:40.335336    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:40:40.345627    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:41:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.CLNa1h.mount: Deactivated successfully.
Mar 13 05:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:45:40.335218    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:45:40.346697    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:50:40.334798    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:50:40.345308    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 05:54:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.nP5TUb.mount: Deactivated successfully.
Mar 13 05:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 05:55:40.334808    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 05:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 05:55:40.346586    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:00:40.335584    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:00:40.345623    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:01:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.plo0XQ.mount: Deactivated successfully.
Mar 13 06:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:05:40.335656    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:05:40.346736    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:10:40.335295    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:10:40.348406    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:15:40.335457    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:15:40.345112    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:17:01 linbit1 CRON[1878517]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 06:20:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SK84lP.mount: Deactivated successfully.
Mar 13 06:20:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.q0xANb.mount: Deactivated successfully.
Mar 13 06:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:20:40.334715    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:20:40.345437    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:25:01 linbit1 CRON[1883750]: (root) CMD (test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.daily ))
Mar 13 06:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:25:40.334991    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:25:40.348528    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:27:22 linbit1 systemd[1]: Starting Daily apt upgrade and clean activities...
Mar 13 06:27:52 linbit1 systemd-networkd-wait-online[1885308]: Timeout occurred while waiting for network connectivity.
Mar 13 06:27:52 linbit1 apt-helper[1885305]: E: Sub-process /lib/systemd/systemd-networkd-wait-online returned an error code (1)
Mar 13 06:27:54 linbit1 systemd[1]: apt-daily-upgrade.service: Deactivated successfully.
Mar 13 06:27:54 linbit1 systemd[1]: Finished Daily apt upgrade and clean activities.
Mar 13 06:27:54 linbit1 systemd[1]: apt-daily-upgrade.service: Consumed 2.513s CPU time.
Mar 13 06:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:30:40.335348    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:30:40.346338    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:35:40.335300    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:35:40.345558    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:36:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TGz54M.mount: Deactivated successfully.
Mar 13 06:39:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.06Lqsx.mount: Deactivated successfully.
Mar 13 06:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:40:40.334713    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:40:40.345006    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:42:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.O4jUZD.mount: Deactivated successfully.
Mar 13 06:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:45:40.335155    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:45:40.345755    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:50:40.335589    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:50:40.347641    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 06:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 06:55:40.335211    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 06:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 06:55:40.346331    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:00:40.335238    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:00:40.345928    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:03:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ZZ1CZq.mount: Deactivated successfully.
Mar 13 07:03:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.D1iyRh.mount: Deactivated successfully.
Mar 13 07:04:29 linbit1 systemd[1]: Starting Check to see whether there is a new version of Ubuntu available...
Mar 13 07:04:29 linbit1 systemd[1]: update-notifier-motd.service: Deactivated successfully.
Mar 13 07:04:29 linbit1 systemd[1]: Finished Check to see whether there is a new version of Ubuntu available.
Mar 13 07:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:05:40.334896    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:05:40.349988    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:06:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.zgmQjh.mount: Deactivated successfully.
Mar 13 07:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:10:40.334759    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:10:40.345301    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:15:40.335462    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:15:40.347479    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:17:01 linbit1 CRON[1918043]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 07:19:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.mhNQto.mount: Deactivated successfully.
Mar 13 07:19:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.t5zbZs.mount: Deactivated successfully.
Mar 13 07:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:20:40.334951    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:20:40.345587    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:25:40.335561    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:25:40.345452    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:29:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JzBYRa.mount: Deactivated successfully.
Mar 13 07:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:30:40.335321    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:30:40.346740    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:33:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.F3UoyH.mount: Deactivated successfully.
Mar 13 07:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:35:40.335266    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:35:40.345264    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:36:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3SkGUY.mount: Deactivated successfully.
Mar 13 07:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:40:40.334844    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:40:40.344897    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:45:40.335479    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:45:40.344989    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:47:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.muPDOR.mount: Deactivated successfully.
Mar 13 07:48:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.BTMNXC.mount: Deactivated successfully.
Mar 13 07:48:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.IzEQgk.mount: Deactivated successfully.
Mar 13 07:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:50:40.335048    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:50:40.345695    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 07:55:40.334666    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 07:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 07:55:40.348105    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 07:57:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UtDXws.mount: Deactivated successfully.
Mar 13 08:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:00:40.334966    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:00:40.345368    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:05:40.334658    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:05:40.345290    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:10:40.334784    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:10:40.346172    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:15:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.p7vJ5w.mount: Deactivated successfully.
Mar 13 08:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:15:40.335479    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:15:40.346552    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:17:01 linbit1 CRON[1957309]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 08:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:20:40.334813    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:20:40.345043    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:25:40.334939    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:25:40.345511    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:30:40.335120    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:30:40.344601    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:33:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.lOg4Is.mount: Deactivated successfully.
Mar 13 08:34:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.4Vav9r.mount: Deactivated successfully.
Mar 13 08:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:35:40.334970    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:35:40.344984    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:36:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.jxqlFX.mount: Deactivated successfully.
Mar 13 08:36:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.U1jqbA.mount: Deactivated successfully.
Mar 13 08:39:37 linbit1 snapd[2460]: storehelpers.go:769: cannot refresh: snap has no updates available: "core18", "core20", "lxd", "microk8s", "snapd"
Mar 13 08:39:37 linbit1 snapd[2460]: autorefresh.go:551: auto-refresh: all snaps are up-to-date
Mar 13 08:39:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.y7VAoI.mount: Deactivated successfully.
Mar 13 08:40:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5Cfjge.mount: Deactivated successfully.
Mar 13 08:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:40:40.334798    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:40:40.344806    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:45:40.335255    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:45:40.346077    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:48:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Ud4Qib.mount: Deactivated successfully.
Mar 13 08:50:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.kr3Pqx.mount: Deactivated successfully.
Mar 13 08:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:50:40.335105    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:50:40.345296    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:54:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.wDblNB.mount: Deactivated successfully.
Mar 13 08:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 08:55:40.334489    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 08:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 08:55:40.346289    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 08:56:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ru5I4L.mount: Deactivated successfully.
Mar 13 09:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:00:40.335577    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:00:40.346078    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:05:40.335097    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:05:40.345750    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:10:40.335047    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:10:40.348026    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:15:40.335439    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:15:40.345932    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:17:01 linbit1 CRON[1996666]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 09:17:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UlkQf6.mount: Deactivated successfully.
Mar 13 09:17:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.fXPXuj.mount: Deactivated successfully.
Mar 13 09:19:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.wRassX.mount: Deactivated successfully.
Mar 13 09:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:20:40.335591    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:20:40.348174    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:25:40.335382    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:25:40.344843    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:30:40.335158    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:30:40.344936    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:31:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.7CcsxM.mount: Deactivated successfully.
Mar 13 09:32:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.9d8Ujc.mount: Deactivated successfully.
Mar 13 09:33:27 linbit1 systemd[1]: Starting Ubuntu Advantage Timer for running repeated jobs...
Mar 13 09:33:27 linbit1 systemd[1]: ua-timer.service: Deactivated successfully.
Mar 13 09:33:27 linbit1 systemd[1]: Finished Ubuntu Advantage Timer for running repeated jobs.
Mar 13 09:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:35:40.335157    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:35:40.346615    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:38:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.X1KOM6.mount: Deactivated successfully.
Mar 13 09:38:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Q4n6gU.mount: Deactivated successfully.
Mar 13 09:39:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.K4QNxD.mount: Deactivated successfully.
Mar 13 09:40:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SPyatI.mount: Deactivated successfully.
Mar 13 09:40:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.iFqsrq.mount: Deactivated successfully.
Mar 13 09:40:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5ZsEQn.mount: Deactivated successfully.
Mar 13 09:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:40:40.334973    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:40:40.344843    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:45:40.335161    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:45:40.347724    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:50:40.335025    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:50:40.345681    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 09:53:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.DuZ7zz.mount: Deactivated successfully.
Mar 13 09:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 09:55:40.334914    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 09:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 09:55:40.344537    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:00:40.335121    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:00:40.345326    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:00:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Gs9L0z.mount: Deactivated successfully.
Mar 13 10:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:05:40.335448    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:05:40.347597    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:10:40.335173    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:10:40.345489    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:15:40.335136    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:15:40.344795    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:16:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.TOjQF0.mount: Deactivated successfully.
Mar 13 10:17:01 linbit1 CRON[2036012]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 10:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:20:40.335043    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:20:40.345346    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:25:40.334690    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:25:40.345295    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:30:40.334612    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:30:40.344926    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:32:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.adcJCL.mount: Deactivated successfully.
Mar 13 10:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:35:40.334597    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:35:40.345258    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:40:40.334630    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:40:40.345147    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:42:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5XncKR.mount: Deactivated successfully.
Mar 13 10:42:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.xNDOh6.mount: Deactivated successfully.
Mar 13 10:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:45:40.335181    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:45:40.344834    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:47:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.JmF2oe.mount: Deactivated successfully.
Mar 13 10:47:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.sx5KrO.mount: Deactivated successfully.
Mar 13 10:49:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.m9B3tU.mount: Deactivated successfully.
Mar 13 10:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:50:40.334844    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:50:40.344502    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:54:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.NpNWTK.mount: Deactivated successfully.
Mar 13 10:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 10:55:40.334928    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 10:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 10:55:40.345511    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 10:56:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UrWX2E.mount: Deactivated successfully.
Mar 13 11:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:00:40.335663    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:00:40.348147    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:03:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.HnjlHI.mount: Deactivated successfully.
Mar 13 11:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:05:40.334485    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:05:40.345004    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:10:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.EiMUDH.mount: Deactivated successfully.
Mar 13 11:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:10:40.335448    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:10:40.348070    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:15:40.335644    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:15:40.347697    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:16:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.RtkRpZ.mount: Deactivated successfully.
Mar 13 11:17:01 linbit1 CRON[2075350]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 11:17:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.0NDjrd.mount: Deactivated successfully.
Mar 13 11:18:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LYJzjC.mount: Deactivated successfully.
Mar 13 11:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:20:40.334679    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:20:40.347006    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:21:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.nvhDPI.mount: Deactivated successfully.
Mar 13 11:22:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.tc7vuM.mount: Deactivated successfully.
Mar 13 11:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:25:40.335415    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:25:40.345926    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:30:40.334775    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:30:40.344541    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:35:40.335279    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:35:40.347713    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:39:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.BhKHPt.mount: Deactivated successfully.
Mar 13 11:40:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.kugewK.mount: Deactivated successfully.
Mar 13 11:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:40:40.334404    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:40:40.344496    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:41:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6RH1Zy.mount: Deactivated successfully.
Mar 13 11:42:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.HDuHrc.mount: Deactivated successfully.
Mar 13 11:45:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.rbitwV.mount: Deactivated successfully.
Mar 13 11:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:45:40.335579    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:45:40.345545    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:48:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.zdim3M.mount: Deactivated successfully.
Mar 13 11:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:50:40.335541    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:50:40.345302    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 11:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 11:55:40.334417    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 11:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 11:55:40.344795    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:00:40.335316    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:00:40.350528    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:04:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.9Wz8T6.mount: Deactivated successfully.
Mar 13 12:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:05:40.335264    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:05:40.346062    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:09:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.lMQ9RU.mount: Deactivated successfully.
Mar 13 12:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:10:40.335352    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:10:40.345611    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:15:40.335349    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:15:40.345513    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:17:01 linbit1 CRON[2114652]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 12:17:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.9L81nX.mount: Deactivated successfully.
Mar 13 12:20:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.IQASuL.mount: Deactivated successfully.
Mar 13 12:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:20:40.335382    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:20:40.345153    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:25:40.334428    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:25:40.345470    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:30:40.335463    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:30:40.345570    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:35:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UJIuqr.mount: Deactivated successfully.
Mar 13 12:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:35:40.334619    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:35:40.346943    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:37:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SNHCP4.mount: Deactivated successfully.
Mar 13 12:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:40:40.335329    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:40:40.346609    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:41:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SdFXNi.mount: Deactivated successfully.
Mar 13 12:41:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.H3rLcp.mount: Deactivated successfully.
Mar 13 12:43:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.tF0yLN.mount: Deactivated successfully.
Mar 13 12:44:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.xfmvYr.mount: Deactivated successfully.
Mar 13 12:45:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.C3pAyN.mount: Deactivated successfully.
Mar 13 12:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:45:40.335115    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:45:40.347094    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:46:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.IIKEE1.mount: Deactivated successfully.
Mar 13 12:48:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.HQMFQD.mount: Deactivated successfully.
Mar 13 12:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:50:40.335430    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:50:40.345890    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 12:55:40.334864    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 12:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 12:55:40.346947    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 12:59:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.oDuJXX.mount: Deactivated successfully.
Mar 13 13:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:00:40.335521    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:00:40.346632    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:03:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.n77x5a.mount: Deactivated successfully.
Mar 13 13:03:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.usptuA.mount: Deactivated successfully.
Mar 13 13:04:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.M85ltl.mount: Deactivated successfully.
Mar 13 13:04:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3ZqlDq.mount: Deactivated successfully.
Mar 13 13:04:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.CkarTM.mount: Deactivated successfully.
Mar 13 13:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:05:40.334911    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:05:40.347532    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:10:40.335407    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:10:40.347089    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:11:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.1SaIQw.mount: Deactivated successfully.
Mar 13 13:11:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.9BUDHV.mount: Deactivated successfully.
Mar 13 13:12:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LwCrjC.mount: Deactivated successfully.
Mar 13 13:12:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.BvjNyR.mount: Deactivated successfully.
Mar 13 13:12:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.vRyVfB.mount: Deactivated successfully.
Mar 13 13:12:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.WHR1VO.mount: Deactivated successfully.
Mar 13 13:14:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.f2AnXN.mount: Deactivated successfully.
Mar 13 13:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:15:40.334876    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:15:40.345359    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:17:01 linbit1 CRON[2153936]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 13:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:20:40.335390    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:20:40.345815    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:22:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VMeLAz.mount: Deactivated successfully.
Mar 13 13:25:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SHbPbP.mount: Deactivated successfully.
Mar 13 13:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:25:40.335155    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:25:40.344833    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:25:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Ku1UG2.mount: Deactivated successfully.
Mar 13 13:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:30:40.334674    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:30:40.344168    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:30:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.424GBD.mount: Deactivated successfully.
Mar 13 13:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:35:40.335533    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:35:40.345257    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:36:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.zD7xjO.mount: Deactivated successfully.
Mar 13 13:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:40:40.335009    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:40:40.344641    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:42:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.cDDSoC.mount: Deactivated successfully.
Mar 13 13:42:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.RQPyAm.mount: Deactivated successfully.
Mar 13 13:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:45:40.335250    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:45:40.344632    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:50:40.334758    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:50:40.344299    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:52:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.YbqbHD.mount: Deactivated successfully.
Mar 13 13:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 13:55:40.335138    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 13:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 13:55:40.345452    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 13:57:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.mbsUK1.mount: Deactivated successfully.
Mar 13 14:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:00:40.335501    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:00:40.347741    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:05:40.334958    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:05:40.345473    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:10:40.335292    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:10:40.346173    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:15:40.335334    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:15:40.346160    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:17:01 linbit1 CRON[2193379]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 14:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:20:40.334999    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:20:40.348883    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:23:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ps0l3Q.mount: Deactivated successfully.
Mar 13 14:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:25:40.334806    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:25:40.345286    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:30:40.334764    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:30:40.345333    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:35:40.335526    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:35:40.345571    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:38:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.adUAOe.mount: Deactivated successfully.
Mar 13 14:40:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.e0PZ60.mount: Deactivated successfully.
Mar 13 14:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:40:40.335388    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:40:40.345185    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:41:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.qkz3mN.mount: Deactivated successfully.
Mar 13 14:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:45:40.334929    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:45:40.345595    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:49:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LkYP4N.mount: Deactivated successfully.
Mar 13 14:49:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SiLsIX.mount: Deactivated successfully.
Mar 13 14:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:50:40.334263    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:50:40.344654    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 14:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 14:55:40.334768    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 14:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 14:55:40.345365    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:00:40.335565    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:00:40.346810    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:05:40.335455    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:05:40.347811    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:07:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MIBKLk.mount: Deactivated successfully.
Mar 13 15:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:10:40.335442    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:10:40.345971    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:13:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.lyNDZ0.mount: Deactivated successfully.
Mar 13 15:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:15:40.334465    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:15:40.345493    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:17:01 linbit1 CRON[2232643]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 15:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:20:40.334812    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:20:40.344529    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:24:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.rwrWGy.mount: Deactivated successfully.
Mar 13 15:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:25:40.335059    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:25:40.344632    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:28:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.mn2Uh4.mount: Deactivated successfully.
Mar 13 15:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:30:40.335354    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:30:40.348239    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:31:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.dIFuF2.mount: Deactivated successfully.
Mar 13 15:32:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.vQGpCf.mount: Deactivated successfully.
Mar 13 15:34:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.0EpMIi.mount: Deactivated successfully.
Mar 13 15:35:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.1UQ4lq.mount: Deactivated successfully.
Mar 13 15:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:35:40.335065    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:35:40.346737    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:40:40.335321    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:40:40.346160    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:45:40.335373    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:45:40.345154    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:47:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.KdwgA5.mount: Deactivated successfully.
Mar 13 15:47:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.yb0qRO.mount: Deactivated successfully.
Mar 13 15:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:50:40.335100    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:50:40.345204    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:52:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VcnXiK.mount: Deactivated successfully.
Mar 13 15:54:20 linbit1 systemd[1]: Starting Ubuntu Advantage Timer for running repeated jobs...
Mar 13 15:54:20 linbit1 systemd[1]: ua-timer.service: Deactivated successfully.
Mar 13 15:54:20 linbit1 systemd[1]: Finished Ubuntu Advantage Timer for running repeated jobs.
Mar 13 15:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 15:55:40.335336    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 15:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 15:55:40.344864    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 15:55:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.db6A1S.mount: Deactivated successfully.
Mar 13 15:56:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.tTxSE8.mount: Deactivated successfully.
Mar 13 15:59:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.7m3mCP.mount: Deactivated successfully.
Mar 13 16:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:00:40.335113    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:00:40.345005    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:03:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.CnmDUJ.mount: Deactivated successfully.
Mar 13 16:04:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.sJ4dnO.mount: Deactivated successfully.
Mar 13 16:04:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.j3C4ui.mount: Deactivated successfully.
Mar 13 16:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:05:40.334931    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:05:40.344584    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:05:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.6MVJAC.mount: Deactivated successfully.
Mar 13 16:06:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.r2ZymY.mount: Deactivated successfully.
Mar 13 16:07:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ABWz4o.mount: Deactivated successfully.
Mar 13 16:07:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5MwFyL.mount: Deactivated successfully.
Mar 13 16:09:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ZFb0Ke.mount: Deactivated successfully.
Mar 13 16:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:10:40.334750    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:10:40.344391    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:13:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.peuWxb.mount: Deactivated successfully.
Mar 13 16:14:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.7yzCU6.mount: Deactivated successfully.
Mar 13 16:14:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.0dWWgk.mount: Deactivated successfully.
Mar 13 16:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:15:40.334547    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:15:40.344249    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:17:01 linbit1 CRON[2272002]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 16:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:20:40.335185    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:20:40.348449    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:21:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.AvqooY.mount: Deactivated successfully.
Mar 13 16:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:25:40.335253    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:25:40.347279    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:30:40.335233    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:30:40.347220    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:32:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FPnkvw.mount: Deactivated successfully.
Mar 13 16:32:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.7wSZiZ.mount: Deactivated successfully.
Mar 13 16:34:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.zSibHt.mount: Deactivated successfully.
Mar 13 16:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:35:40.335085    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:35:40.346628    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:40:40.334927    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:40:40.345300    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:42:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.sc3JL0.mount: Deactivated successfully.
Mar 13 16:43:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.WOOv5K.mount: Deactivated successfully.
Mar 13 16:44:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3nVA1F.mount: Deactivated successfully.
Mar 13 16:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:45:40.335004    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:45:40.345349    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:47:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VqyI6N.mount: Deactivated successfully.
Mar 13 16:49:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.KcYAqp.mount: Deactivated successfully.
Mar 13 16:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:50:40.335103    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:50:40.344865    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:51:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.teb2dd.mount: Deactivated successfully.
Mar 13 16:52:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.eimNrW.mount: Deactivated successfully.
Mar 13 16:54:12 linbit1 systemd[1]: Starting Message of the Day...
Mar 13 16:54:18 linbit1 50-motd-news[2296498]:  * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
Mar 13 16:54:18 linbit1 50-motd-news[2296498]:    just raised the bar for easy, resilient and secure K8s cluster deployment.
Mar 13 16:54:18 linbit1 50-motd-news[2296498]:    https://ubuntu.com/engage/secure-kubernetes-at-the-edge
Mar 13 16:54:18 linbit1 systemd[1]: motd-news.service: Deactivated successfully.
Mar 13 16:54:18 linbit1 systemd[1]: Finished Message of the Day.
Mar 13 16:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 16:55:40.335327    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 16:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 16:55:40.345915    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 16:57:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.quzVzj.mount: Deactivated successfully.
Mar 13 16:59:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.SMRvvY.mount: Deactivated successfully.
Mar 13 17:00:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.HwYo0p.mount: Deactivated successfully.
Mar 13 17:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:00:40.335202    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:00:40.347078    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:00:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.M7PTgb.mount: Deactivated successfully.
Mar 13 17:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:05:40.334789    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:05:40.344447    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:10:40.334985    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:10:40.345218    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:13:12 linbit1 systemd[1]: Starting Refresh fwupd metadata and update motd...
Mar 13 17:13:12 linbit1 systemd[1]: fwupd-refresh.service: Main process exited, code=exited, status=1/FAILURE
Mar 13 17:13:12 linbit1 systemd[1]: fwupd-refresh.service: Failed with result 'exit-code'.
Mar 13 17:13:12 linbit1 systemd[1]: Failed to start Refresh fwupd metadata and update motd.
Mar 13 17:15:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.E0WAAI.mount: Deactivated successfully.
Mar 13 17:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:15:40.335389    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:15:40.345375    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:17:01 linbit1 CRON[2311327]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 17:17:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.W2noGX.mount: Deactivated successfully.
Mar 13 17:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:20:40.334747    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:20:40.345222    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:24:37 linbit1 snapd[2460]: storehelpers.go:769: cannot refresh: snap has no updates available: "core18", "core20", "lxd", "microk8s", "snapd"
Mar 13 17:24:37 linbit1 snapd[2460]: autorefresh.go:551: auto-refresh: all snaps are up-to-date
Mar 13 17:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:25:40.334946    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:25:40.345458    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:30:40.334501    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:30:40.345859    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:34:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.bRHQCO.mount: Deactivated successfully.
Mar 13 17:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:35:40.335034    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:35:40.344846    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:38:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.PdFODQ.mount: Deactivated successfully.
Mar 13 17:38:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.jUZtv4.mount: Deactivated successfully.
Mar 13 17:39:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.ByhuuA.mount: Deactivated successfully.
Mar 13 17:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:40:40.335378    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:40:40.345504    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:45:40.334514    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:45:40.346365    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:50:40.334539    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:50:40.343956    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:50:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.QDDALm.mount: Deactivated successfully.
Mar 13 17:52:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.19GyPu.mount: Deactivated successfully.
Mar 13 17:52:29 linbit1 systemd[1]: Starting Daily apt download activities...
Mar 13 17:52:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.3ax74X.mount: Deactivated successfully.
Mar 13 17:52:59 linbit1 systemd-networkd-wait-online[2334613]: Timeout occurred while waiting for network connectivity.
Mar 13 17:53:00 linbit1 apt-helper[2334610]: E: Sub-process /lib/systemd/systemd-networkd-wait-online returned an error code (1)
Mar 13 17:53:12 linbit1 systemd[1]: apt-daily.service: Deactivated successfully.
Mar 13 17:53:12 linbit1 systemd[1]: Finished Daily apt download activities.
Mar 13 17:53:12 linbit1 systemd[1]: apt-daily.service: Consumed 10.779s CPU time.
Mar 13 17:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 17:55:40.334935    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 17:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 17:55:40.345828    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 17:59:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.zIGUxJ.mount: Deactivated successfully.
Mar 13 17:59:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.sZ7hJk.mount: Deactivated successfully.
Mar 13 18:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:00:40.335209    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:00:40.345869    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:05:40.335392    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:05:40.345628    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:06:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.yY8dC3.mount: Deactivated successfully.
Mar 13 18:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:10:40.335203    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:10:40.347691    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:15:40.334895    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:15:40.347477    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:17:01 linbit1 CRON[2351036]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 18:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:20:40.334339    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:20:40.344179    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:25:40.334619    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:25:40.345325    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:30:40.335743    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:30:40.345946    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:35:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.hdR16G.mount: Deactivated successfully.
Mar 13 18:35:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.v04BBW.mount: Deactivated successfully.
Mar 13 18:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:35:40.335304    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:35:40.348759    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:40:40.335419    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:40:40.345093    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:40:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.tvDNqY.mount: Deactivated successfully.
Mar 13 18:41:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.QaEoQQ.mount: Deactivated successfully.
Mar 13 18:41:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.aaiX0j.mount: Deactivated successfully.
Mar 13 18:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:45:40.334556    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:45:40.344397    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:46:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.DTHLSM.mount: Deactivated successfully.
Mar 13 18:49:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.vFWiBF.mount: Deactivated successfully.
Mar 13 18:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:50:40.334682    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:50:40.345184    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 18:55:40.335581    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 18:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 18:55:40.345763    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 18:56:30 linbit1 systemd[1]: session-45.scope: Deactivated successfully.
Mar 13 18:56:30 linbit1 systemd[1]: session-45.scope: Consumed 1.333s CPU time.
Mar 13 18:57:27 linbit1 systemd[1]: Started Session 168 of User ampere.
Mar 13 18:59:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.l6CrPY.mount: Deactivated successfully.
Mar 13 18:59:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Ua0sBt.mount: Deactivated successfully.
Mar 13 19:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:00:40.335688    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:00:40.346545    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:03:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.5A62bB.mount: Deactivated successfully.
Mar 13 19:04:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.i8022H.mount: Deactivated successfully.
Mar 13 19:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:05:40.335368    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:05:40.346480    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:08:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LUpeSe.mount: Deactivated successfully.
Mar 13 19:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:10:40.335407    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:10:40.345577    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:12:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.U6LCCN.mount: Deactivated successfully.
Mar 13 19:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:15:40.334634    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:15:40.344974    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:17:01 linbit1 CRON[2390491]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 19:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:20:40.335280    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:20:40.345075    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:25:40.335407    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:25:40.345683    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:30:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.qJze4p.mount: Deactivated successfully.
Mar 13 19:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:30:40.334708    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:30:40.344640    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:34:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.x6MWeX.mount: Deactivated successfully.
Mar 13 19:35:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.zK4egR.mount: Deactivated successfully.
Mar 13 19:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:35:40.334504    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:35:40.346954    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:37:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.bWUXZ0.mount: Deactivated successfully.
Mar 13 19:38:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.tJpfmq.mount: Deactivated successfully.
Mar 13 19:38:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.mIpmQe.mount: Deactivated successfully.
Mar 13 19:39:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.98loCC.mount: Deactivated successfully.
Mar 13 19:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:40:40.334716    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:40:40.348718    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:41:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.UGJJsY.mount: Deactivated successfully.
Mar 13 19:42:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.WIYbwA.mount: Deactivated successfully.
Mar 13 19:44:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.MCKAV7.mount: Deactivated successfully.
Mar 13 19:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:45:40.335328    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:45:40.346971    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:49:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.9lp1Hl.mount: Deactivated successfully.
Mar 13 19:50:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.aJRzvN.mount: Deactivated successfully.
Mar 13 19:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:50:40.336038    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:50:40.352274    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 19:55:40.335015    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 19:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 19:55:40.346123    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 19:55:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.bFS66u.mount: Deactivated successfully.
Mar 13 19:57:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.DRXsKa.mount: Deactivated successfully.
Mar 13 19:58:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.R1BHsc.mount: Deactivated successfully.
Mar 13 20:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:00:40.335534    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:00:40.345319    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:02:42 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Lls8oM.mount: Deactivated successfully.
Mar 13 20:04:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.IFNKQz.mount: Deactivated successfully.
Mar 13 20:05:02 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.jLjOb4.mount: Deactivated successfully.
Mar 13 20:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:05:40.334965    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:05:40.345463    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:09:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.FatFmN.mount: Deactivated successfully.
Mar 13 20:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:10:40.335105    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:10:40.344998    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:13:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.VS1N4K.mount: Deactivated successfully.
Mar 13 20:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:15:40.335008    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:15:40.344859    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:17:01 linbit1 CRON[2429857]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 20:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:20:40.335013    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:20:40.347994    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:20:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.GHkilD.mount: Deactivated successfully.
Mar 13 20:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:25:40.335281    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:25:40.346613    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:30:40.334941    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:30:40.345900    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:31:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.LzzkhR.mount: Deactivated successfully.
Mar 13 20:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:35:40.335580    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:35:40.345886    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:36:52 linbit1 systemd[2625]: Started snap.microk8s.kubectl.6af32e51-6f14-4785-b5fe-d2c23d39761a.scope.
Mar 13 20:36:52 linbit1 systemd[2625]: Started snap.microk8s.kubectl.d0efb989-5332-4943-a5c8-0f2ad12b6a38.scope.
Mar 13 20:36:55 linbit1 systemd[2625]: Started snap.microk8s.kubectl.b36c9c70-7f79-435b-80ef-3fb4dcdee7af.scope.
Mar 13 20:36:55 linbit1 systemd[2625]: Started snap.microk8s.kubectl.87ac024c-af9f-4610-9880-079c3d7cf029.scope.
Mar 13 20:37:01 linbit1 systemd[2625]: Started snap.microk8s.kubectl.19b10bce-3a5c-472f-8e64-200c55bdc0d5.scope.
Mar 13 20:37:01 linbit1 systemd[2625]: Started snap.microk8s.kubectl.7d17f7b7-f676-420f-ba42-f711d0ff24d1.scope.
Mar 13 20:37:14 linbit1 systemd[2625]: Started snap.microk8s.kubectl.d0252eed-a870-4d8e-9f9f-0fd1595d09eb.scope.
Mar 13 20:37:14 linbit1 systemd[2625]: Started snap.microk8s.kubectl.0640f81c-073a-4bbb-b74b-13ad4a801a23.scope.
Mar 13 20:37:18 linbit1 systemd[2625]: Started snap.microk8s.kubectl.de0f8c39-c486-47e7-b34a-653835b3d9f4.scope.
Mar 13 20:37:18 linbit1 systemd[2625]: Started snap.microk8s.kubectl.b2eb00e8-e699-4eae-aa2f-0485519041a2.scope.
Mar 13 20:37:20 linbit1 systemd[2625]: Started snap.microk8s.kubectl.567ef2e0-fcb2-4a3b-96a4-65305b879e17.scope.
Mar 13 20:37:20 linbit1 systemd[2625]: Started snap.microk8s.kubectl.5b1fdb05-349c-4c7c-bc3f-c2d29b79544c.scope.
Mar 13 20:37:22 linbit1 systemd[2625]: Started snap.microk8s.kubectl.077fadf3-c6a5-44cc-89fa-eed3dd47ecdb.scope.
Mar 13 20:37:22 linbit1 systemd[2625]: Started snap.microk8s.kubectl.3f8de0ca-7d13-46a0-aff2-32cb88fb7176.scope.
Mar 13 20:37:25 linbit1 systemd[2625]: Started snap.microk8s.kubectl.b9110cd4-e33f-4cec-b2bd-95e8a55a1d2a.scope.
Mar 13 20:37:27 linbit1 systemd[2625]: snap.microk8s.kubectl.b9110cd4-e33f-4cec-b2bd-95e8a55a1d2a.scope: Consumed 5.695s CPU time.
Mar 13 20:37:29 linbit1 systemd-udevd[2444331]: dm-0: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-0' failed with exit code 1.
Mar 13 20:37:29 linbit1 systemd-udevd[2444332]: dm-1: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-1' failed with exit code 1.
Mar 13 20:37:29 linbit1 kernel: [504640.492227] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Starting worker thread (from drbdsetup [2444382])
Mar 13 20:37:29 linbit1 kernel: [504640.499064] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: meta-data IO uses: blk-bio
Mar 13 20:37:29 linbit1 kernel: [504640.499152] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: rs_discard_granularity feature disabled
Mar 13 20:37:29 linbit1 kernel: [504640.499181] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: disk( Diskless -> Attaching )
Mar 13 20:37:29 linbit1 kernel: [504640.499193] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: Maximum number of peer devices = 7
Mar 13 20:37:29 linbit1 kernel: [504640.499898] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Method to ensure write ordering: drain
Mar 13 20:37:29 linbit1 kernel: [504640.499906] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:29 linbit1 kernel: [504640.507506] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:29 linbit1 kernel: [504640.507514] drbd1000: detected capacity change from 0 to 20971520
Mar 13 20:37:29 linbit1 kernel: [504640.507518] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: size = 10 GB (10485760 KB)
Mar 13 20:37:29 linbit1 systemd-udevd[2444332]: drbd1000: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1000' failed with exit code 1.
Mar 13 20:37:29 linbit1 kernel: [504640.516530] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: recounting of set bits took additional 8ms
Mar 13 20:37:29 linbit1 kernel: [504640.516550] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: disk( Attaching -> Inconsistent )
Mar 13 20:37:29 linbit1 kernel: [504640.516556] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: attached to current UUID: 0000000000000004
Mar 13 20:37:30 linbit1 kernel: [504641.918393] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: rs_discard_granularity feature disabled
Mar 13 20:37:30 linbit1 systemd-udevd[2444332]: dm-2: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-2' failed with exit code 1.
Mar 13 20:37:31 linbit1 systemd-udevd[2444332]: dm-3: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-3' failed with exit code 1.
Mar 13 20:37:31 linbit1 kernel: [504642.192907] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Starting worker thread (from drbdsetup [2444501])
Mar 13 20:37:31 linbit1 kernel: [504642.200339] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: meta-data IO uses: blk-bio
Mar 13 20:37:31 linbit1 kernel: [504642.200427] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: rs_discard_granularity feature disabled
Mar 13 20:37:31 linbit1 kernel: [504642.200459] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: disk( Diskless -> Attaching )
Mar 13 20:37:31 linbit1 kernel: [504642.200470] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: Maximum number of peer devices = 7
Mar 13 20:37:31 linbit1 kernel: [504642.201217] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Method to ensure write ordering: drain
Mar 13 20:37:31 linbit1 kernel: [504642.201225] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:31 linbit1 kernel: [504642.208822] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:31 linbit1 kernel: [504642.208830] drbd1001: detected capacity change from 0 to 20971520
Mar 13 20:37:31 linbit1 kernel: [504642.208835] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: size = 10 GB (10485760 KB)
Mar 13 20:37:31 linbit1 systemd-udevd[2444332]: drbd1001: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1001' failed with exit code 1.
Mar 13 20:37:31 linbit1 kernel: [504642.214283] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: recounting of set bits took additional 4ms
Mar 13 20:37:31 linbit1 kernel: [504642.214294] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: disk( Attaching -> Inconsistent )
Mar 13 20:37:31 linbit1 kernel: [504642.214297] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: attached to current UUID: 0000000000000004
Mar 13 20:37:32 linbit1 systemd-udevd[2444332]: dm-4: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-4' failed with exit code 1.
Mar 13 20:37:32 linbit1 systemd-udevd[2444332]: dm-5: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-5' failed with exit code 1.
Mar 13 20:37:32 linbit1 kernel: [504643.637358] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Starting worker thread (from drbdsetup [2444651])
Mar 13 20:37:32 linbit1 kernel: [504643.688140] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: meta-data IO uses: blk-bio
Mar 13 20:37:32 linbit1 kernel: [504643.688229] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: rs_discard_granularity feature disabled
Mar 13 20:37:32 linbit1 kernel: [504643.688272] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: disk( Diskless -> Attaching )
Mar 13 20:37:32 linbit1 kernel: [504643.688284] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: Maximum number of peer devices = 7
Mar 13 20:37:32 linbit1 kernel: [504643.688982] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Method to ensure write ordering: drain
Mar 13 20:37:32 linbit1 kernel: [504643.688990] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:32 linbit1 kernel: [504643.696593] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:32 linbit1 kernel: [504643.696601] drbd1005: detected capacity change from 0 to 20971520
Mar 13 20:37:32 linbit1 kernel: [504643.696606] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: size = 10 GB (10485760 KB)
Mar 13 20:37:32 linbit1 systemd-udevd[2444332]: drbd1005: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1005' failed with exit code 1.
Mar 13 20:37:32 linbit1 kernel: [504643.701832] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: recounting of set bits took additional 4ms
Mar 13 20:37:32 linbit1 kernel: [504643.701842] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: disk( Attaching -> Inconsistent )
Mar 13 20:37:32 linbit1 kernel: [504643.701844] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: attached to current UUID: 0000000000000004
Mar 13 20:37:32 linbit1 kernel: [504643.715045] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: rs_discard_granularity feature disabled
Mar 13 20:37:32 linbit1 kernel: [504643.764976] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 1173757666 (0->-1 7683/4609)
Mar 13 20:37:32 linbit1 kernel: [504643.764986] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 1173757666 (0ms)
Mar 13 20:37:32 linbit1 kernel: [504643.764994] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: role( Secondary -> Primary )
Mar 13 20:37:32 linbit1 kernel: [504643.764997] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: disk( Inconsistent -> UpToDate )
Mar 13 20:37:32 linbit1 kernel: [504643.765068] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: size = 10 GB (10485760 KB)
Mar 13 20:37:32 linbit1 kernel: [504643.765246] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Forced to consider local data as UpToDate!
Mar 13 20:37:32 linbit1 kernel: [504643.765339] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: new current UUID: 365522F30E0CAB99 weak: FFFFFFFFFFFFFFFE
Mar 13 20:37:32 linbit1 kernel: [504643.770026] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 3216342658 (0->-1 3/2)
Mar 13 20:37:32 linbit1 kernel: [504643.770034] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 3216342658 (0ms)
Mar 13 20:37:32 linbit1 kernel: [504643.770041] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: role( Primary -> Secondary )
Mar 13 20:37:32 linbit1 kernel: [504643.774949] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 1538878253 (0->-1 3/1)
Mar 13 20:37:32 linbit1 kernel: [504643.774957] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 1538878253 (0ms)
Mar 13 20:37:32 linbit1 kernel: [504643.774964] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: role( Secondary -> Primary )
Mar 13 20:37:32 linbit1 kernel: [504643.778748] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 1083942621 (0->-1 3/2)
Mar 13 20:37:32 linbit1 kernel: [504643.778757] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 1083942621 (0ms)
Mar 13 20:37:32 linbit1 kernel: [504643.778764] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: role( Primary -> Secondary )
Mar 13 20:37:32 linbit1 systemd-udevd[2444332]: dm-6: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-6' failed with exit code 1.
Mar 13 20:37:32 linbit1 systemd-udevd[2444332]: dm-7: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-7' failed with exit code 1.
Mar 13 20:37:32 linbit1 kernel: [504644.014376] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Starting worker thread (from drbdsetup [2444771])
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: drbd1006: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1006' failed with exit code 1.
Mar 13 20:37:33 linbit1 kernel: [504644.022149] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: meta-data IO uses: blk-bio
Mar 13 20:37:33 linbit1 kernel: [504644.022223] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: rs_discard_granularity feature disabled
Mar 13 20:37:33 linbit1 kernel: [504644.022261] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: disk( Diskless -> Attaching )
Mar 13 20:37:33 linbit1 kernel: [504644.022271] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: Maximum number of peer devices = 7
Mar 13 20:37:33 linbit1 kernel: [504644.022965] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Method to ensure write ordering: drain
Mar 13 20:37:33 linbit1 kernel: [504644.022973] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:33 linbit1 kernel: [504644.030058] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:33 linbit1 kernel: [504644.030066] drbd1006: detected capacity change from 0 to 20971520
Mar 13 20:37:33 linbit1 kernel: [504644.030070] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: size = 10 GB (10485760 KB)
Mar 13 20:37:33 linbit1 kernel: [504644.038856] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: recounting of set bits took additional 4ms
Mar 13 20:37:33 linbit1 kernel: [504644.038866] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: disk( Attaching -> Inconsistent )
Mar 13 20:37:33 linbit1 kernel: [504644.038869] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: attached to current UUID: 0000000000000004
Mar 13 20:37:33 linbit1 kernel: [504644.052950] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: rs_discard_granularity feature disabled
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: dm-8: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-8' failed with exit code 1.
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: dm-9: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-9' failed with exit code 1.
Mar 13 20:37:33 linbit1 kernel: [504644.375262] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Starting worker thread (from drbdsetup [2444857])
Mar 13 20:37:33 linbit1 kernel: [504644.382879] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: meta-data IO uses: blk-bio
Mar 13 20:37:33 linbit1 kernel: [504644.382948] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: rs_discard_granularity feature disabled
Mar 13 20:37:33 linbit1 kernel: [504644.382975] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: disk( Diskless -> Attaching )
Mar 13 20:37:33 linbit1 kernel: [504644.382986] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: Maximum number of peer devices = 7
Mar 13 20:37:33 linbit1 kernel: [504644.383735] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Method to ensure write ordering: drain
Mar 13 20:37:33 linbit1 kernel: [504644.383743] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:33 linbit1 kernel: [504644.391372] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:33 linbit1 kernel: [504644.391380] drbd1003: detected capacity change from 0 to 20971520
Mar 13 20:37:33 linbit1 kernel: [504644.391384] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: size = 10 GB (10485760 KB)
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: drbd1003: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1003' failed with exit code 1.
Mar 13 20:37:33 linbit1 kernel: [504644.396869] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: recounting of set bits took additional 4ms
Mar 13 20:37:33 linbit1 kernel: [504644.396881] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: disk( Attaching -> Inconsistent )
Mar 13 20:37:33 linbit1 kernel: [504644.396883] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: attached to current UUID: 0000000000000004
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: dm-10: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-10' failed with exit code 1.
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: dm-11: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-11' failed with exit code 1.
Mar 13 20:37:33 linbit1 kernel: [504644.642644] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Starting worker thread (from drbdsetup [2444931])
Mar 13 20:37:33 linbit1 kernel: [504644.650105] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: meta-data IO uses: blk-bio
Mar 13 20:37:33 linbit1 kernel: [504644.650169] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: rs_discard_granularity feature disabled
Mar 13 20:37:33 linbit1 kernel: [504644.650198] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: disk( Diskless -> Attaching )
Mar 13 20:37:33 linbit1 kernel: [504644.650207] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: Maximum number of peer devices = 7
Mar 13 20:37:33 linbit1 kernel: [504644.650796] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Method to ensure write ordering: drain
Mar 13 20:37:33 linbit1 kernel: [504644.650803] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:33 linbit1 kernel: [504644.656554] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:33 linbit1 kernel: [504644.656560] drbd1004: detected capacity change from 0 to 20971520
Mar 13 20:37:33 linbit1 kernel: [504644.656564] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: size = 10 GB (10485760 KB)
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: drbd1004: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1004' failed with exit code 1.
Mar 13 20:37:33 linbit1 kernel: [504644.662142] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: recounting of set bits took additional 4ms
Mar 13 20:37:33 linbit1 kernel: [504644.662153] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: disk( Attaching -> Inconsistent )
Mar 13 20:37:33 linbit1 kernel: [504644.662156] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: attached to current UUID: 0000000000000004
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: dm-12: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-12' failed with exit code 1.
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: dm-13: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-13' failed with exit code 1.
Mar 13 20:37:33 linbit1 kernel: [504644.923546] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Starting worker thread (from drbdsetup [2445010])
Mar 13 20:37:33 linbit1 kernel: [504644.931137] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: meta-data IO uses: blk-bio
Mar 13 20:37:33 linbit1 kernel: [504644.931212] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: rs_discard_granularity feature disabled
Mar 13 20:37:33 linbit1 kernel: [504644.931246] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: disk( Diskless -> Attaching )
Mar 13 20:37:33 linbit1 kernel: [504644.931255] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: Maximum number of peer devices = 7
Mar 13 20:37:33 linbit1 kernel: [504644.931812] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Method to ensure write ordering: drain
Mar 13 20:37:33 linbit1 kernel: [504644.931819] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:33 linbit1 systemd-udevd[2444332]: drbd1002: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1002' failed with exit code 1.
Mar 13 20:37:33 linbit1 kernel: [504644.936485] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:33 linbit1 kernel: [504644.936490] drbd1002: detected capacity change from 0 to 20971520
Mar 13 20:37:33 linbit1 kernel: [504644.936493] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: size = 10 GB (10485760 KB)
Mar 13 20:37:33 linbit1 kernel: [504644.942531] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: recounting of set bits took additional 4ms
Mar 13 20:37:33 linbit1 kernel: [504644.942541] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: disk( Attaching -> Inconsistent )
Mar 13 20:37:33 linbit1 kernel: [504644.942544] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: attached to current UUID: 0000000000000004
Mar 13 20:37:35 linbit1 kernel: [504646.288580] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: rs_discard_granularity feature disabled
Mar 13 20:37:35 linbit1 kernel: [504646.325631] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 333143022 (0->-1 7683/4609)
Mar 13 20:37:35 linbit1 kernel: [504646.325641] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 333143022 (0ms)
Mar 13 20:37:35 linbit1 kernel: [504646.325648] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: role( Secondary -> Primary )
Mar 13 20:37:35 linbit1 kernel: [504646.325652] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: disk( Inconsistent -> UpToDate )
Mar 13 20:37:35 linbit1 kernel: [504646.325710] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: size = 10 GB (10485760 KB)
Mar 13 20:37:35 linbit1 kernel: [504646.325814] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Forced to consider local data as UpToDate!
Mar 13 20:37:35 linbit1 kernel: [504646.325906] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: new current UUID: 6E639916C4467B01 weak: FFFFFFFFFFFFFFFE
Mar 13 20:37:35 linbit1 kernel: [504646.330907] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 4288222272 (0->-1 3/2)
Mar 13 20:37:35 linbit1 kernel: [504646.330916] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 4288222272 (0ms)
Mar 13 20:37:35 linbit1 kernel: [504646.330922] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: role( Primary -> Secondary )
Mar 13 20:37:35 linbit1 kernel: [504646.335744] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 3721854703 (0->-1 3/1)
Mar 13 20:37:35 linbit1 kernel: [504646.335752] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 3721854703 (0ms)
Mar 13 20:37:35 linbit1 kernel: [504646.335759] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: role( Secondary -> Primary )
Mar 13 20:37:35 linbit1 kernel: [504646.340905] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 2161490844 (0->-1 3/2)
Mar 13 20:37:35 linbit1 kernel: [504646.340913] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 2161490844 (0ms)
Mar 13 20:37:35 linbit1 kernel: [504646.340920] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: role( Primary -> Secondary )
Mar 13 20:37:35 linbit1 kernel: [504646.355380] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: rs_discard_granularity feature disabled
Mar 13 20:37:35 linbit1 kernel: [504646.418738] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: rs_discard_granularity feature disabled
Mar 13 20:37:35 linbit1 kernel: [504646.453510] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 3334029663 (0->-1 7683/4609)
Mar 13 20:37:35 linbit1 kernel: [504646.453519] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 3334029663 (0ms)
Mar 13 20:37:35 linbit1 kernel: [504646.453525] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: role( Secondary -> Primary )
Mar 13 20:37:35 linbit1 kernel: [504646.453528] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: disk( Inconsistent -> UpToDate )
Mar 13 20:37:35 linbit1 kernel: [504646.453592] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: size = 10 GB (10485760 KB)
Mar 13 20:37:35 linbit1 kernel: [504646.453709] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Forced to consider local data as UpToDate!
Mar 13 20:37:35 linbit1 kernel: [504646.453793] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: new current UUID: D5FDCB3B6CEDFD0F weak: FFFFFFFFFFFFFFFE
Mar 13 20:37:35 linbit1 kernel: [504646.459091] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 736190394 (0->-1 3/2)
Mar 13 20:37:35 linbit1 kernel: [504646.459100] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 736190394 (0ms)
Mar 13 20:37:35 linbit1 kernel: [504646.459107] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: role( Primary -> Secondary )
Mar 13 20:37:35 linbit1 kernel: [504646.464514] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 4010987340 (0->-1 3/1)
Mar 13 20:37:35 linbit1 kernel: [504646.464522] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 4010987340 (0ms)
Mar 13 20:37:35 linbit1 kernel: [504646.464528] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: role( Secondary -> Primary )
Mar 13 20:37:35 linbit1 kernel: [504646.469563] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 969303115 (0->-1 3/2)
Mar 13 20:37:35 linbit1 kernel: [504646.469569] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 969303115 (0ms)
Mar 13 20:37:35 linbit1 kernel: [504646.469574] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: role( Primary -> Secondary )
Mar 13 20:37:35 linbit1 kernel: [504646.483675] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: rs_discard_granularity feature disabled
Mar 13 20:37:35 linbit1 kernel: [504646.559121] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: rs_discard_granularity feature disabled
Mar 13 20:37:35 linbit1 kernel: [504646.606232] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: rs_discard_granularity feature disabled
Mar 13 20:37:37 linbit1 kernel: [504648.092721] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Starting sender thread (from drbdsetup [2445276])
Mar 13 20:37:37 linbit1 kernel: [504648.094443] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Starting sender thread (from drbdsetup [2445278])
Mar 13 20:37:37 linbit1 kernel: [504648.141804] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: rs_discard_granularity feature disabled
Mar 13 20:37:37 linbit1 kernel: [504648.169641] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:37 linbit1 kernel: [504648.169648] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: quorum( yes -> no )
Mar 13 20:37:37 linbit1 kernel: [504648.169799] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Starting receiver thread (from drbd_w_pvc-1f71 [2444383])
Mar 13 20:37:37 linbit1 kernel: [504648.169958] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:37 linbit1 kernel: [504648.171147] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:37 linbit1 kernel: [504648.171193] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Starting receiver thread (from drbd_w_pvc-1f71 [2444383])
Mar 13 20:37:37 linbit1 kernel: [504648.171327] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:37 linbit1 kernel: [504648.185900] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: rs_discard_granularity feature disabled
Mar 13 20:37:37 linbit1 kernel: [504648.217719] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 2066455176 (0->-1 7683/4609)
Mar 13 20:37:37 linbit1 kernel: [504648.217727] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 2066455176 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.217734] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: role( Secondary -> Primary )
Mar 13 20:37:37 linbit1 kernel: [504648.217738] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: disk( Inconsistent -> UpToDate )
Mar 13 20:37:37 linbit1 kernel: [504648.217806] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: size = 10 GB (10485760 KB)
Mar 13 20:37:37 linbit1 kernel: [504648.217927] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Forced to consider local data as UpToDate!
Mar 13 20:37:37 linbit1 kernel: [504648.218014] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: new current UUID: 666FBF2D8784402D weak: FFFFFFFFFFFFFFFE
Mar 13 20:37:37 linbit1 kernel: [504648.224311] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 1264875416 (0->-1 3/2)
Mar 13 20:37:37 linbit1 kernel: [504648.224320] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 1264875416 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.224328] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: role( Primary -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.233040] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 1249299112 (0->-1 3/1)
Mar 13 20:37:37 linbit1 kernel: [504648.233048] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 1249299112 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.233056] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: role( Secondary -> Primary )
Mar 13 20:37:37 linbit1 kernel: [504648.238582] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 3903818090 (0->-1 3/2)
Mar 13 20:37:37 linbit1 kernel: [504648.238590] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 3903818090 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.238597] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: role( Primary -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.250909] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74: Starting worker thread (from drbdsetup [2445322])
Mar 13 20:37:37 linbit1 kernel: [504648.255051] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Starting sender thread (from drbdsetup [2445327])
Mar 13 20:37:37 linbit1 kernel: [504648.256699] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Starting sender thread (from drbdsetup [2445330])
Mar 13 20:37:37 linbit1 kernel: [504648.260539] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:37 linbit1 kernel: [504648.260578] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Starting receiver thread (from drbd_w_pvc-409c [2445323])
Mar 13 20:37:37 linbit1 kernel: [504648.260700] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:37 linbit1 systemd-udevd[2444332]: drbd1007: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1007' failed with exit code 1.
Mar 13 20:37:37 linbit1 kernel: [504648.261863] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:37 linbit1 kernel: [504648.261893] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Starting receiver thread (from drbd_w_pvc-409c [2445323])
Mar 13 20:37:37 linbit1 kernel: [504648.262026] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:37 linbit1 systemd-udevd[2444332]: dm-14: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-14' failed with exit code 1.
Mar 13 20:37:37 linbit1 systemd-udevd[2444332]: dm-15: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-15' failed with exit code 1.
Mar 13 20:37:37 linbit1 kernel: [504648.511825] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Starting worker thread (from drbdsetup [2445392])
Mar 13 20:37:37 linbit1 kernel: [504648.516261] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Starting sender thread (from drbdsetup [2445397])
Mar 13 20:37:37 linbit1 kernel: [504648.517957] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Starting sender thread (from drbdsetup [2445400])
Mar 13 20:37:37 linbit1 systemd-udevd[2444332]: drbd1009: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1009' failed with exit code 1.
Mar 13 20:37:37 linbit1 kernel: [504648.562067] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: meta-data IO uses: blk-bio
Mar 13 20:37:37 linbit1 kernel: [504648.562154] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: rs_discard_granularity feature disabled
Mar 13 20:37:37 linbit1 kernel: [504648.562204] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: disk( Diskless -> Attaching )
Mar 13 20:37:37 linbit1 kernel: [504648.562218] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: Maximum number of peer devices = 7
Mar 13 20:37:37 linbit1 kernel: [504648.562977] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Method to ensure write ordering: drain
Mar 13 20:37:37 linbit1 kernel: [504648.562987] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:37 linbit1 kernel: [504648.570052] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:37 linbit1 kernel: [504648.570061] drbd1009: detected capacity change from 0 to 20971520
Mar 13 20:37:37 linbit1 kernel: [504648.570065] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: size = 10 GB (10485760 KB)
Mar 13 20:37:37 linbit1 kernel: [504648.579374] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: recounting of set bits took additional 4ms
Mar 13 20:37:37 linbit1 kernel: [504648.579397] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: disk( Attaching -> Inconsistent )
Mar 13 20:37:37 linbit1 kernel: [504648.579403] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: attached to current UUID: 0000000000000004
Mar 13 20:37:37 linbit1 kernel: [504648.581319] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:37 linbit1 kernel: [504648.581418] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Starting receiver thread (from drbd_w_pvc-837c [2445393])
Mar 13 20:37:37 linbit1 kernel: [504648.581581] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:37 linbit1 kernel: [504648.582933] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:37 linbit1 kernel: [504648.582967] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Starting receiver thread (from drbd_w_pvc-837c [2445393])
Mar 13 20:37:37 linbit1 kernel: [504648.583102] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:37 linbit1 kernel: [504648.598316] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: rs_discard_granularity feature disabled
Mar 13 20:37:37 linbit1 kernel: [504648.646108] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 1588922123 (0->-1 7683/4609)
Mar 13 20:37:37 linbit1 kernel: [504648.646118] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 1588922123 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.646125] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: role( Secondary -> Primary )
Mar 13 20:37:37 linbit1 kernel: [504648.646129] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: disk( Inconsistent -> UpToDate )
Mar 13 20:37:37 linbit1 kernel: [504648.646193] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: size = 10 GB (10485760 KB)
Mar 13 20:37:37 linbit1 kernel: [504648.646301] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Forced to consider local data as UpToDate!
Mar 13 20:37:37 linbit1 kernel: [504648.646393] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: new current UUID: 1CE8657E0B4F4A9B weak: FFFFFFFFFFFFFFFE
Mar 13 20:37:37 linbit1 kernel: [504648.652078] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 3601203717 (0->-1 3/2)
Mar 13 20:37:37 linbit1 kernel: [504648.652114] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 3601203717 (4ms)
Mar 13 20:37:37 linbit1 kernel: [504648.652121] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: role( Primary -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.657918] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 1111730147 (0->-1 3/1)
Mar 13 20:37:37 linbit1 kernel: [504648.657926] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 1111730147 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.657934] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: role( Secondary -> Primary )
Mar 13 20:37:37 linbit1 kernel: [504648.663532] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 1239792522 (0->-1 3/2)
Mar 13 20:37:37 linbit1 kernel: [504648.663539] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 1239792522 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.663544] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: role( Primary -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.675842] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd: Starting worker thread (from drbdsetup [2445454])
Mar 13 20:37:37 linbit1 kernel: [504648.679605] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Starting sender thread (from drbdsetup [2445459])
Mar 13 20:37:37 linbit1 kernel: [504648.681255] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Starting sender thread (from drbdsetup [2445461])
Mar 13 20:37:37 linbit1 systemd-udevd[2444332]: drbd1008: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1008' failed with exit code 1.
Mar 13 20:37:37 linbit1 kernel: [504648.685581] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:37 linbit1 kernel: [504648.685626] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Starting receiver thread (from drbd_w_pvc-b352 [2445455])
Mar 13 20:37:37 linbit1 kernel: [504648.685727] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:37 linbit1 kernel: [504648.687100] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:37 linbit1 kernel: [504648.687130] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Starting receiver thread (from drbd_w_pvc-b352 [2445455])
Mar 13 20:37:37 linbit1 kernel: [504648.687249] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:37 linbit1 kernel: [504648.690063] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:37 linbit1 kernel: [504648.690072] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:37 linbit1 kernel: [504648.690194] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:37 linbit1 kernel: [504648.699007] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:37:37 linbit1 kernel: [504648.699015] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:37 linbit1 kernel: [504648.699188] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:37 linbit1 kernel: [504648.700661] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: rs_discard_granularity feature disabled
Mar 13 20:37:37 linbit1 kernel: [504648.716132] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 3467310593 (0->1 499/146)
Mar 13 20:37:37 linbit1 kernel: [504648.750203] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 3507733887 (0->-1 7683/4609)
Mar 13 20:37:37 linbit1 kernel: [504648.750212] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 3507733887 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.750218] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: role( Secondary -> Primary )
Mar 13 20:37:37 linbit1 kernel: [504648.750223] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: disk( Inconsistent -> UpToDate )
Mar 13 20:37:37 linbit1 kernel: [504648.750293] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: size = 10 GB (10485760 KB)
Mar 13 20:37:37 linbit1 kernel: [504648.750393] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Forced to consider local data as UpToDate!
Mar 13 20:37:37 linbit1 kernel: [504648.750460] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: new current UUID: AECE884230265E81 weak: FFFFFFFFFFFFFFFE
Mar 13 20:37:37 linbit1 kernel: [504648.757121] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 736128398 (0->-1 3/2)
Mar 13 20:37:37 linbit1 kernel: [504648.757129] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 736128398 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.757135] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: role( Primary -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.763234] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 276570594 (0->-1 3/1)
Mar 13 20:37:37 linbit1 kernel: [504648.763242] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 276570594 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.763250] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: role( Secondary -> Primary )
Mar 13 20:37:37 linbit1 kernel: [504648.768158] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: drbd_sync_handshake:
Mar 13 20:37:37 linbit1 kernel: [504648.768167] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: self 365522F30E0CAB98:395ED3DF8CCA85D1:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:37:37 linbit1 kernel: [504648.768177] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:37 linbit1 kernel: [504648.768184] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:37:37 linbit1 kernel: [504648.768190] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:37:37 linbit1 kernel: [504648.769810] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 2599818233 (0->-1 3/2)
Mar 13 20:37:37 linbit1 kernel: [504648.769819] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 2599818233 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.769826] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: role( Primary -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.771305] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change 3467310593: primary_nodes=0, weak_nodes=0
Mar 13 20:37:37 linbit1 kernel: [504648.771315] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 3467310593 (52ms)
Mar 13 20:37:37 linbit1 kernel: [504648.771364] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.771368] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: quorum( no -> yes )
Mar 13 20:37:37 linbit1 kernel: [504648.771373] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:37:37 linbit1 kernel: [504648.772778] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:37 linbit1 kernel: [504648.776345] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:37 linbit1 kernel: [504648.776369] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:37:37 linbit1 kernel: [504648.776535] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:37:37 linbit1 kernel: [504648.785412] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: rs_discard_granularity feature disabled
Mar 13 20:37:37 linbit1 kernel: [504648.800122] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 3277774558 (0->2 499/146)
Mar 13 20:37:37 linbit1 kernel: [504648.836127] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit3: self 365522F30E0CAB98:395ED3DF8CCA85D1:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:37:37 linbit1 kernel: [504648.836142] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:37:37 linbit1 kernel: [504648.842910] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 2307742917 (0->-1 7683/4609)
Mar 13 20:37:37 linbit1 kernel: [504648.842920] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 2307742917 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.842927] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: role( Secondary -> Primary )
Mar 13 20:37:37 linbit1 kernel: [504648.842931] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: disk( Inconsistent -> UpToDate )
Mar 13 20:37:37 linbit1 kernel: [504648.843002] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change 3277774558: primary_nodes=0, weak_nodes=0
Mar 13 20:37:37 linbit1 kernel: [504648.843004] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: size = 10 GB (10485760 KB)
Mar 13 20:37:37 linbit1 kernel: [504648.843016] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 3277774558 (40ms)
Mar 13 20:37:37 linbit1 kernel: [504648.843092] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.843099] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:37:37 linbit1 kernel: [504648.843111] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Forced to consider local data as UpToDate!
Mar 13 20:37:37 linbit1 kernel: [504648.843207] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: new current UUID: C6D5DA1F69944505 weak: FFFFFFFFFFFFFFFE
Mar 13 20:37:37 linbit1 kernel: [504648.849631] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 2481887876 (0->-1 3/2)
Mar 13 20:37:37 linbit1 kernel: [504648.849641] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 2481887876 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.849648] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: role( Primary -> Secondary )
Mar 13 20:37:37 linbit1 kernel: [504648.856325] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 865557391 (0->-1 3/1)
Mar 13 20:37:37 linbit1 kernel: [504648.856335] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 865557391 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.856343] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: role( Secondary -> Primary )
Mar 13 20:37:37 linbit1 kernel: [504648.863291] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 2295127749 (0->-1 3/2)
Mar 13 20:37:37 linbit1 kernel: [504648.863302] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 2295127749 (0ms)
Mar 13 20:37:37 linbit1 kernel: [504648.863310] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: role( Primary -> Secondary )
Mar 13 20:37:38 linbit1 kernel: [504649.211023] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:37:38 linbit1 kernel: [504649.211035] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:38 linbit1 kernel: [504649.211148] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:38 linbit1 kernel: [504649.262992] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Preparing remote state change 949169440
Mar 13 20:37:38 linbit1 kernel: [504649.284260] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:38 linbit1 kernel: [504649.284270] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:38 linbit1 kernel: [504649.284535] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:38 linbit1 kernel: [504649.288102] drbd1008: detected capacity change from 0 to 20971520
Mar 13 20:37:38 linbit1 kernel: [504649.288110] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008: size = 10 GB (10485760 KB)
Mar 13 20:37:38 linbit1 kernel: [504649.288250] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:37:38 linbit1 kernel: [504649.288258] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit3: peer EEF0D87FF1D965AA:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:37:38 linbit1 kernel: [504649.299043] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Committing remote state change 949169440 (primary_nodes=0)
Mar 13 20:37:38 linbit1 kernel: [504649.299062] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:38 linbit1 kernel: [504649.299068] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:37:38 linbit1 kernel: [504649.316322] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:37:38 linbit1 kernel: [504649.316332] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:38 linbit1 kernel: [504649.316481] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:38 linbit1 kernel: [504649.342987] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Preparing remote state change 2550016622
Mar 13 20:37:38 linbit1 kernel: [504649.392101] drbd1007: detected capacity change from 0 to 20971520
Mar 13 20:37:38 linbit1 kernel: [504649.392109] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007: size = 10 GB (10485760 KB)
Mar 13 20:37:38 linbit1 kernel: [504649.392249] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:37:38 linbit1 kernel: [504649.392257] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit3: peer 0000000000000004:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:38 linbit1 kernel: [504649.392438] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Committing remote state change 2550016622 (primary_nodes=0)
Mar 13 20:37:38 linbit1 kernel: [504649.392451] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:38 linbit1 kernel: [504649.392455] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established )
Mar 13 20:37:38 linbit1 kernel: [504649.508137] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Preparing cluster-wide state change 1860088235 (1->2 499/146)
Mar 13 20:37:38 linbit1 kernel: [504649.532114] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit3: self 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:37:38 linbit1 kernel: [504649.532125] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:37:38 linbit1 kernel: [504649.532138] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: State change 1860088235: primary_nodes=0, weak_nodes=0
Mar 13 20:37:38 linbit1 kernel: [504649.532145] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Committing cluster-wide state change 1860088235 (24ms)
Mar 13 20:37:38 linbit1 kernel: [504649.532180] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:38 linbit1 kernel: [504649.532186] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:37:38 linbit1 kernel: [504649.764202] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:37:38 linbit1 kernel: [504649.764210] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:38 linbit1 kernel: [504649.764387] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:38 linbit1 kernel: [504649.806001] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Preparing remote state change 818657974
Mar 13 20:37:38 linbit1 kernel: [504649.828120] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:37:38 linbit1 kernel: [504649.828127] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit2: peer 8F1D52E9CE37BBF6:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:37:38 linbit1 kernel: [504649.834021] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Committing remote state change 818657974 (primary_nodes=0)
Mar 13 20:37:38 linbit1 kernel: [504649.834038] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:38 linbit1 kernel: [504649.834043] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007: quorum( no -> yes )
Mar 13 20:37:38 linbit1 kernel: [504649.834049] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:37:38 linbit1 kernel: [504649.860283] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:37:38 linbit1 kernel: [504649.860291] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:38 linbit1 kernel: [504649.860487] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:38 linbit1 kernel: [504649.938038] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Preparing remote state change 3374129985
Mar 13 20:37:38 linbit1 kernel: [504649.942030] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Preparing remote state change 756835794
Mar 13 20:37:38 linbit1 kernel: [504649.976250] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: drbd_sync_handshake:
Mar 13 20:37:38 linbit1 kernel: [504649.976259] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: self 0000000000000004:0000000000000000:10FC5CC0C6FEC7C8:0000000000000000 bits:0 flags:24
Mar 13 20:37:38 linbit1 kernel: [504649.976267] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: peer 67E6FE2E8F370A4C:10FC5CC0C6FEC7C8:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:37:38 linbit1 kernel: [504649.976275] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: uuid_compare()=target-set-bitmap by rule=just-created-self
Mar 13 20:37:38 linbit1 kernel: [504649.976281] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: Setting and writing the whole bitmap, fresh node
Mar 13 20:37:38 linbit1 kernel: [504649.980412] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Committing remote state change 3374129985 (primary_nodes=0)
Mar 13 20:37:38 linbit1 kernel: [504649.987234] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit3: resync-susp( no -> peer )
Mar 13 20:37:38 linbit1 kernel: [504649.994556] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Committing remote state change 756835794 (primary_nodes=0)
Mar 13 20:37:38 linbit1 kernel: [504649.994578] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:38 linbit1 kernel: [504649.994583] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: quorum( no -> yes )
Mar 13 20:37:38 linbit1 kernel: [504649.994588] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:37:38 linbit1 kernel: [504649.994714] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: size = 10 GB (10485760 KB)
Mar 13 20:37:38 linbit1 kernel: [504649.997919] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:38 linbit1 kernel: [504649.998697] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:38 linbit1 kernel: [504649.998704] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: setting UUIDs to 10FC5CC0C6FEC7C8:0000000000000000:10FC5CC0C6FEC7C8:0000000000000000
Mar 13 20:37:38 linbit1 kernel: [504649.998717] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit3: resync-susp( no -> connection dependency )
Mar 13 20:37:38 linbit1 kernel: [504649.998721] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: repl( WFBitMapT -> SyncTarget )
Mar 13 20:37:38 linbit1 kernel: [504649.998813] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: Began resync as SyncTarget (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:37:38 linbit1 kernel: [504649.998835] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Preparing remote state change 1159823094
Mar 13 20:37:39 linbit1 kernel: [504650.038000] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Committing remote state change 1159823094 (primary_nodes=0)
Mar 13 20:37:39 linbit1 kernel: [504650.226159] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:39 linbit1 kernel: [504650.226168] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:39 linbit1 kernel: [504650.226414] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:39 linbit1 kernel: [504650.394024] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Preparing remote state change 2748893700
Mar 13 20:37:39 linbit1 kernel: [504650.416233] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:37:39 linbit1 kernel: [504650.416241] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit2: peer 0000000000000004:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:39 linbit1 kernel: [504650.422023] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Committing remote state change 2748893700 (primary_nodes=0)
Mar 13 20:37:39 linbit1 kernel: [504650.422042] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:39 linbit1 kernel: [504650.422047] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008: quorum( no -> yes )
Mar 13 20:37:39 linbit1 kernel: [504650.422053] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established )
Mar 13 20:37:39 linbit1 kernel: [504650.422469] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Preparing remote state change 115745656
Mar 13 20:37:39 linbit1 kernel: [504650.462046] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Committing remote state change 115745656 (primary_nodes=0)
Mar 13 20:37:39 linbit1 kernel: [504650.465467] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit2: resync-susp( no -> peer )
Mar 13 20:37:39 linbit1 kernel: [504650.959666] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Starting sender thread (from drbdsetup [2445623])
Mar 13 20:37:39 linbit1 kernel: [504650.961221] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Starting sender thread (from drbdsetup [2445625])
Mar 13 20:37:39 linbit1 kernel: [504651.001493] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: rs_discard_granularity feature disabled
Mar 13 20:37:40 linbit1 kernel: [504651.045351] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:40 linbit1 kernel: [504651.045356] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: quorum( yes -> no )
Mar 13 20:37:40 linbit1 kernel: [504651.045554] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Starting receiver thread (from drbd_w_pvc-01ec [2444652])
Mar 13 20:37:40 linbit1 kernel: [504651.045686] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:40 linbit1 kernel: [504651.046277] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:40 linbit1 kernel: [504651.046313] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Starting receiver thread (from drbd_w_pvc-01ec [2444652])
Mar 13 20:37:40 linbit1 kernel: [504651.046439] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:40 linbit1 kernel: [504651.578997] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:37:40 linbit1 kernel: [504651.579007] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:40 linbit1 kernel: [504651.579125] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:40 linbit1 kernel: [504651.744123] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 560969117 (0->2 499/146)
Mar 13 20:37:40 linbit1 kernel: [504651.768108] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: self 6E639916C4467B00:8E546FDABA594BCB:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:37:40 linbit1 kernel: [504651.768120] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:37:40 linbit1 kernel: [504651.768134] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change 560969117: primary_nodes=0, weak_nodes=0
Mar 13 20:37:40 linbit1 kernel: [504651.768141] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 560969117 (24ms)
Mar 13 20:37:40 linbit1 kernel: [504651.768174] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:40 linbit1 kernel: [504651.768180] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:37:41 linbit1 kernel: [504652.050179] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:41 linbit1 kernel: [504652.050189] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:41 linbit1 kernel: [504652.050324] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:41 linbit1 kernel: [504652.130842] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: rs_discard_granularity feature disabled
Mar 13 20:37:41 linbit1 kernel: [504652.185998] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Starting sender thread (from drbdsetup [2445696])
Mar 13 20:37:41 linbit1 kernel: [504652.187884] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Starting sender thread (from drbdsetup [2445698])
Mar 13 20:37:41 linbit1 kernel: [504652.204127] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 3633339456 (0->1 499/146)
Mar 13 20:37:41 linbit1 kernel: [504652.225760] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: rs_discard_granularity feature disabled
Mar 13 20:37:41 linbit1 kernel: [504652.236139] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: drbd_sync_handshake:
Mar 13 20:37:41 linbit1 kernel: [504652.236145] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: self 6E639916C4467B00:8E546FDABA594BCB:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:37:41 linbit1 kernel: [504652.236153] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:41 linbit1 kernel: [504652.236160] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:37:41 linbit1 kernel: [504652.236165] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:37:41 linbit1 kernel: [504652.249163] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change 3633339456: primary_nodes=0, weak_nodes=0
Mar 13 20:37:41 linbit1 kernel: [504652.249174] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 3633339456 (44ms)
Mar 13 20:37:41 linbit1 kernel: [504652.249237] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:41 linbit1 kernel: [504652.249243] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: quorum( no -> yes )
Mar 13 20:37:41 linbit1 kernel: [504652.249248] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:37:41 linbit1 kernel: [504652.250650] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:41 linbit1 kernel: [504652.250682] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Preparing remote state change 4014774595
Mar 13 20:37:41 linbit1 kernel: [504652.253863] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:41 linbit1 kernel: [504652.253879] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:37:41 linbit1 kernel: [504652.253988] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:37:41 linbit1 kernel: [504652.261968] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.261976] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: quorum( yes -> no )
Mar 13 20:37:41 linbit1 kernel: [504652.262190] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Starting receiver thread (from drbd_w_pvc-20f0 [2444772])
Mar 13 20:37:41 linbit1 kernel: [504652.262348] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.263583] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.263626] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Starting receiver thread (from drbd_w_pvc-20f0 [2444772])
Mar 13 20:37:41 linbit1 kernel: [504652.263776] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.279190] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Committing remote state change 4014774595 (primary_nodes=0)
Mar 13 20:37:41 linbit1 kernel: [504652.292688] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Starting sender thread (from drbdsetup [2445726])
Mar 13 20:37:41 linbit1 kernel: [504652.294350] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Starting sender thread (from drbdsetup [2445728])
Mar 13 20:37:41 linbit1 kernel: [504652.337667] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: rs_discard_granularity feature disabled
Mar 13 20:37:41 linbit1 kernel: [504652.365510] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.365561] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Starting receiver thread (from drbd_w_pvc-6361 [2444502])
Mar 13 20:37:41 linbit1 kernel: [504652.365692] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.366559] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.366563] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: quorum( yes -> no )
Mar 13 20:37:41 linbit1 kernel: [504652.366732] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Starting receiver thread (from drbd_w_pvc-6361 [2444502])
Mar 13 20:37:41 linbit1 kernel: [504652.366846] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.383043] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: rs_discard_granularity feature disabled
Mar 13 20:37:41 linbit1 kernel: [504652.434813] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Starting sender thread (from drbdsetup [2445760])
Mar 13 20:37:41 linbit1 kernel: [504652.436570] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Starting sender thread (from drbdsetup [2445762])
Mar 13 20:37:41 linbit1 kernel: [504652.489816] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: rs_discard_granularity feature disabled
Mar 13 20:37:41 linbit1 kernel: [504652.517591] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.517598] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: quorum( yes -> no )
Mar 13 20:37:41 linbit1 kernel: [504652.517815] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Starting receiver thread (from drbd_w_pvc-986e [2444858])
Mar 13 20:37:41 linbit1 kernel: [504652.517973] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.518911] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.518959] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Starting receiver thread (from drbd_w_pvc-986e [2444858])
Mar 13 20:37:41 linbit1 kernel: [504652.519092] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.548500] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Starting sender thread (from drbdsetup [2445790])
Mar 13 20:37:41 linbit1 kernel: [504652.549990] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Starting sender thread (from drbdsetup [2445792])
Mar 13 20:37:41 linbit1 kernel: [504652.589766] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: rs_discard_granularity feature disabled
Mar 13 20:37:41 linbit1 kernel: [504652.629765] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.629772] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: quorum( yes -> no )
Mar 13 20:37:41 linbit1 kernel: [504652.629922] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Starting receiver thread (from drbd_w_pvc-c419 [2444932])
Mar 13 20:37:41 linbit1 kernel: [504652.630109] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.631035] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.631077] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Starting receiver thread (from drbd_w_pvc-c419 [2444932])
Mar 13 20:37:41 linbit1 kernel: [504652.631241] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.649669] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Starting sender thread (from drbdsetup [2445813])
Mar 13 20:37:41 linbit1 kernel: [504652.651475] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Starting sender thread (from drbdsetup [2445815])
Mar 13 20:37:41 linbit1 kernel: [504652.705709] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: rs_discard_granularity feature disabled
Mar 13 20:37:41 linbit1 kernel: [504652.745735] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.745742] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: quorum( yes -> no )
Mar 13 20:37:41 linbit1 kernel: [504652.745957] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Starting receiver thread (from drbd_w_pvc-d174 [2445011])
Mar 13 20:37:41 linbit1 kernel: [504652.746136] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.747116] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:41 linbit1 kernel: [504652.747160] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Starting receiver thread (from drbd_w_pvc-d174 [2445011])
Mar 13 20:37:41 linbit1 kernel: [504652.747316] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:41 linbit1 kernel: [504652.795085] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:37:41 linbit1 kernel: [504652.795094] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:41 linbit1 kernel: [504652.795210] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:41 linbit1 kernel: [504652.890988] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:41 linbit1 kernel: [504652.890998] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:41 linbit1 kernel: [504652.891149] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:41 linbit1 kernel: [504652.916122] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 557405243 (0->2 499/146)
Mar 13 20:37:41 linbit1 kernel: [504652.956099] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit3: self 666FBF2D8784402C:1C2A5C04FAB8719E:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:37:41 linbit1 kernel: [504652.956110] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:37:41 linbit1 kernel: [504652.956124] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: State change 557405243: primary_nodes=0, weak_nodes=0
Mar 13 20:37:41 linbit1 kernel: [504652.956131] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 557405243 (40ms)
Mar 13 20:37:41 linbit1 kernel: [504652.956163] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:41 linbit1 kernel: [504652.956169] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:37:41 linbit1 kernel: [504652.992119] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 281481650 (0->1 499/146)
Mar 13 20:37:42 linbit1 kernel: [504653.016204] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: drbd_sync_handshake:
Mar 13 20:37:42 linbit1 kernel: [504653.016211] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: self D5FDCB3B6CEDFD0E:E73DAC87D51DD529:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:37:42 linbit1 kernel: [504653.016220] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:42 linbit1 kernel: [504653.016227] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:37:42 linbit1 kernel: [504653.016233] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:37:42 linbit1 kernel: [504653.019272] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: State change 281481650: primary_nodes=0, weak_nodes=0
Mar 13 20:37:42 linbit1 kernel: [504653.019280] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 281481650 (24ms)
Mar 13 20:37:42 linbit1 kernel: [504653.019325] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:42 linbit1 kernel: [504653.019329] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: quorum( no -> yes )
Mar 13 20:37:42 linbit1 kernel: [504653.019334] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:37:42 linbit1 kernel: [504653.020783] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:42 linbit1 kernel: [504653.024358] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:42 linbit1 kernel: [504653.024386] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: repl( WFBitMapS -> SyncSource )
Mar 13 20:37:42 linbit1 kernel: [504653.024547] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:37:42 linbit1 kernel: [504653.786986] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:37:42 linbit1 kernel: [504653.786995] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:42 linbit1 kernel: [504653.787112] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:42 linbit1 kernel: [504653.824116] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 492001736 (0->2 499/146)
Mar 13 20:37:42 linbit1 kernel: [504653.860114] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit3: self 1CE8657E0B4F4A9A:7F08074A817B57ED:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:37:42 linbit1 kernel: [504653.860125] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:37:42 linbit1 kernel: [504653.860137] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: State change 492001736: primary_nodes=0, weak_nodes=0
Mar 13 20:37:42 linbit1 kernel: [504653.860143] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 492001736 (36ms)
Mar 13 20:37:42 linbit1 kernel: [504653.860157] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:37:42 linbit1 kernel: [504653.860167] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:42 linbit1 kernel: [504653.860177] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:42 linbit1 kernel: [504653.860183] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:37:42 linbit1 kernel: [504653.860430] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:42 linbit1 kernel: [504653.883143] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:37:42 linbit1 kernel: [504653.883153] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:42 linbit1 kernel: [504653.883293] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:42 linbit1 kernel: [504653.984119] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 157400468 (0->2 499/146)
Mar 13 20:37:42 linbit1 kernel: [504654.002033] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:42 linbit1 kernel: [504654.002042] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:42 linbit1 kernel: [504654.002168] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:42 linbit1 kernel: [504654.012091] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit3: self AECE884230265E80:24534757ACAB0A62:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:37:42 linbit1 kernel: [504654.012103] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:37:42 linbit1 kernel: [504654.012121] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: State change 157400468: primary_nodes=0, weak_nodes=0
Mar 13 20:37:42 linbit1 kernel: [504654.012127] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 157400468 (28ms)
Mar 13 20:37:42 linbit1 kernel: [504654.012157] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:42 linbit1 kernel: [504654.012164] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:37:43 linbit1 kernel: [504654.052117] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 3081174233 (0->2 499/146)
Mar 13 20:37:43 linbit1 kernel: [504654.066065] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:37:43 linbit1 kernel: [504654.066076] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:43 linbit1 kernel: [504654.066289] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:43 linbit1 kernel: [504654.084097] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit3: self C6D5DA1F69944504:674E2E475BF406B2:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:37:43 linbit1 kernel: [504654.084109] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:37:43 linbit1 kernel: [504654.087053] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: State change 3081174233: primary_nodes=0, weak_nodes=0
Mar 13 20:37:43 linbit1 kernel: [504654.087060] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 3081174233 (32ms)
Mar 13 20:37:43 linbit1 kernel: [504654.087096] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:43 linbit1 kernel: [504654.087103] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:37:43 linbit1 kernel: [504654.088093] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 4227516079 (0->2 499/146)
Mar 13 20:37:43 linbit1 kernel: [504654.120104] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 2806317854 (0->1 499/146)
Mar 13 20:37:43 linbit1 kernel: [504654.136101] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit2: self D5FDCB3B6CEDFD0E:E73DAC87D51DD529:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:37:43 linbit1 kernel: [504654.136112] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:37:43 linbit1 kernel: [504654.136124] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: State change 4227516079: primary_nodes=0, weak_nodes=0
Mar 13 20:37:43 linbit1 kernel: [504654.136130] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 4227516079 (48ms)
Mar 13 20:37:43 linbit1 kernel: [504654.136170] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:43 linbit1 kernel: [504654.136177] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:37:43 linbit1 kernel: [504654.138922] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Preparing remote state change 3462149193
Mar 13 20:37:43 linbit1 kernel: [504654.152207] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: drbd_sync_handshake:
Mar 13 20:37:43 linbit1 kernel: [504654.152214] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: self 666FBF2D8784402C:1C2A5C04FAB8719E:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:37:43 linbit1 kernel: [504654.152223] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:43 linbit1 kernel: [504654.152230] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:37:43 linbit1 kernel: [504654.152235] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:37:43 linbit1 kernel: [504654.155218] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: State change 2806317854: primary_nodes=0, weak_nodes=0
Mar 13 20:37:43 linbit1 kernel: [504654.155226] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 2806317854 (32ms)
Mar 13 20:37:43 linbit1 kernel: [504654.155277] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:43 linbit1 kernel: [504654.155281] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: quorum( no -> yes )
Mar 13 20:37:43 linbit1 kernel: [504654.155285] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:37:43 linbit1 kernel: [504654.156667] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:43 linbit1 kernel: [504654.156694] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Preparing remote state change 1462862483
Mar 13 20:37:43 linbit1 kernel: [504654.160353] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:43 linbit1 kernel: [504654.160381] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:37:43 linbit1 kernel: [504654.160530] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:37:43 linbit1 kernel: [504654.174127] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Committing remote state change 3462149193 (primary_nodes=0)
Mar 13 20:37:43 linbit1 kernel: [504654.218236] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Committing remote state change 1462862483 (primary_nodes=0)
Mar 13 20:37:43 linbit1 kernel: [504654.468256] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:43 linbit1 kernel: [504654.468267] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:43 linbit1 kernel: [504654.468532] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:43 linbit1 kernel: [504654.494229] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Preparing remote state change 3275955522
Mar 13 20:37:43 linbit1 kernel: [504654.558126] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Committing remote state change 3275955522 (primary_nodes=0)
Mar 13 20:37:43 linbit1 kernel: [504654.668113] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 981723431 (0->1 499/146)
Mar 13 20:37:43 linbit1 kernel: [504654.691843] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: rs_discard_granularity feature disabled
Mar 13 20:37:43 linbit1 kernel: [504654.712135] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: drbd_sync_handshake:
Mar 13 20:37:43 linbit1 kernel: [504654.712142] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: self 1CE8657E0B4F4A9A:7F08074A817B57ED:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:37:43 linbit1 kernel: [504654.712150] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:43 linbit1 kernel: [504654.712157] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:37:43 linbit1 kernel: [504654.712163] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:37:43 linbit1 kernel: [504654.715120] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: State change 981723431: primary_nodes=0, weak_nodes=0
Mar 13 20:37:43 linbit1 kernel: [504654.715130] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 981723431 (44ms)
Mar 13 20:37:43 linbit1 kernel: [504654.715182] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:43 linbit1 kernel: [504654.715187] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: quorum( no -> yes )
Mar 13 20:37:43 linbit1 kernel: [504654.715192] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:37:43 linbit1 kernel: [504654.716576] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:43 linbit1 kernel: [504654.719905] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:43 linbit1 kernel: [504654.719926] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:37:43 linbit1 kernel: [504654.720103] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:37:43 linbit1 kernel: [504654.902057] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:43 linbit1 kernel: [504654.902066] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:43 linbit1 kernel: [504654.902197] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:43 linbit1 kernel: [504654.990213] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Preparing remote state change 1346127010
Mar 13 20:37:44 linbit1 kernel: [504655.026192] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Committing remote state change 1346127010 (primary_nodes=0)
Mar 13 20:37:44 linbit1 kernel: [504655.088113] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 1821996547 (0->1 499/146)
Mar 13 20:37:44 linbit1 kernel: [504655.116208] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: drbd_sync_handshake:
Mar 13 20:37:44 linbit1 kernel: [504655.116216] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: self AECE884230265E80:24534757ACAB0A62:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:37:44 linbit1 kernel: [504655.116224] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:44 linbit1 kernel: [504655.116230] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:37:44 linbit1 kernel: [504655.116236] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:37:44 linbit1 kernel: [504655.124145] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: State change 1821996547: primary_nodes=0, weak_nodes=0
Mar 13 20:37:44 linbit1 kernel: [504655.124155] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 1821996547 (36ms)
Mar 13 20:37:44 linbit1 kernel: [504655.124219] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:44 linbit1 kernel: [504655.124224] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: quorum( no -> yes )
Mar 13 20:37:44 linbit1 kernel: [504655.124230] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:37:44 linbit1 kernel: [504655.125364] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:44 linbit1 kernel: [504655.127819] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:44 linbit1 kernel: [504655.127833] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:37:44 linbit1 kernel: [504655.127985] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:37:44 linbit1 kernel: [504655.282051] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:37:44 linbit1 kernel: [504655.282061] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:37:44 linbit1 kernel: [504655.282187] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:37:44 linbit1 kernel: [504655.344096] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 3373070157 (0->1 499/146)
Mar 13 20:37:44 linbit1 kernel: [504655.380133] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: drbd_sync_handshake:
Mar 13 20:37:44 linbit1 kernel: [504655.380139] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: self C6D5DA1F69944504:674E2E475BF406B2:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:37:44 linbit1 kernel: [504655.380148] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:37:44 linbit1 kernel: [504655.380155] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:37:44 linbit1 kernel: [504655.380160] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:37:44 linbit1 kernel: [504655.383173] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: State change 3373070157: primary_nodes=0, weak_nodes=0
Mar 13 20:37:44 linbit1 kernel: [504655.383183] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 3373070157 (36ms)
Mar 13 20:37:44 linbit1 kernel: [504655.383232] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:37:44 linbit1 kernel: [504655.383237] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: quorum( no -> yes )
Mar 13 20:37:44 linbit1 kernel: [504655.383242] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:37:44 linbit1 kernel: [504655.384638] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:44 linbit1 kernel: [504655.384663] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Preparing remote state change 3144939
Mar 13 20:37:44 linbit1 kernel: [504655.388225] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:37:44 linbit1 kernel: [504655.388246] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:37:44 linbit1 kernel: [504655.388410] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:37:44 linbit1 kernel: [504655.422027] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Committing remote state change 3144939 (primary_nodes=0)
Mar 13 20:37:45 linbit1 kernel: [504656.831867] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: rs_discard_granularity feature disabled
Mar 13 20:37:46 linbit1 kernel: [504658.000386] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: rs_discard_granularity feature disabled
Mar 13 20:37:47 linbit1 kernel: [504658.063408] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: rs_discard_granularity feature disabled
Mar 13 20:37:47 linbit1 kernel: [504658.140404] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: rs_discard_granularity feature disabled
Mar 13 20:37:47 linbit1 kernel: [504658.199299] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: rs_discard_granularity feature disabled
Mar 13 20:37:47 linbit1 kernel: [504658.261596] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: rs_discard_granularity feature disabled
Mar 13 20:37:47 linbit1 kernel: [504658.333547] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: rs_discard_granularity feature disabled
Mar 13 20:37:49 linbit1 kernel: [504660.227069] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: rs_discard_granularity feature disabled
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321328    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321405    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="3e3f5565-2ed4-4a44-99e9-a213c3671a80" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321431    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="2d42c73e-6b31-477e-8a93-f70c00244dd0" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321449    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0590b140-3073-4207-afce-9b3f9c3c3444" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321467    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b56a53cc-cc74-4f87-87cc-d5939cac4e14" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321482    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c8e99c44-3a56-4111-8da7-16b323c11a48" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321500    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="ad18fb5a-93d8-4b1e-86ef-b6b8ad4c7356" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321518    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="d28ecb81-bdfa-41c8-9493-1646526814c2" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321532    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="3e3f5565-2ed4-4a44-99e9-a213c3671a80" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321549    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="d28ecb81-bdfa-41c8-9493-1646526814c2" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321566    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="0590b140-3073-4207-afce-9b3f9c3c3444" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321582    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="222a4d56-8ed9-4ccd-80ea-5b88188b43df" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321600    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8c9ea299-749a-4810-a3db-a7beb1533844" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321614    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="97d6d665-457e-4ddc-ad75-dc53d3868d56" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321629    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="fd1e29c3-4748-4e44-b092-a4a5f20943d4" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321646    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f38db5c7-2d3f-4304-8b98-baae94396424" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321662    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="29e27e44-1911-4f82-9efc-4d060335d1cb" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321677    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="8c9ea299-749a-4810-a3db-a7beb1533844" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321695    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b56a53cc-cc74-4f87-87cc-d5939cac4e14" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321712    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="97d6d665-457e-4ddc-ad75-dc53d3868d56" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321728    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="222a4d56-8ed9-4ccd-80ea-5b88188b43df" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321744    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="fd1e29c3-4748-4e44-b092-a4a5f20943d4" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321758    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="29e27e44-1911-4f82-9efc-4d060335d1cb" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321774    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="c8e99c44-3a56-4111-8da7-16b323c11a48" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321790    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="f38db5c7-2d3f-4304-8b98-baae94396424" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321806    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="ad18fb5a-93d8-4b1e-86ef-b6b8ad4c7356" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:37:50.321824    2448 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="2d42c73e-6b31-477e-8a93-f70c00244dd0" containerName="wait-for-sync"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321862    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="ad18fb5a-93d8-4b1e-86ef-b6b8ad4c7356" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321876    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="c8e99c44-3a56-4111-8da7-16b323c11a48" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321892    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="3e3f5565-2ed4-4a44-99e9-a213c3671a80" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321906    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="fd1e29c3-4748-4e44-b092-a4a5f20943d4" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321922    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="0590b140-3073-4207-afce-9b3f9c3c3444" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321936    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="f38db5c7-2d3f-4304-8b98-baae94396424" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321951    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="97d6d665-457e-4ddc-ad75-dc53d3868d56" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321965    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="29e27e44-1911-4f82-9efc-4d060335d1cb" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321980    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="222a4d56-8ed9-4ccd-80ea-5b88188b43df" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.321994    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="b56a53cc-cc74-4f87-87cc-d5939cac4e14" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.322007    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="2d42c73e-6b31-477e-8a93-f70c00244dd0" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.322022    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="d28ecb81-bdfa-41c8-9493-1646526814c2" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.322035    2448 memory_manager.go:346] "RemoveStaleState removing state" podUID="8c9ea299-749a-4810-a3db-a7beb1533844" containerName="fio-bench"
Mar 13 20:37:50 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:50.346554    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5qsvc\" (UniqueName: \"kubernetes.io/projected/1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f-kube-api-access-5qsvc\") pod \"fio-bench-r2-n0-0-pml7f\" (UID: \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\") " pod="default/fio-bench-r2-n0-0-pml7f"
Mar 13 20:37:50 linbit1 kernel: [504661.499495] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: rs_discard_granularity feature disabled
Mar 13 20:37:50 linbit1 kernel: [504661.554067] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: rs_discard_granularity feature disabled
Mar 13 20:37:50 linbit1 kernel: [504661.608307] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: rs_discard_granularity feature disabled
Mar 13 20:37:50 linbit1 kernel: [504661.666001] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: rs_discard_granularity feature disabled
Mar 13 20:37:50 linbit1 kernel: [504661.709789] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: rs_discard_granularity feature disabled
Mar 13 20:37:51 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:51.552742    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:37:51 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:51.654054    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rpjrh\" (UniqueName: \"kubernetes.io/projected/ad7770d4-2d2d-4215-a027-2d52b0e30ae0-kube-api-access-rpjrh\") pod \"fio-bench-r2-n1-3-hntjb\" (UID: \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\") " pod="default/fio-bench-r2-n1-3-hntjb"
Mar 13 20:37:52 linbit1 kernel: [504663.697814] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: rs_discard_granularity feature disabled
Mar 13 20:37:53 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:53.326709    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:37:53 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:53.336036    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:37:53 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:53.361227    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:37:53 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:53.464713    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5kb72\" (UniqueName: \"kubernetes.io/projected/5d081096-b326-416f-9081-50608c26d3d1-kube-api-access-5kb72\") pod \"fio-bench-r2-n0-1-559g2\" (UID: \"5d081096-b326-416f-9081-50608c26d3d1\") " pod="default/fio-bench-r2-n0-1-559g2"
Mar 13 20:37:53 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:53.464947    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b88x9\" (UniqueName: \"kubernetes.io/projected/e5888ce9-60e2-4ded-94a7-5ac6c28650b6-kube-api-access-b88x9\") pod \"fio-bench-r2-n0-4-9lr25\" (UID: \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\") " pod="default/fio-bench-r2-n0-4-9lr25"
Mar 13 20:37:53 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:53.546073    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:37:53 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:53.766874    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5v499\" (UniqueName: \"kubernetes.io/projected/f15c9220-13bf-47a1-93d8-bf5be8a9a7a5-kube-api-access-5v499\") pod \"fio-bench-r2-n1-1-vjkzb\" (UID: \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\") " pod="default/fio-bench-r2-n1-1-vjkzb"
Mar 13 20:37:54 linbit1 kernel: [504665.308716] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: rs_discard_granularity feature disabled
Mar 13 20:37:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:54.334318    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:37:54 linbit1 kernel: [504665.388100] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: rs_discard_granularity feature disabled
Mar 13 20:37:54 linbit1 systemd-udevd[2446534]: dm-16: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-16' failed with exit code 1.
Mar 13 20:37:54 linbit1 systemd-udevd[2446534]: dm-17: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-17' failed with exit code 1.
Mar 13 20:37:54 linbit1 kernel: [504665.717098] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Starting worker thread (from drbdsetup [2446598])
Mar 13 20:37:54 linbit1 kernel: [504665.725934] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: meta-data IO uses: blk-bio
Mar 13 20:37:54 linbit1 kernel: [504665.726007] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: rs_discard_granularity feature disabled
Mar 13 20:37:54 linbit1 kernel: [504665.726043] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: disk( Diskless -> Attaching )
Mar 13 20:37:54 linbit1 kernel: [504665.726054] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: Maximum number of peer devices = 7
Mar 13 20:37:54 linbit1 kernel: [504665.726922] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Method to ensure write ordering: drain
Mar 13 20:37:54 linbit1 kernel: [504665.726933] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:54 linbit1 kernel: [504665.734510] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:54 linbit1 kernel: [504665.734519] drbd1012: detected capacity change from 0 to 20971520
Mar 13 20:37:54 linbit1 kernel: [504665.734524] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: size = 10 GB (10485760 KB)
Mar 13 20:37:54 linbit1 systemd-udevd[2446534]: drbd1012: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1012' failed with exit code 1.
Mar 13 20:37:54 linbit1 kernel: [504665.743493] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: recounting of set bits took additional 4ms
Mar 13 20:37:54 linbit1 kernel: [504665.743508] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: disk( Attaching -> Inconsistent )
Mar 13 20:37:54 linbit1 kernel: [504665.743513] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: attached to current UUID: 0000000000000004
Mar 13 20:37:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:55.178765    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zws8f\" (UniqueName: \"kubernetes.io/projected/75ce48a0-322f-463e-9859-01950deee06d-kube-api-access-zws8f\") pod \"fio-bench-r2-n0-2-hmskv\" (UID: \"75ce48a0-322f-463e-9859-01950deee06d\") " pod="default/fio-bench-r2-n0-2-hmskv"
Mar 13 20:37:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:56.186042    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dgb5b\" (UniqueName: \"kubernetes.io/projected/163fb3b6-cd3e-4b72-8f21-6a84e97abdc3-kube-api-access-dgb5b\") pod \"fio-bench-r2-n0-3-bh94n\" (UID: \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\") " pod="default/fio-bench-r2-n0-3-bh94n"
Mar 13 20:37:57 linbit1 kernel: [504668.526889] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: rs_discard_granularity feature disabled
Mar 13 20:37:57 linbit1 kernel: [504668.564146] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: rs_discard_granularity feature disabled
Mar 13 20:37:58 linbit1 kernel: [504669.529627] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Preparing remote state change 3666677578
Mar 13 20:37:58 linbit1 kernel: [504669.530648] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Committing remote state change 3666677578 (primary_nodes=1)
Mar 13 20:37:58 linbit1 kernel: [504669.530659] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: peer( Secondary -> Primary )
Mar 13 20:37:59 linbit1 kernel: [504670.087339] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: rs_discard_granularity feature disabled
Mar 13 20:37:59 linbit1 systemd-udevd[2446773]: dm-18: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-18' failed with exit code 1.
Mar 13 20:37:59 linbit1 kernel: [504670.244254] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Preparing remote state change 3375952872
Mar 13 20:37:59 linbit1 kernel: [504670.244568] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Committing remote state change 3375952872 (primary_nodes=1)
Mar 13 20:37:59 linbit1 kernel: [504670.244579] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: peer( Secondary -> Primary )
Mar 13 20:37:59 linbit1 systemd-udevd[2446772]: dm-19: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-19' failed with exit code 1.
Mar 13 20:37:59 linbit1 kernel: [504670.403426] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Starting worker thread (from drbdsetup [2446823])
Mar 13 20:37:59 linbit1 kernel: [504670.410483] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: meta-data IO uses: blk-bio
Mar 13 20:37:59 linbit1 kernel: [504670.410609] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: rs_discard_granularity feature disabled
Mar 13 20:37:59 linbit1 kernel: [504670.410634] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: disk( Diskless -> Attaching )
Mar 13 20:37:59 linbit1 kernel: [504670.410646] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: Maximum number of peer devices = 7
Mar 13 20:37:59 linbit1 kernel: [504670.411428] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Method to ensure write ordering: drain
Mar 13 20:37:59 linbit1 kernel: [504670.411436] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:59 linbit1 systemd-udevd[2446772]: drbd1017: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1017' failed with exit code 1.
Mar 13 20:37:59 linbit1 kernel: [504670.418475] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:59 linbit1 kernel: [504670.418482] drbd1017: detected capacity change from 0 to 20971520
Mar 13 20:37:59 linbit1 kernel: [504670.418486] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: size = 10 GB (10485760 KB)
Mar 13 20:37:59 linbit1 kernel: [504670.423343] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: recounting of set bits took additional 0ms
Mar 13 20:37:59 linbit1 kernel: [504670.423352] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: disk( Attaching -> Inconsistent )
Mar 13 20:37:59 linbit1 kernel: [504670.423355] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: attached to current UUID: 0000000000000004
Mar 13 20:37:59 linbit1 kernel: [504670.441087] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: rs_discard_granularity feature disabled
Mar 13 20:37:59 linbit1 systemd-udevd[2446772]: dm-20: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-20' failed with exit code 1.
Mar 13 20:37:59 linbit1 systemd-udevd[2446772]: dm-21: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-21' failed with exit code 1.
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.712753    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\") pod \"fio-bench-r2-n0-0-pml7f\" (UID: \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\") " pod="default/fio-bench-r2-n0-0-pml7f"
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.712849    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\") pod \"fio-bench-r2-n1-3-hntjb\" (UID: \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\") " pod="default/fio-bench-r2-n1-3-hntjb"
Mar 13 20:37:59 linbit1 kernel: [504670.731752] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Starting worker thread (from drbdsetup [2446908])
Mar 13 20:37:59 linbit1 kernel: [504670.735965] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Starting sender thread (from drbdsetup [2446913])
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.723051    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\") pod \"fio-bench-r2-n0-0-pml7f\" (UID: \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\") device path: \"\"" pod="default/fio-bench-r2-n0-0-pml7f"
Mar 13 20:37:59 linbit1 kernel: [504670.737538] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Starting sender thread (from drbdsetup [2446916])
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.724752    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\") pod \"fio-bench-r2-n1-3-hntjb\" (UID: \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\") device path: \"\"" pod="default/fio-bench-r2-n1-3-hntjb"
Mar 13 20:37:59 linbit1 systemd-udevd[2446772]: drbd1011: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1011' failed with exit code 1.
Mar 13 20:37:59 linbit1 kernel: [504670.793632] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: meta-data IO uses: blk-bio
Mar 13 20:37:59 linbit1 kernel: [504670.793703] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: rs_discard_granularity feature disabled
Mar 13 20:37:59 linbit1 kernel: [504670.793745] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: disk( Diskless -> Attaching )
Mar 13 20:37:59 linbit1 kernel: [504670.793758] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: Maximum number of peer devices = 7
Mar 13 20:37:59 linbit1 kernel: [504670.794623] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Method to ensure write ordering: drain
Mar 13 20:37:59 linbit1 kernel: [504670.794633] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: drbd_bm_resize called with capacity == 20971520
Mar 13 20:37:59 linbit1 kernel: [504670.802076] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:37:59 linbit1 kernel: [504670.802083] drbd1011: detected capacity change from 0 to 20971520
Mar 13 20:37:59 linbit1 kernel: [504670.802088] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: size = 10 GB (10485760 KB)
Mar 13 20:37:59 linbit1 kernel: [504670.811581] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: recounting of set bits took additional 4ms
Mar 13 20:37:59 linbit1 kernel: [504670.811601] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: disk( Attaching -> Inconsistent )
Mar 13 20:37:59 linbit1 kernel: [504670.811607] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: attached to current UUID: 0000000000000004
Mar 13 20:37:59 linbit1 kernel: [504670.813069] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:37:59 linbit1 kernel: [504670.813219] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Starting receiver thread (from drbd_w_pvc-654a [2446909])
Mar 13 20:37:59 linbit1 kernel: [504670.813350] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Unconnected -> Connecting )
Mar 13 20:37:59 linbit1 kernel: [504670.814243] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:37:59 linbit1 kernel: [504670.814304] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Starting receiver thread (from drbd_w_pvc-654a [2446909])
Mar 13 20:37:59 linbit1 kernel: [504670.814419] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( Unconnected -> Connecting )
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.814614    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\") pod \"fio-bench-r2-n0-0-pml7f\" (UID: \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\") DevicePath \"\"" pod="default/fio-bench-r2-n0-0-pml7f"
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.814858    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\") pod \"fio-bench-r2-n1-3-hntjb\" (UID: \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\") DevicePath \"\"" pod="default/fio-bench-r2-n1-3-hntjb"
Mar 13 20:37:59 linbit1 systemd-udevd[2446772]: dm-22: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-22' failed with exit code 1.
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.821418    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\") pod \"fio-bench-r2-n0-0-pml7f\" (UID: \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\") DevicePath \"csi-e7dbd1760198db829ad19315e7d405805ec36c7e9231010cade5a87804675afa\"" pod="default/fio-bench-r2-n0-0-pml7f"
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.825130    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\") pod \"fio-bench-r2-n1-3-hntjb\" (UID: \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\") DevicePath \"csi-c4f5a5d1320085b9ae36cc1f4086db4495f4f6ab90936e733e4891f6b0c48be3\"" pod="default/fio-bench-r2-n1-3-hntjb"
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.829149    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:37:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:37:59.831128    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:37:59 linbit1 kernel: [504670.886983] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 1500695924 (0->-1 3/1)
Mar 13 20:37:59 linbit1 kernel: [504670.887196] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change 1500695924: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:37:59 linbit1 kernel: [504670.887201] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 1500695924 (0ms)
Mar 13 20:37:59 linbit1 kernel: [504670.887210] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: role( Secondary -> Primary )
Mar 13 20:37:59 linbit1 kernel: [504670.887463] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 829462039 (0->-1 3/1)
Mar 13 20:37:59 linbit1 kernel: [504670.887499] loop0: detected capacity change from 0 to 20971520
Mar 13 20:37:59 linbit1 kernel: [504670.887695] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change 829462039: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:37:59 linbit1 kernel: [504670.887701] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 829462039 (0ms)
Mar 13 20:37:59 linbit1 kernel: [504670.887710] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: role( Secondary -> Primary )
Mar 13 20:37:59 linbit1 kernel: [504670.916185] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 2653269014 (0->-1 3/2)
Mar 13 20:37:59 linbit1 kernel: [504670.916442] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change 2653269014: primary_nodes=0, weak_nodes=0
Mar 13 20:37:59 linbit1 kernel: [504670.916448] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 2653269014 (0ms)
Mar 13 20:37:59 linbit1 kernel: [504670.916483] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: role( Primary -> Secondary )
Mar 13 20:37:59 linbit1 systemd-udevd[2446992]: dm-23: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-23' failed with exit code 1.
Mar 13 20:37:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:37:59.982310714Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n1-3-hntjb,Uid:ad7770d4-2d2d-4215-a027-2d52b0e30ae0,Namespace:default,Attempt:0,}"
Mar 13 20:38:00 linbit1 kernel: [504671.070936] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Starting worker thread (from drbdsetup [2447052])
Mar 13 20:38:00 linbit1 kernel: [504671.078872] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: meta-data IO uses: blk-bio
Mar 13 20:38:00 linbit1 kernel: [504671.078963] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: rs_discard_granularity feature disabled
Mar 13 20:38:00 linbit1 kernel: [504671.079024] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: disk( Diskless -> Attaching )
Mar 13 20:38:00 linbit1 kernel: [504671.079035] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: Maximum number of peer devices = 7
Mar 13 20:38:00 linbit1 kernel: [504671.079916] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Method to ensure write ordering: drain
Mar 13 20:38:00 linbit1 kernel: [504671.079926] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:00 linbit1 kernel: [504671.087521] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:00 linbit1 kernel: [504671.087529] drbd1019: detected capacity change from 0 to 20971520
Mar 13 20:38:00 linbit1 kernel: [504671.087533] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: size = 10 GB (10485760 KB)
Mar 13 20:38:00 linbit1 systemd-udevd[2446772]: drbd1019: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1019' failed with exit code 1.
Mar 13 20:38:00 linbit1 kernel: [504671.092879] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: recounting of set bits took additional 4ms
Mar 13 20:38:00 linbit1 kernel: [504671.092891] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: disk( Attaching -> Inconsistent )
Mar 13 20:38:00 linbit1 kernel: [504671.092893] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: attached to current UUID: 0000000000000004
Mar 13 20:38:00 linbit1 systemd-udevd[2446772]: dm-24: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-24' failed with exit code 1.
Mar 13 20:38:00 linbit1 kernel: [504671.144327] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 764592839 (0->-1 3/1)
Mar 13 20:38:00 linbit1 kernel: [504671.147184] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change 764592839: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:00 linbit1 kernel: [504671.147190] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 764592839 (0ms)
Mar 13 20:38:00 linbit1 kernel: [504671.147202] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: role( Secondary -> Primary )
Mar 13 20:38:00 linbit1 kernel: [504671.148511] loop11: detected capacity change from 0 to 20971520
Mar 13 20:38:00 linbit1 systemd-udevd[2446772]: dm-25: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-25' failed with exit code 1.
Mar 13 20:38:00 linbit1 kernel: [504671.236547] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:38:00 linbit1 kernel: [504671.236703] IPv6: ADDRCONF(NETDEV_CHANGE): califebb37077c9: link becomes ready
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.258575709Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-0-pml7f,Uid:1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f,Namespace:default,Attempt:0,}"
Mar 13 20:38:00 linbit1 systemd-networkd[2297221]: califebb37077c9: Link UP
Mar 13 20:38:00 linbit1 systemd-networkd[2297221]: califebb37077c9: Gained carrier
Mar 13 20:38:00 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 94 seen, reloading interface list
Mar 13 20:38:00 linbit1 systemd-udevd[2447099]: Using default interface naming scheme 'v249'.
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.016 [INFO][2447003] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.043 [INFO][2447003] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0 fio-bench-r2-n1-3- default  ad7770d4-2d2d-4215-a027-2d52b0e30ae0 17523391 0 2023-03-13 20:37:26 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:2c62deca-9bef-4fe5-8cb0-cea805ea49c6 job-name:fio-bench-r2-n1-3 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n1-3-hntjb eth0 default [] []   [kns.default ksa.default.default] califebb37077c9  [] []}} ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Namespace="default" Pod="fio-bench-r2-n1-3-hntjb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.043 [INFO][2447003] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Namespace="default" Pod="fio-bench-r2-n1-3-hntjb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.089 [INFO][2447060] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.127 [INFO][2447060] ipam_plugin.go 264: Auto assigning IP ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x400071c200), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n1-3-hntjb", "timestamp":"2023-03-13 20:38:00.089845959 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.127 [INFO][2447060] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.136 [INFO][2447060] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.151 [INFO][2447060] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.163 [INFO][2447060] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.170 [INFO][2447060] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.177 [INFO][2447060] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.178 [INFO][2447060] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.186 [INFO][2447060] ipam.go 1680: Creating new handle: k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.197 [INFO][2447060] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.213 [INFO][2447060] ipam.go 1214: Successfully claimed IPs: [10.1.217.7/26] block=10.1.217.0/26 handle="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.213 [INFO][2447060] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.7/26] handle="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.214 [INFO][2447060] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.7/26] IPv6=[] ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.218 [INFO][2447003] k8s.go 383: Populated endpoint ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Namespace="default" Pod="fio-bench-r2-n1-3-hntjb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0", GenerateName:"fio-bench-r2-n1-3-", Namespace:"default", SelfLink:"", UID:"ad7770d4-2d2d-4215-a027-2d52b0e30ae0", ResourceVersion:"17523391", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"2c62deca-9bef-4fe5-8cb0-cea805ea49c6", "job-name":"fio-bench-r2-n1-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n1-3-hntjb", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.7/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"califebb37077c9", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.218 [INFO][2447003] k8s.go 384: Calico CNI using IPs: [10.1.217.7/32] ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Namespace="default" Pod="fio-bench-r2-n1-3-hntjb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.218 [INFO][2447003] dataplane_linux.go 68: Setting the host side veth name to califebb37077c9 ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Namespace="default" Pod="fio-bench-r2-n1-3-hntjb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.220 [INFO][2447003] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Namespace="default" Pod="fio-bench-r2-n1-3-hntjb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.269 [INFO][2447003] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Namespace="default" Pod="fio-bench-r2-n1-3-hntjb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0", GenerateName:"fio-bench-r2-n1-3-", Namespace:"default", SelfLink:"", UID:"ad7770d4-2d2d-4215-a027-2d52b0e30ae0", ResourceVersion:"17523391", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"2c62deca-9bef-4fe5-8cb0-cea805ea49c6", "job-name":"fio-bench-r2-n1-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf", Pod:"fio-bench-r2-n1-3-hntjb", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.7/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"califebb37077c9", MAC:"96:63:09:34:b3:b8", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.286 [INFO][2447003] k8s.go 489: Wrote updated endpoint to datastore ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Namespace="default" Pod="fio-bench-r2-n1-3-hntjb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.313776835Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.313879076Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.313906996Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.314219357Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf pid=2447183 runtime=io.containerd.runc.v2
Mar 13 20:38:00 linbit1 kernel: [504671.354752] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Starting worker thread (from drbdsetup [2447223])
Mar 13 20:38:00 linbit1 kernel: [504671.363531] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: meta-data IO uses: blk-bio
Mar 13 20:38:00 linbit1 kernel: [504671.363625] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: rs_discard_granularity feature disabled
Mar 13 20:38:00 linbit1 kernel: [504671.363705] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: disk( Diskless -> Attaching )
Mar 13 20:38:00 linbit1 kernel: [504671.363717] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: Maximum number of peer devices = 7
Mar 13 20:38:00 linbit1 kernel: [504671.364130] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:00 linbit1 kernel: [504671.364139] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:00 linbit1 kernel: [504671.364301] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:00 linbit1 kernel: [504671.364651] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Method to ensure write ordering: drain
Mar 13 20:38:00 linbit1 kernel: [504671.364661] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:00 linbit1 kernel: [504671.372224] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:00 linbit1 kernel: [504671.372232] drbd1018: detected capacity change from 0 to 20971520
Mar 13 20:38:00 linbit1 kernel: [504671.372236] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: size = 10 GB (10485760 KB)
Mar 13 20:38:00 linbit1 systemd-udevd[2446772]: drbd1018: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1018' failed with exit code 1.
Mar 13 20:38:00 linbit1 kernel: [504671.381166] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: recounting of set bits took additional 8ms
Mar 13 20:38:00 linbit1 kernel: [504671.381182] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: disk( Attaching -> Inconsistent )
Mar 13 20:38:00 linbit1 kernel: [504671.381187] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: attached to current UUID: 0000000000000004
Mar 13 20:38:00 linbit1 kernel: [504671.406571] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: rs_discard_granularity feature disabled
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.414076458Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n1-3-hntjb,Uid:ad7770d4-2d2d-4215-a027-2d52b0e30ae0,Namespace:default,Attempt:0,} returns sandbox id \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\""
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.415956584Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:00 linbit1 kernel: [504671.458959] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Preparing remote state change 3270469498
Mar 13 20:38:00 linbit1 kernel: [504671.475955] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: rs_discard_granularity feature disabled
Mar 13 20:38:00 linbit1 kernel: [504671.492101] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: drbd_sync_handshake:
Mar 13 20:38:00 linbit1 kernel: [504671.492110] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: self 0000000000000004:0000000000000000:C0DAB66176F1E84C:0000000000000000 bits:0 flags:24
Mar 13 20:38:00 linbit1 kernel: [504671.492119] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: peer CACE1D3C53DDAC64:C0DAB66176F1E84D:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:00 linbit1 kernel: [504671.492127] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: uuid_compare()=target-set-bitmap by rule=just-created-self
Mar 13 20:38:00 linbit1 kernel: [504671.492133] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: Setting and writing the whole bitmap, fresh node
Mar 13 20:38:00 linbit1 kernel: [504671.495191] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Committing remote state change 3270469498 (primary_nodes=0)
Mar 13 20:38:00 linbit1 kernel: [504671.495210] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:00 linbit1 kernel: [504671.495214] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: quorum( no -> yes )
Mar 13 20:38:00 linbit1 kernel: [504671.495219] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:38:00 linbit1 kernel: [504671.495318] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: size = 10 GB (10485760 KB)
Mar 13 20:38:00 linbit1 kernel: [504671.497819] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:00 linbit1 kernel: [504671.498985] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:00 linbit1 kernel: [504671.498994] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: setting UUIDs to C0DAB66176F1E84C:0000000000000000:C0DAB66176F1E84C:0000000000000000
Mar 13 20:38:00 linbit1 kernel: [504671.499013] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: repl( WFBitMapT -> SyncTarget )
Mar 13 20:38:00 linbit1 kernel: [504671.499017] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit2: resync-susp( no -> connection dependency )
Mar 13 20:38:00 linbit1 kernel: [504671.499204] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: Began resync as SyncTarget (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:00 linbit1 kernel: [504671.499912] IPv6: ADDRCONF(NETDEV_CHANGE): caliccb68bb9c36: link becomes ready
Mar 13 20:38:00 linbit1 kernel: [504671.525716] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: rs_discard_granularity feature disabled
Mar 13 20:38:00 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 95 seen, reloading interface list
Mar 13 20:38:00 linbit1 systemd-networkd[2297221]: caliccb68bb9c36: Link UP
Mar 13 20:38:00 linbit1 systemd-networkd[2297221]: caliccb68bb9c36: Gained carrier
Mar 13 20:38:00 linbit1 kernel: [504671.570279] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 2297171364 (0->-1 7683/4609)
Mar 13 20:38:00 linbit1 kernel: [504671.570288] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 2297171364 (0ms)
Mar 13 20:38:00 linbit1 kernel: [504671.570293] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: role( Secondary -> Primary )
Mar 13 20:38:00 linbit1 kernel: [504671.570297] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: disk( Inconsistent -> UpToDate )
Mar 13 20:38:00 linbit1 kernel: [504671.570371] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: size = 10 GB (10485760 KB)
Mar 13 20:38:00 linbit1 kernel: [504671.570516] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Forced to consider local data as UpToDate!
Mar 13 20:38:00 linbit1 kernel: [504671.570606] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: new current UUID: 4AA31DF55B1E1999 weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.301 [INFO][2447126] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.325 [INFO][2447126] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0 fio-bench-r2-n0-0- default  1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f 17523290 0 2023-03-13 20:37:25 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:24cec60f-fd8f-4207-ad8e-fdb3d5db24b4 job-name:fio-bench-r2-n0-0 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n0-0-pml7f eth0 default [] []   [kns.default ksa.default.default] caliccb68bb9c36  [] []}} ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Namespace="default" Pod="fio-bench-r2-n0-0-pml7f" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.325 [INFO][2447126] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Namespace="default" Pod="fio-bench-r2-n0-0-pml7f" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.369 [INFO][2447226] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.393 [INFO][2447226] ipam_plugin.go 264: Auto assigning IP ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x400062f6b0), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n0-0-pml7f", "timestamp":"2023-03-13 20:38:00.369871805 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.393 [INFO][2447226] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.408 [INFO][2447226] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.420 [INFO][2447226] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.431 [INFO][2447226] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.436 [INFO][2447226] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.444 [INFO][2447226] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.444 [INFO][2447226] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.449 [INFO][2447226] ipam.go 1680: Creating new handle: k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.459 [INFO][2447226] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.476 [INFO][2447226] ipam.go 1214: Successfully claimed IPs: [10.1.217.61/26] block=10.1.217.0/26 handle="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.477 [INFO][2447226] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.61/26] handle="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" host="linbit1"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.477 [INFO][2447226] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.61/26] IPv6=[] ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.481 [INFO][2447126] k8s.go 383: Populated endpoint ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Namespace="default" Pod="fio-bench-r2-n0-0-pml7f" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0", GenerateName:"fio-bench-r2-n0-0-", Namespace:"default", SelfLink:"", UID:"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f", ResourceVersion:"17523290", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"24cec60f-fd8f-4207-ad8e-fdb3d5db24b4", "job-name":"fio-bench-r2-n0-0", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n0-0-pml7f", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.61/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"caliccb68bb9c36", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.481 [INFO][2447126] k8s.go 384: Calico CNI using IPs: [10.1.217.61/32] ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Namespace="default" Pod="fio-bench-r2-n0-0-pml7f" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:38:00 linbit1 kernel: [504671.577100] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 619143779 (0->-1 3/2)
Mar 13 20:38:00 linbit1 kernel: [504671.577110] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 619143779 (0ms)
Mar 13 20:38:00 linbit1 kernel: [504671.577118] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: role( Primary -> Secondary )
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.481 [INFO][2447126] dataplane_linux.go 68: Setting the host side veth name to caliccb68bb9c36 ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Namespace="default" Pod="fio-bench-r2-n0-0-pml7f" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.484 [INFO][2447126] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Namespace="default" Pod="fio-bench-r2-n0-0-pml7f" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.537 [INFO][2447126] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Namespace="default" Pod="fio-bench-r2-n0-0-pml7f" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0", GenerateName:"fio-bench-r2-n0-0-", Namespace:"default", SelfLink:"", UID:"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f", ResourceVersion:"17523290", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"24cec60f-fd8f-4207-ad8e-fdb3d5db24b4", "job-name":"fio-bench-r2-n0-0", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc", Pod:"fio-bench-r2-n0-0-pml7f", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.61/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"caliccb68bb9c36", MAC:"1a:cc:98:be:fa:f5", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:00.555 [INFO][2447126] k8s.go 489: Wrote updated endpoint to datastore ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Namespace="default" Pod="fio-bench-r2-n0-0-pml7f" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:38:00 linbit1 kernel: [504671.582697] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 1682072444 (0->-1 3/1)
Mar 13 20:38:00 linbit1 kernel: [504671.582703] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 1682072444 (0ms)
Mar 13 20:38:00 linbit1 kernel: [504671.582707] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: role( Secondary -> Primary )
Mar 13 20:38:00 linbit1 kernel: [504671.590098] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 2612505225 (0->-1 3/2)
Mar 13 20:38:00 linbit1 kernel: [504671.590108] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 2612505225 (0ms)
Mar 13 20:38:00 linbit1 kernel: [504671.590116] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: role( Primary -> Secondary )
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.586313379Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.586434379Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.586460179Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.586758980Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc pid=2447353 runtime=io.containerd.runc.v2
Mar 13 20:38:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:00.685948360Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-0-pml7f,Uid:1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f,Namespace:default,Attempt:0,} returns sandbox id \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\""
Mar 13 20:38:01 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:01.127605    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\") pod \"fio-bench-r2-n0-4-9lr25\" (UID: \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\") " pod="default/fio-bench-r2-n0-4-9lr25"
Mar 13 20:38:01 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:01.137632    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\") pod \"fio-bench-r2-n0-4-9lr25\" (UID: \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\") device path: \"\"" pod="default/fio-bench-r2-n0-4-9lr25"
Mar 13 20:38:01 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:01.228544    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\") pod \"fio-bench-r2-n0-4-9lr25\" (UID: \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\") DevicePath \"\"" pod="default/fio-bench-r2-n0-4-9lr25"
Mar 13 20:38:01 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:01.234254    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\") pod \"fio-bench-r2-n0-4-9lr25\" (UID: \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\") DevicePath \"csi-dbf341461dbd5695df051678b5c4c34ebe918ecb5652e7730f070cb61b606e9e\"" pod="default/fio-bench-r2-n0-4-9lr25"
Mar 13 20:38:01 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:01.239874    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:01 linbit1 kernel: [504672.292527] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 745174054 (0->-1 3/1)
Mar 13 20:38:01 linbit1 kernel: [504672.292781] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: State change 745174054: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:01 linbit1 kernel: [504672.292786] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 745174054 (0ms)
Mar 13 20:38:01 linbit1 kernel: [504672.292795] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: role( Secondary -> Primary )
Mar 13 20:38:01 linbit1 kernel: [504672.292994] loop12: detected capacity change from 0 to 20971520
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01.458693935Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-4-9lr25,Uid:e5888ce9-60e2-4ded-94a7-5ac6c28650b6,Namespace:default,Attempt:0,}"
Mar 13 20:38:01 linbit1 kernel: [504672.556019] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Preparing remote state change 1671451358
Mar 13 20:38:01 linbit1 kernel: [504672.556401] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Committing remote state change 1671451358 (primary_nodes=1)
Mar 13 20:38:01 linbit1 kernel: [504672.556417] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: peer( Secondary -> Primary )
Mar 13 20:38:01 linbit1 kernel: [504672.709649] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:38:01 linbit1 kernel: [504672.709806] IPv6: ADDRCONF(NETDEV_CHANGE): calib820d340f7b: link becomes ready
Mar 13 20:38:01 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 96 seen, reloading interface list
Mar 13 20:38:01 linbit1 systemd-networkd[2297221]: calib820d340f7b: Link UP
Mar 13 20:38:01 linbit1 systemd-networkd[2297221]: calib820d340f7b: Gained carrier
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.494 [INFO][2447442] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.522 [INFO][2447442] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0 fio-bench-r2-n0-4- default  e5888ce9-60e2-4ded-94a7-5ac6c28650b6 17523532 0 2023-03-13 20:37:26 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:4dfa7d3c-7c83-4984-ad19-8b1d2eedc894 job-name:fio-bench-r2-n0-4 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n0-4-9lr25 eth0 default [] []   [kns.default ksa.default.default] calib820d340f7b  [] []}} ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Namespace="default" Pod="fio-bench-r2-n0-4-9lr25" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.522 [INFO][2447442] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Namespace="default" Pod="fio-bench-r2-n0-4-9lr25" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.566 [INFO][2447465] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.594 [INFO][2447465] ipam_plugin.go 264: Auto assigning IP ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x40004b3610), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n0-4-9lr25", "timestamp":"2023-03-13 20:38:01.566079899 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.594 [INFO][2447465] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.605 [INFO][2447465] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.622 [INFO][2447465] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.633 [INFO][2447465] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.637 [INFO][2447465] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.649 [INFO][2447465] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.649 [INFO][2447465] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.656 [INFO][2447465] ipam.go 1680: Creating new handle: k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.667 [INFO][2447465] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.686 [INFO][2447465] ipam.go 1214: Successfully claimed IPs: [10.1.217.23/26] block=10.1.217.0/26 handle="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.686 [INFO][2447465] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.23/26] handle="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" host="linbit1"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.686 [INFO][2447465] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.23/26] IPv6=[] ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.691 [INFO][2447442] k8s.go 383: Populated endpoint ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Namespace="default" Pod="fio-bench-r2-n0-4-9lr25" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0", GenerateName:"fio-bench-r2-n0-4-", Namespace:"default", SelfLink:"", UID:"e5888ce9-60e2-4ded-94a7-5ac6c28650b6", ResourceVersion:"17523532", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"4dfa7d3c-7c83-4984-ad19-8b1d2eedc894", "job-name":"fio-bench-r2-n0-4", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n0-4-9lr25", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.23/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"calib820d340f7b", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.691 [INFO][2447442] k8s.go 384: Calico CNI using IPs: [10.1.217.23/32] ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Namespace="default" Pod="fio-bench-r2-n0-4-9lr25" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.691 [INFO][2447442] dataplane_linux.go 68: Setting the host side veth name to calib820d340f7b ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Namespace="default" Pod="fio-bench-r2-n0-4-9lr25" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.693 [INFO][2447442] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Namespace="default" Pod="fio-bench-r2-n0-4-9lr25" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.737 [INFO][2447442] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Namespace="default" Pod="fio-bench-r2-n0-4-9lr25" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0", GenerateName:"fio-bench-r2-n0-4-", Namespace:"default", SelfLink:"", UID:"e5888ce9-60e2-4ded-94a7-5ac6c28650b6", ResourceVersion:"17523532", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"4dfa7d3c-7c83-4984-ad19-8b1d2eedc894", "job-name":"fio-bench-r2-n0-4", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672", Pod:"fio-bench-r2-n0-4-9lr25", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.23/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"calib820d340f7b", MAC:"6e:27:a3:90:ac:23", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:01.755 [INFO][2447442] k8s.go 489: Wrote updated endpoint to datastore ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Namespace="default" Pod="fio-bench-r2-n0-4-9lr25" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01.784739720Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01.784860840Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01.784887320Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01.785239561Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672 pid=2447516 runtime=io.containerd.runc.v2
Mar 13 20:38:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:01.878155122Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-4-9lr25,Uid:e5888ce9-60e2-4ded-94a7-5ac6c28650b6,Namespace:default,Attempt:0,} returns sandbox id \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\""
Mar 13 20:38:02 linbit1 kernel: [504673.042201] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:02 linbit1 kernel: [504673.042213] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:02 linbit1 kernel: [504673.042340] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:02 linbit1 kernel: [504673.122928] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Preparing remote state change 3059333345
Mar 13 20:38:02 linbit1 kernel: [504673.173007] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Committing remote state change 3059333345 (primary_nodes=0)
Mar 13 20:38:02 linbit1 kernel: [504673.216056] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Preparing cluster-wide state change 1518837804 (1->2 499/146)
Mar 13 20:38:02 linbit1 kernel: [504673.240050] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit2: self CACE1D3C53DDAC64:C0DAB66176F1E84C:C0DAB66176F1E84C:0000000000000000 bits:0 flags:0
Mar 13 20:38:02 linbit1 kernel: [504673.240062] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:02 linbit1 kernel: [504673.246117] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: State change 1518837804: primary_nodes=0, weak_nodes=0
Mar 13 20:38:02 linbit1 kernel: [504673.246125] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Committing cluster-wide state change 1518837804 (28ms)
Mar 13 20:38:02 linbit1 kernel: [504673.246169] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:02 linbit1 kernel: [504673.246175] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:02 linbit1 systemd-networkd[2297221]: califebb37077c9: Gained IPv6LL
Mar 13 20:38:02 linbit1 systemd-networkd[2297221]: caliccb68bb9c36: Gained IPv6LL
Mar 13 20:38:03 linbit1 kernel: [504674.160736] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: rs_discard_granularity feature disabled
Mar 13 20:38:03 linbit1 kernel: [504674.197344] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 2493362983 (0->-1 7683/4609)
Mar 13 20:38:03 linbit1 kernel: [504674.197355] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 2493362983 (0ms)
Mar 13 20:38:03 linbit1 kernel: [504674.197362] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: role( Secondary -> Primary )
Mar 13 20:38:03 linbit1 kernel: [504674.197366] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: disk( Inconsistent -> UpToDate )
Mar 13 20:38:03 linbit1 kernel: [504674.197432] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: size = 10 GB (10485760 KB)
Mar 13 20:38:03 linbit1 kernel: [504674.197576] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Forced to consider local data as UpToDate!
Mar 13 20:38:03 linbit1 kernel: [504674.197663] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: new current UUID: 9ED24C56D5CABD9F weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:03 linbit1 kernel: [504674.204688] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 3031064140 (0->-1 3/2)
Mar 13 20:38:03 linbit1 kernel: [504674.204697] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 3031064140 (0ms)
Mar 13 20:38:03 linbit1 kernel: [504674.204705] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: role( Primary -> Secondary )
Mar 13 20:38:03 linbit1 kernel: [504674.211723] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 1655164695 (0->-1 3/1)
Mar 13 20:38:03 linbit1 kernel: [504674.211732] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 1655164695 (0ms)
Mar 13 20:38:03 linbit1 kernel: [504674.211739] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: role( Secondary -> Primary )
Mar 13 20:38:03 linbit1 kernel: [504674.218512] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 2564411618 (0->-1 3/2)
Mar 13 20:38:03 linbit1 kernel: [504674.218519] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 2564411618 (0ms)
Mar 13 20:38:03 linbit1 kernel: [504674.218524] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: role( Primary -> Secondary )
Mar 13 20:38:03 linbit1 kernel: [504674.232299] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: rs_discard_granularity feature disabled
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.241188    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\") pod \"fio-bench-r2-n0-2-hmskv\" (UID: \"75ce48a0-322f-463e-9859-01950deee06d\") " pod="default/fio-bench-r2-n0-2-hmskv"
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.254483    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\") pod \"fio-bench-r2-n0-2-hmskv\" (UID: \"75ce48a0-322f-463e-9859-01950deee06d\") device path: \"\"" pod="default/fio-bench-r2-n0-2-hmskv"
Mar 13 20:38:03 linbit1 kernel: [504674.297790] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: rs_discard_granularity feature disabled
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.342253    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\") pod \"fio-bench-r2-n0-2-hmskv\" (UID: \"75ce48a0-322f-463e-9859-01950deee06d\") DevicePath \"\"" pod="default/fio-bench-r2-n0-2-hmskv"
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.350442    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\") pod \"fio-bench-r2-n0-2-hmskv\" (UID: \"75ce48a0-322f-463e-9859-01950deee06d\") DevicePath \"csi-58417343e0edfc70295e89378a86c59bff8aadf948427910f43ba69866cec74a\"" pod="default/fio-bench-r2-n0-2-hmskv"
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.357450    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:03 linbit1 kernel: [504674.579498] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 2872072457 (0->-1 3/1)
Mar 13 20:38:03 linbit1 kernel: [504674.579736] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: State change 2872072457: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:03 linbit1 kernel: [504674.579742] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 2872072457 (0ms)
Mar 13 20:38:03 linbit1 kernel: [504674.579752] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: role( Secondary -> Primary )
Mar 13 20:38:03 linbit1 kernel: [504674.580032] loop13: detected capacity change from 0 to 20971520
Mar 13 20:38:03 linbit1 systemd-networkd[2297221]: calib820d340f7b: Gained IPv6LL
Mar 13 20:38:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:03.798676245Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-2-hmskv,Uid:75ce48a0-322f-463e-9859-01950deee06d,Namespace:default,Attempt:0,}"
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.946063    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-d1746b46-d2be-4657-aaf4-05198959329b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b\") pod \"fio-bench-r2-n0-1-559g2\" (UID: \"5d081096-b326-416f-9081-50608c26d3d1\") " pod="default/fio-bench-r2-n0-1-559g2"
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.946191    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-63619ac2-d122-438b-adb1-8d387095e32b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b\") pod \"fio-bench-r2-n0-3-bh94n\" (UID: \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\") " pod="default/fio-bench-r2-n0-3-bh94n"
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.957959    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-d1746b46-d2be-4657-aaf4-05198959329b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b\") pod \"fio-bench-r2-n0-1-559g2\" (UID: \"5d081096-b326-416f-9081-50608c26d3d1\") device path: \"\"" pod="default/fio-bench-r2-n0-1-559g2"
Mar 13 20:38:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:03.961825    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-63619ac2-d122-438b-adb1-8d387095e32b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b\") pod \"fio-bench-r2-n0-3-bh94n\" (UID: \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\") device path: \"\"" pod="default/fio-bench-r2-n0-3-bh94n"
Mar 13 20:38:04 linbit1 kernel: [504675.063351] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:38:04 linbit1 kernel: [504675.063509] IPv6: ADDRCONF(NETDEV_CHANGE): cali05d59ceb4c6: link becomes ready
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.047547    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-d1746b46-d2be-4657-aaf4-05198959329b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b\") pod \"fio-bench-r2-n0-1-559g2\" (UID: \"5d081096-b326-416f-9081-50608c26d3d1\") DevicePath \"\"" pod="default/fio-bench-r2-n0-1-559g2"
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.047740    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\") pod \"fio-bench-r2-n1-1-vjkzb\" (UID: \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\") " pod="default/fio-bench-r2-n1-1-vjkzb"
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.047809    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-63619ac2-d122-438b-adb1-8d387095e32b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b\") pod \"fio-bench-r2-n0-3-bh94n\" (UID: \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\") DevicePath \"\"" pod="default/fio-bench-r2-n0-3-bh94n"
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.054186    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-d1746b46-d2be-4657-aaf4-05198959329b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b\") pod \"fio-bench-r2-n0-1-559g2\" (UID: \"5d081096-b326-416f-9081-50608c26d3d1\") DevicePath \"csi-730f3bf2dbac1e20c1b46563b8b58e1f91334141e64b73dcdc16c8cbd5a47692\"" pod="default/fio-bench-r2-n0-1-559g2"
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.057639    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-63619ac2-d122-438b-adb1-8d387095e32b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b\") pod \"fio-bench-r2-n0-3-bh94n\" (UID: \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\") DevicePath \"csi-ab9eeb9f80d171fa91d71abb949481305ee498842e85fcf7051a9966bfb907e6\"" pod="default/fio-bench-r2-n0-3-bh94n"
Mar 13 20:38:04 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 97 seen, reloading interface list
Mar 13 20:38:04 linbit1 systemd-networkd[2297221]: cali05d59ceb4c6: Link UP
Mar 13 20:38:04 linbit1 systemd-networkd[2297221]: cali05d59ceb4c6: Gained carrier
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.830 [INFO][2447759] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.862 [INFO][2447759] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0 fio-bench-r2-n0-2- default  75ce48a0-322f-463e-9859-01950deee06d 17523652 0 2023-03-13 20:37:25 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:19ceaf5d-1a86-42d3-99a2-a8e0cb0d08ea job-name:fio-bench-r2-n0-2 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n0-2-hmskv eth0 default [] []   [kns.default ksa.default.default] cali05d59ceb4c6  [] []}} ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Namespace="default" Pod="fio-bench-r2-n0-2-hmskv" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.862 [INFO][2447759] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Namespace="default" Pod="fio-bench-r2-n0-2-hmskv" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.906 [INFO][2447791] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.933 [INFO][2447791] ipam_plugin.go 264: Auto assigning IP ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x4000ba0090), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n0-2-hmskv", "timestamp":"2023-03-13 20:38:03.906626851 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:03Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:03Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.933 [INFO][2447791] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.941 [INFO][2447791] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.955 [INFO][2447791] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.971 [INFO][2447791] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.979 [INFO][2447791] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.987 [INFO][2447791] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.987 [INFO][2447791] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:03.998 [INFO][2447791] ipam.go 1680: Creating new handle: k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.008 [INFO][2447791] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.040 [INFO][2447791] ipam.go 1214: Successfully claimed IPs: [10.1.217.40/26] block=10.1.217.0/26 handle="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.040 [INFO][2447791] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.40/26] handle="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" host="linbit1"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:04Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.040 [INFO][2447791] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.40/26] IPv6=[] ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.044 [INFO][2447759] k8s.go 383: Populated endpoint ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Namespace="default" Pod="fio-bench-r2-n0-2-hmskv" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0", GenerateName:"fio-bench-r2-n0-2-", Namespace:"default", SelfLink:"", UID:"75ce48a0-322f-463e-9859-01950deee06d", ResourceVersion:"17523652", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"19ceaf5d-1a86-42d3-99a2-a8e0cb0d08ea", "job-name":"fio-bench-r2-n0-2", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n0-2-hmskv", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.40/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali05d59ceb4c6", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.044 [INFO][2447759] k8s.go 384: Calico CNI using IPs: [10.1.217.40/32] ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Namespace="default" Pod="fio-bench-r2-n0-2-hmskv" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.045 [INFO][2447759] dataplane_linux.go 68: Setting the host side veth name to cali05d59ceb4c6 ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Namespace="default" Pod="fio-bench-r2-n0-2-hmskv" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.047 [INFO][2447759] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Namespace="default" Pod="fio-bench-r2-n0-2-hmskv" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.089 [INFO][2447759] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Namespace="default" Pod="fio-bench-r2-n0-2-hmskv" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0", GenerateName:"fio-bench-r2-n0-2-", Namespace:"default", SelfLink:"", UID:"75ce48a0-322f-463e-9859-01950deee06d", ResourceVersion:"17523652", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"19ceaf5d-1a86-42d3-99a2-a8e0cb0d08ea", "job-name":"fio-bench-r2-n0-2", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0", Pod:"fio-bench-r2-n0-2-hmskv", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.40/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali05d59ceb4c6", MAC:"ce:b3:76:cf:35:cf", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:04.109 [INFO][2447759] k8s.go 489: Wrote updated endpoint to datastore ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Namespace="default" Pod="fio-bench-r2-n0-2-hmskv" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.118171    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:04.140126077Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:04.140243237Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:04.140292597Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:04.140591038Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0 pid=2447845 runtime=io.containerd.runc.v2
Mar 13 20:38:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:04.239878618Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-2-hmskv,Uid:75ce48a0-322f-463e-9859-01950deee06d,Namespace:default,Attempt:0,} returns sandbox id \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\""
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.333071    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.520744    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\") pod \"fio-bench-r2-n1-1-vjkzb\" (UID: \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\") device path: \"\"" pod="default/fio-bench-r2-n1-1-vjkzb"
Mar 13 20:38:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:04.555319    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\") pod \"fio-bench-r2-n1-1-vjkzb\" (UID: \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\") DevicePath \"\"" pod="default/fio-bench-r2-n1-1-vjkzb"
Mar 13 20:38:04 linbit1 kernel: [504675.769671] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 1463300810 (0->-1 3/1)
Mar 13 20:38:04 linbit1 kernel: [504675.769922] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: State change 1463300810: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:04 linbit1 kernel: [504675.769928] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 1463300810 (0ms)
Mar 13 20:38:04 linbit1 kernel: [504675.769939] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: role( Secondary -> Primary )
Mar 13 20:38:04 linbit1 kernel: [504675.770243] loop14: detected capacity change from 0 to 20971520
Mar 13 20:38:04 linbit1 kernel: [504675.969758] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 1963258772 (0->-1 3/1)
Mar 13 20:38:04 linbit1 kernel: [504675.970000] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: State change 1963258772: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:04 linbit1 kernel: [504675.970006] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 1963258772 (0ms)
Mar 13 20:38:04 linbit1 kernel: [504675.970017] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: role( Secondary -> Primary )
Mar 13 20:38:04 linbit1 kernel: [504675.970266] loop15: detected capacity change from 0 to 20971520
Mar 13 20:38:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:05.120253    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\") pod \"fio-bench-r2-n1-1-vjkzb\" (UID: \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\") DevicePath \"csi-e59262679948e8b34913bae3df2a6e6d28f8f68b606084b97e780169c4433a19\"" pod="default/fio-bench-r2-n1-1-vjkzb"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05.360101683Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-1-559g2,Uid:5d081096-b326-416f-9081-50608c26d3d1,Namespace:default,Attempt:0,}"
Mar 13 20:38:05 linbit1 kernel: [504676.627533] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:38:05 linbit1 kernel: [504676.627681] IPv6: ADDRCONF(NETDEV_CHANGE): cali9bae66a6c22: link becomes ready
Mar 13 20:38:05 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 98 seen, reloading interface list
Mar 13 20:38:05 linbit1 systemd-networkd[2297221]: cali9bae66a6c22: Link UP
Mar 13 20:38:05 linbit1 systemd-networkd[2297221]: cali9bae66a6c22: Gained carrier
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.393 [INFO][2447939] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.427 [INFO][2447939] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0 fio-bench-r2-n0-1- default  5d081096-b326-416f-9081-50608c26d3d1 17523536 0 2023-03-13 20:37:25 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:e2dd3f69-9ac8-49a1-a7f1-ab1ab1294bdd job-name:fio-bench-r2-n0-1 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n0-1-559g2 eth0 default [] []   [kns.default ksa.default.default] cali9bae66a6c22  [] []}} ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Namespace="default" Pod="fio-bench-r2-n0-1-559g2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.427 [INFO][2447939] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Namespace="default" Pod="fio-bench-r2-n0-1-559g2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.472 [INFO][2447966] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.499 [INFO][2447966] ipam_plugin.go 264: Auto assigning IP ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x4000cbdef0), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n0-1-559g2", "timestamp":"2023-03-13 20:38:05.472811624 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.499 [INFO][2447966] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.508 [INFO][2447966] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.522 [INFO][2447966] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.532 [INFO][2447966] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.541 [INFO][2447966] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.548 [INFO][2447966] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.548 [INFO][2447966] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.557 [INFO][2447966] ipam.go 1680: Creating new handle: k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.569 [INFO][2447966] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.604 [INFO][2447966] ipam.go 1214: Successfully claimed IPs: [10.1.217.4/26] block=10.1.217.0/26 handle="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.604 [INFO][2447966] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.4/26] handle="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" host="linbit1"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.604 [INFO][2447966] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.4/26] IPv6=[] ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.608 [INFO][2447939] k8s.go 383: Populated endpoint ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Namespace="default" Pod="fio-bench-r2-n0-1-559g2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0", GenerateName:"fio-bench-r2-n0-1-", Namespace:"default", SelfLink:"", UID:"5d081096-b326-416f-9081-50608c26d3d1", ResourceVersion:"17523536", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"e2dd3f69-9ac8-49a1-a7f1-ab1ab1294bdd", "job-name":"fio-bench-r2-n0-1", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n0-1-559g2", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.4/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali9bae66a6c22", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.609 [INFO][2447939] k8s.go 384: Calico CNI using IPs: [10.1.217.4/32] ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Namespace="default" Pod="fio-bench-r2-n0-1-559g2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.609 [INFO][2447939] dataplane_linux.go 68: Setting the host side veth name to cali9bae66a6c22 ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Namespace="default" Pod="fio-bench-r2-n0-1-559g2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.611 [INFO][2447939] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Namespace="default" Pod="fio-bench-r2-n0-1-559g2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.657 [INFO][2447939] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Namespace="default" Pod="fio-bench-r2-n0-1-559g2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0", GenerateName:"fio-bench-r2-n0-1-", Namespace:"default", SelfLink:"", UID:"5d081096-b326-416f-9081-50608c26d3d1", ResourceVersion:"17523536", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"e2dd3f69-9ac8-49a1-a7f1-ab1ab1294bdd", "job-name":"fio-bench-r2-n0-1", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078", Pod:"fio-bench-r2-n0-1-559g2", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.4/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali9bae66a6c22", MAC:"3a:9a:8b:b9:2a:2b", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.677 [INFO][2447939] k8s.go 489: Wrote updated endpoint to datastore ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Namespace="default" Pod="fio-bench-r2-n0-1-559g2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05.707910134Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05.708037134Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05.708064894Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05.708823257Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078 pid=2448009 runtime=io.containerd.runc.v2
Mar 13 20:38:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:05.719583    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05.786419731Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-3-bh94n,Uid:163fb3b6-cd3e-4b72-8f21-6a84e97abdc3,Namespace:default,Attempt:0,}"
Mar 13 20:38:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05.810153563Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-1-559g2,Uid:5d081096-b326-416f-9081-50608c26d3d1,Namespace:default,Attempt:0,} returns sandbox id \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\""
Mar 13 20:38:05 linbit1 kernel: [504676.968966] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 2343534191 (0->-1 3/1)
Mar 13 20:38:05 linbit1 kernel: [504676.969319] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: State change 2343534191: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:05 linbit1 kernel: [504676.969328] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 2343534191 (0ms)
Mar 13 20:38:05 linbit1 kernel: [504676.969343] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: role( Secondary -> Primary )
Mar 13 20:38:05 linbit1 kernel: [504676.969622] loop16: detected capacity change from 0 to 20971520
Mar 13 20:38:06 linbit1 kernel: [504677.021492] IPv6: ADDRCONF(NETDEV_CHANGE): cali7bcd57bedf4: link becomes ready
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.046777678Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:06 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 99 seen, reloading interface list
Mar 13 20:38:06 linbit1 systemd-networkd[2297221]: cali05d59ceb4c6: Gained IPv6LL
Mar 13 20:38:06 linbit1 systemd-networkd[2297221]: cali7bcd57bedf4: Link UP
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.049516886Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:06 linbit1 systemd-networkd[2297221]: cali7bcd57bedf4: Gained carrier
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.054593301Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.058759234Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.060387239Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.060973641Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.065008333Z" level=info msg="CreateContainer within sandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.818 [INFO][2448041] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.848 [INFO][2448041] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0 fio-bench-r2-n0-3- default  163fb3b6-cd3e-4b72-8f21-6a84e97abdc3 17523664 0 2023-03-13 20:37:25 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:8a039640-9e55-4033-98e7-fa82876427aa job-name:fio-bench-r2-n0-3 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n0-3-bh94n eth0 default [] []   [kns.default ksa.default.default] cali7bcd57bedf4  [] []}} ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Namespace="default" Pod="fio-bench-r2-n0-3-bh94n" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.848 [INFO][2448041] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Namespace="default" Pod="fio-bench-r2-n0-3-bh94n" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.894 [INFO][2448074] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.921 [INFO][2448074] ipam_plugin.go 264: Auto assigning IP ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x4000d8de70), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n0-3-bh94n", "timestamp":"2023-03-13 20:38:05.894838019 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.921 [INFO][2448074] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.928 [INFO][2448074] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.939 [INFO][2448074] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.953 [INFO][2448074] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.958 [INFO][2448074] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.966 [INFO][2448074] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.966 [INFO][2448074] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.971 [INFO][2448074] ipam.go 1680: Creating new handle: k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.982 [INFO][2448074] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.997 [INFO][2448074] ipam.go 1214: Successfully claimed IPs: [10.1.217.54/26] block=10.1.217.0/26 handle="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.998 [INFO][2448074] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.54/26] handle="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:05Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:05.998 [INFO][2448074] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.54/26] IPv6=[] ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.002 [INFO][2448041] k8s.go 383: Populated endpoint ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Namespace="default" Pod="fio-bench-r2-n0-3-bh94n" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0", GenerateName:"fio-bench-r2-n0-3-", Namespace:"default", SelfLink:"", UID:"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3", ResourceVersion:"17523664", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"8a039640-9e55-4033-98e7-fa82876427aa", "job-name":"fio-bench-r2-n0-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n0-3-bh94n", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.54/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali7bcd57bedf4", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.002 [INFO][2448041] k8s.go 384: Calico CNI using IPs: [10.1.217.54/32] ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Namespace="default" Pod="fio-bench-r2-n0-3-bh94n" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.002 [INFO][2448041] dataplane_linux.go 68: Setting the host side veth name to cali7bcd57bedf4 ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Namespace="default" Pod="fio-bench-r2-n0-3-bh94n" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.005 [INFO][2448041] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Namespace="default" Pod="fio-bench-r2-n0-3-bh94n" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.049 [INFO][2448041] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Namespace="default" Pod="fio-bench-r2-n0-3-bh94n" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0", GenerateName:"fio-bench-r2-n0-3-", Namespace:"default", SelfLink:"", UID:"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3", ResourceVersion:"17523664", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 25, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"8a039640-9e55-4033-98e7-fa82876427aa", "job-name":"fio-bench-r2-n0-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba", Pod:"fio-bench-r2-n0-3-bh94n", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.54/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali7bcd57bedf4", MAC:"fe:a9:da:bb:13:d0", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.063 [INFO][2448041] k8s.go 489: Wrote updated endpoint to datastore ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Namespace="default" Pod="fio-bench-r2-n0-3-bh94n" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.073276918Z" level=info msg="CreateContainer within sandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.073716239Z" level=info msg="StartContainer for \"8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.094075941Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.094174541Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.094201661Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.094421782Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba pid=2448145 runtime=io.containerd.runc.v2
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.172566578Z" level=info msg="StartContainer for \"8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291\" returns successfully"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.185754778Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n0-3-bh94n,Uid:163fb3b6-cd3e-4b72-8f21-6a84e97abdc3,Namespace:default,Attempt:0,} returns sandbox id \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.265242898Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n1-1-vjkzb,Uid:f15c9220-13bf-47a1-93d8-bf5be8a9a7a5,Namespace:default,Attempt:0,}"
Mar 13 20:38:06 linbit1 kernel: [504677.508682] IPv6: ADDRCONF(NETDEV_CHANGE): calid3c598300ec: link becomes ready
Mar 13 20:38:06 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 100 seen, reloading interface list
Mar 13 20:38:06 linbit1 systemd-networkd[2297221]: calid3c598300ec: Link UP
Mar 13 20:38:06 linbit1 systemd-networkd[2297221]: calid3c598300ec: Gained carrier
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.294 [INFO][2448201] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.328 [INFO][2448201] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0 fio-bench-r2-n1-1- default  f15c9220-13bf-47a1-93d8-bf5be8a9a7a5 17523540 0 2023-03-13 20:37:26 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:5c1ce93e-2e7b-4fd4-9b8b-a95ff20d597e job-name:fio-bench-r2-n1-1 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n1-1-vjkzb eth0 default [] []   [kns.default ksa.default.default] calid3c598300ec  [] []}} ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Namespace="default" Pod="fio-bench-r2-n1-1-vjkzb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.328 [INFO][2448201] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Namespace="default" Pod="fio-bench-r2-n1-1-vjkzb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.376 [INFO][2448227] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.403 [INFO][2448227] ipam_plugin.go 264: Auto assigning IP ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x400059d5d0), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n1-1-vjkzb", "timestamp":"2023-03-13 20:38:06.376659715 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.403 [INFO][2448227] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.411 [INFO][2448227] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.423 [INFO][2448227] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.435 [INFO][2448227] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.440 [INFO][2448227] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.449 [INFO][2448227] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.449 [INFO][2448227] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.455 [INFO][2448227] ipam.go 1680: Creating new handle: k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.465 [INFO][2448227] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.484 [INFO][2448227] ipam.go 1214: Successfully claimed IPs: [10.1.217.48/26] block=10.1.217.0/26 handle="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.484 [INFO][2448227] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.48/26] handle="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" host="linbit1"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.484 [INFO][2448227] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.48/26] IPv6=[] ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.489 [INFO][2448201] k8s.go 383: Populated endpoint ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Namespace="default" Pod="fio-bench-r2-n1-1-vjkzb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0", GenerateName:"fio-bench-r2-n1-1-", Namespace:"default", SelfLink:"", UID:"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5", ResourceVersion:"17523540", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"5c1ce93e-2e7b-4fd4-9b8b-a95ff20d597e", "job-name":"fio-bench-r2-n1-1", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n1-1-vjkzb", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.48/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"calid3c598300ec", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.489 [INFO][2448201] k8s.go 384: Calico CNI using IPs: [10.1.217.48/32] ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Namespace="default" Pod="fio-bench-r2-n1-1-vjkzb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.490 [INFO][2448201] dataplane_linux.go 68: Setting the host side veth name to calid3c598300ec ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Namespace="default" Pod="fio-bench-r2-n1-1-vjkzb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.492 [INFO][2448201] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Namespace="default" Pod="fio-bench-r2-n1-1-vjkzb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.534 [INFO][2448201] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Namespace="default" Pod="fio-bench-r2-n1-1-vjkzb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0", GenerateName:"fio-bench-r2-n1-1-", Namespace:"default", SelfLink:"", UID:"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5", ResourceVersion:"17523540", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"5c1ce93e-2e7b-4fd4-9b8b-a95ff20d597e", "job-name":"fio-bench-r2-n1-1", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2", Pod:"fio-bench-r2-n1-1-vjkzb", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.48/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"calid3c598300ec", MAC:"b6:50:c3:be:1e:3b", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:06.550 [INFO][2448201] k8s.go 489: Wrote updated endpoint to datastore ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Namespace="default" Pod="fio-bench-r2-n1-1-vjkzb" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.579963249Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.580084489Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.580134649Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.580464770Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2 pid=2448281 runtime=io.containerd.runc.v2
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.627544793Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.630185681Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.635208576Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.639880670Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.642396158Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.642876559Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.646063209Z" level=info msg="CreateContainer within sandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:06 linbit1 systemd-udevd[2446773]: dm-26: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-26' failed with exit code 1.
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.653828792Z" level=info msg="CreateContainer within sandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.654361114Z" level=info msg="StartContainer for \"8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.683429162Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n1-1-vjkzb,Uid:f15c9220-13bf-47a1-93d8-bf5be8a9a7a5,Namespace:default,Attempt:0,} returns sandbox id \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\""
Mar 13 20:38:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:06.716066700Z" level=info msg="StartContainer for \"8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1\" returns successfully"
Mar 13 20:38:06 linbit1 systemd-udevd[2446773]: dm-27: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-27' failed with exit code 1.
Mar 13 20:38:06 linbit1 kernel: [504677.934207] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Starting worker thread (from drbdsetup [2448401])
Mar 13 20:38:06 linbit1 kernel: [504677.938368] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Starting sender thread (from drbdsetup [2448406])
Mar 13 20:38:06 linbit1 kernel: [504677.940104] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Starting sender thread (from drbdsetup [2448409])
Mar 13 20:38:06 linbit1 systemd-udevd[2446773]: drbd1013: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1013' failed with exit code 1.
Mar 13 20:38:06 linbit1 kernel: [504677.989359] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: meta-data IO uses: blk-bio
Mar 13 20:38:06 linbit1 kernel: [504677.989442] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: rs_discard_granularity feature disabled
Mar 13 20:38:06 linbit1 kernel: [504677.989484] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: disk( Diskless -> Attaching )
Mar 13 20:38:06 linbit1 kernel: [504677.989495] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: Maximum number of peer devices = 7
Mar 13 20:38:06 linbit1 kernel: [504677.990213] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Method to ensure write ordering: drain
Mar 13 20:38:06 linbit1 kernel: [504677.990221] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:06 linbit1 kernel: [504677.996048] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:06 linbit1 kernel: [504677.996054] drbd1013: detected capacity change from 0 to 20971520
Mar 13 20:38:06 linbit1 kernel: [504677.996058] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: size = 10 GB (10485760 KB)
Mar 13 20:38:06 linbit1 kernel: [504678.001056] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: recounting of set bits took additional 4ms
Mar 13 20:38:06 linbit1 kernel: [504678.001069] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: disk( Attaching -> Inconsistent )
Mar 13 20:38:06 linbit1 kernel: [504678.001071] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: attached to current UUID: 0000000000000004
Mar 13 20:38:06 linbit1 kernel: [504678.002459] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:06 linbit1 kernel: [504678.002745] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Starting receiver thread (from drbd_w_pvc-4ab0 [2448402])
Mar 13 20:38:06 linbit1 kernel: [504678.002874] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:06 linbit1 kernel: [504678.003421] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:06 linbit1 kernel: [504678.003490] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Starting receiver thread (from drbd_w_pvc-4ab0 [2448402])
Mar 13 20:38:06 linbit1 kernel: [504678.003586] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:07 linbit1 kernel: [504678.021230] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: rs_discard_granularity feature disabled
Mar 13 20:38:07 linbit1 kernel: [504678.081112] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: rs_discard_granularity feature disabled
Mar 13 20:38:07 linbit1 systemd-networkd[2297221]: cali9bae66a6c22: Gained IPv6LL
Mar 13 20:38:07 linbit1 kernel: [504678.130937] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 888789424 (0->-1 7683/4609)
Mar 13 20:38:07 linbit1 kernel: [504678.130948] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 888789424 (0ms)
Mar 13 20:38:07 linbit1 kernel: [504678.130955] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: role( Secondary -> Primary )
Mar 13 20:38:07 linbit1 kernel: [504678.130959] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: disk( Inconsistent -> UpToDate )
Mar 13 20:38:07 linbit1 kernel: [504678.131054] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: size = 10 GB (10485760 KB)
Mar 13 20:38:07 linbit1 kernel: [504678.131204] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Forced to consider local data as UpToDate!
Mar 13 20:38:07 linbit1 kernel: [504678.131295] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: new current UUID: 0D7C606C36409063 weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:07 linbit1 kernel: [504678.137448] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 3027515209 (0->-1 3/2)
Mar 13 20:38:07 linbit1 kernel: [504678.137456] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 3027515209 (0ms)
Mar 13 20:38:07 linbit1 kernel: [504678.137463] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: role( Primary -> Secondary )
Mar 13 20:38:07 linbit1 kernel: [504678.144959] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 880385518 (0->-1 3/1)
Mar 13 20:38:07 linbit1 kernel: [504678.144967] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 880385518 (0ms)
Mar 13 20:38:07 linbit1 kernel: [504678.144974] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: role( Secondary -> Primary )
Mar 13 20:38:07 linbit1 kernel: [504678.152517] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 2576561993 (0->-1 3/2)
Mar 13 20:38:07 linbit1 kernel: [504678.152526] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 2576561993 (0ms)
Mar 13 20:38:07 linbit1 kernel: [504678.152533] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: role( Primary -> Secondary )
Mar 13 20:38:07 linbit1 kernel: [504678.165147] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc: Starting worker thread (from drbdsetup [2448474])
Mar 13 20:38:07 linbit1 kernel: [504678.169675] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Starting sender thread (from drbdsetup [2448479])
Mar 13 20:38:07 linbit1 kernel: [504678.171503] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Starting sender thread (from drbdsetup [2448482])
Mar 13 20:38:07 linbit1 systemd-udevd[2446773]: drbd1010: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1010' failed with exit code 1.
Mar 13 20:38:07 linbit1 kernel: [504678.175832] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:07 linbit1 kernel: [504678.175893] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Starting receiver thread (from drbd_w_pvc-b5e1 [2448475])
Mar 13 20:38:07 linbit1 kernel: [504678.176085] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:07 linbit1 kernel: [504678.177257] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:07 linbit1 kernel: [504678.177297] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Starting receiver thread (from drbd_w_pvc-b5e1 [2448475])
Mar 13 20:38:07 linbit1 kernel: [504678.177430] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:07 linbit1 kernel: [504678.194892] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: rs_discard_granularity feature disabled
Mar 13 20:38:07 linbit1 kernel: [504678.257486] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 1335735478 (0->-1 7683/4609)
Mar 13 20:38:07 linbit1 kernel: [504678.257496] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 1335735478 (0ms)
Mar 13 20:38:07 linbit1 kernel: [504678.257503] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: role( Secondary -> Primary )
Mar 13 20:38:07 linbit1 kernel: [504678.257507] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: disk( Inconsistent -> UpToDate )
Mar 13 20:38:07 linbit1 kernel: [504678.257576] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: size = 10 GB (10485760 KB)
Mar 13 20:38:07 linbit1 kernel: [504678.257686] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Forced to consider local data as UpToDate!
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.241846129Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:07 linbit1 kernel: [504678.257770] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: new current UUID: E6C9CF72D8357D5D weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.244589497Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:07 linbit1 kernel: [504678.264893] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 1152135402 (0->-1 3/2)
Mar 13 20:38:07 linbit1 kernel: [504678.264902] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 1152135402 (0ms)
Mar 13 20:38:07 linbit1 kernel: [504678.264909] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: role( Primary -> Secondary )
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.250354635Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.254598328Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:07 linbit1 kernel: [504678.271601] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 1924047269 (0->-1 3/1)
Mar 13 20:38:07 linbit1 kernel: [504678.271607] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 1924047269 (0ms)
Mar 13 20:38:07 linbit1 kernel: [504678.271611] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: role( Secondary -> Primary )
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.257068055Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.257941698Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.261102747Z" level=info msg="CreateContainer within sandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:07 linbit1 kernel: [504678.277607] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 331688117 (0->-1 3/2)
Mar 13 20:38:07 linbit1 kernel: [504678.277615] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 331688117 (0ms)
Mar 13 20:38:07 linbit1 kernel: [504678.277622] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: role( Primary -> Secondary )
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.266410763Z" level=info msg="CreateContainer within sandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928\""
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.266888925Z" level=info msg="StartContainer for \"716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928\""
Mar 13 20:38:07 linbit1 kernel: [504678.295671] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Starting sender thread (from drbdsetup [2448538])
Mar 13 20:38:07 linbit1 kernel: [504678.297079] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Starting sender thread (from drbdsetup [2448542])
Mar 13 20:38:07 linbit1 kernel: [504678.333734] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: rs_discard_granularity feature disabled
Mar 13 20:38:07 linbit1 kernel: [504678.365726] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:07 linbit1 kernel: [504678.365786] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Starting receiver thread (from drbd_w_pvc-f35d [2446599])
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.351328980Z" level=info msg="StartContainer for \"716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928\" returns successfully"
Mar 13 20:38:07 linbit1 kernel: [504678.365944] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:07 linbit1 kernel: [504678.367095] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:07 linbit1 kernel: [504678.367102] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: quorum( yes -> no )
Mar 13 20:38:07 linbit1 kernel: [504678.367269] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Starting receiver thread (from drbd_w_pvc-f35d [2446599])
Mar 13 20:38:07 linbit1 kernel: [504678.367374] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:07 linbit1 systemd-udevd[2446773]: dm-28: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-28' failed with exit code 1.
Mar 13 20:38:07 linbit1 systemd-udevd[2446773]: dm-29: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-29' failed with exit code 1.
Mar 13 20:38:07 linbit1 kernel: [504678.532112] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:07 linbit1 kernel: [504678.532121] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:07 linbit1 kernel: [504678.532122] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:07 linbit1 kernel: [504678.532132] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:07 linbit1 kernel: [504678.532227] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:07 linbit1 kernel: [504678.532303] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:07 linbit1 kernel: [504678.646761] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Starting worker thread (from drbdsetup [2448628])
Mar 13 20:38:07 linbit1 kernel: [504678.650902] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Starting sender thread (from drbdsetup [2448633])
Mar 13 20:38:07 linbit1 kernel: [504678.652713] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Starting sender thread (from drbdsetup [2448636])
Mar 13 20:38:07 linbit1 systemd-udevd[2446773]: drbd1014: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1014' failed with exit code 1.
Mar 13 20:38:07 linbit1 kernel: [504678.668062] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Preparing cluster-wide state change 96052770 (1->2 499/146)
Mar 13 20:38:07 linbit1 kernel: [504678.696126] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:07 linbit1 kernel: [504678.696136] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:07 linbit1 kernel: [504678.696304] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:07 linbit1 kernel: [504678.706018] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:07 linbit1 kernel: [504678.706028] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:07 linbit1 kernel: [504678.706136] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:07 linbit1 kernel: [504678.712032] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit3: self 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:07 linbit1 kernel: [504678.712044] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:38:07 linbit1 kernel: [504678.712058] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: State change 96052770: primary_nodes=0, weak_nodes=0
Mar 13 20:38:07 linbit1 kernel: [504678.712064] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Committing cluster-wide state change 96052770 (44ms)
Mar 13 20:38:07 linbit1 kernel: [504678.712099] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:07 linbit1 kernel: [504678.712105] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:07 linbit1 kernel: [504678.712567] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Preparing remote state change 2683894703
Mar 13 20:38:07 linbit1 kernel: [504678.721377] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: meta-data IO uses: blk-bio
Mar 13 20:38:07 linbit1 kernel: [504678.721475] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: rs_discard_granularity feature disabled
Mar 13 20:38:07 linbit1 kernel: [504678.721542] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: disk( Diskless -> Attaching )
Mar 13 20:38:07 linbit1 kernel: [504678.721555] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: Maximum number of peer devices = 7
Mar 13 20:38:07 linbit1 kernel: [504678.722488] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Method to ensure write ordering: drain
Mar 13 20:38:07 linbit1 kernel: [504678.722497] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:07 linbit1 kernel: [504678.730083] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:07 linbit1 kernel: [504678.730091] drbd1014: detected capacity change from 0 to 20971520
Mar 13 20:38:07 linbit1 kernel: [504678.730095] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: size = 10 GB (10485760 KB)
Mar 13 20:38:07 linbit1 kernel: [504678.739640] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: recounting of set bits took additional 4ms
Mar 13 20:38:07 linbit1 kernel: [504678.739660] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: disk( Attaching -> Inconsistent )
Mar 13 20:38:07 linbit1 kernel: [504678.739665] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: attached to current UUID: 0000000000000004
Mar 13 20:38:07 linbit1 kernel: [504678.741240] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:07 linbit1 kernel: [504678.741397] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Starting receiver thread (from drbd_w_pvc-f893 [2448629])
Mar 13 20:38:07 linbit1 kernel: [504678.741500] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:07 linbit1 kernel: [504678.742325] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:07 linbit1 kernel: [504678.742390] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Starting receiver thread (from drbd_w_pvc-f893 [2448629])
Mar 13 20:38:07 linbit1 kernel: [504678.742523] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:07 linbit1 kernel: [504678.748021] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Preparing remote state change 2029845538
Mar 13 20:38:07 linbit1 kernel: [504678.748070] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: drbd_sync_handshake:
Mar 13 20:38:07 linbit1 kernel: [504678.748075] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: self 0000000000000004:0000000000000000:9878D4B3AC0CDFE6:0000000000000000 bits:0 flags:24
Mar 13 20:38:07 linbit1 kernel: [504678.748080] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: peer B08AE663A5D508EC:9878D4B3AC0CDFE7:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:07 linbit1 kernel: [504678.748086] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: uuid_compare()=target-set-bitmap by rule=just-created-self
Mar 13 20:38:07 linbit1 kernel: [504678.748090] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: Setting and writing the whole bitmap, fresh node
Mar 13 20:38:07 linbit1 kernel: [504678.750701] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Committing remote state change 2683894703 (primary_nodes=0)
Mar 13 20:38:07 linbit1 kernel: [504678.750717] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:07 linbit1 kernel: [504678.750721] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: quorum( no -> yes )
Mar 13 20:38:07 linbit1 kernel: [504678.750724] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:38:07 linbit1 kernel: [504678.750953] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: size = 10 GB (10485760 KB)
Mar 13 20:38:07 linbit1 kernel: [504678.753031] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:07 linbit1 kernel: [504678.754072] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:07 linbit1 kernel: [504678.754081] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: setting UUIDs to 9878D4B3AC0CDFE6:0000000000000000:9878D4B3AC0CDFE6:0000000000000000
Mar 13 20:38:07 linbit1 kernel: [504678.754098] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit3: resync-susp( no -> connection dependency )
Mar 13 20:38:07 linbit1 kernel: [504678.754102] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: repl( WFBitMapT -> SyncTarget )
Mar 13 20:38:07 linbit1 kernel: [504678.754377] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: Began resync as SyncTarget (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:07 linbit1 systemd-networkd[2297221]: cali7bcd57bedf4: Gained IPv6LL
Mar 13 20:38:07 linbit1 kernel: [504678.792029] drbd1010: detected capacity change from 0 to 20971520
Mar 13 20:38:07 linbit1 kernel: [504678.792039] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010: size = 10 GB (10485760 KB)
Mar 13 20:38:07 linbit1 kernel: [504678.792171] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:38:07 linbit1 kernel: [504678.792178] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: peer 8CECECC5242E4A6E:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:07 linbit1 kernel: [504678.793979] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Committing remote state change 2029845538 (primary_nodes=0)
Mar 13 20:38:07 linbit1 kernel: [504678.793998] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:07 linbit1 kernel: [504678.794003] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010: quorum( no -> yes )
Mar 13 20:38:07 linbit1 kernel: [504678.794008] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:38:07 linbit1 kernel: [504678.826900] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Preparing remote state change 611550991
Mar 13 20:38:07 linbit1 kernel: [504678.852111] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:38:07 linbit1 kernel: [504678.852118] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit3: peer 8CECECC5242E4A6E:FFFFFFFFFFFFFFFF:2E5F4E1213C623B8:0000000000000000 bits:0 flags:1824
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.842259143Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.845869194Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.850025927Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:07 linbit1 kernel: [504678.866921] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Committing remote state change 611550991 (primary_nodes=0)
Mar 13 20:38:07 linbit1 kernel: [504678.866937] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:07 linbit1 kernel: [504678.866943] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established ) resync-susp( no -> peer )
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.854689981Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.856671747Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.860202597Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.864446450Z" level=info msg="CreateContainer within sandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:07 linbit1 kernel: [504678.884142] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:07 linbit1 kernel: [504678.884152] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:07 linbit1 kernel: [504678.884275] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.872087793Z" level=info msg="CreateContainer within sandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c\""
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.874092359Z" level=info msg="StartContainer for \"1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c\""
Mar 13 20:38:07 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-containerd-tmpmounts-containerd\x2dmount3099535160.mount: Deactivated successfully.
Mar 13 20:38:07 linbit1 kernel: [504678.897972] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:07 linbit1 kernel: [504678.897982] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:07 linbit1 kernel: [504678.898105] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:07 linbit1 kernel: [504678.944034] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 385003809 (0->2 499/146)
Mar 13 20:38:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:07.972729378Z" level=info msg="StartContainer for \"1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c\" returns successfully"
Mar 13 20:38:07 linbit1 kernel: [504678.996012] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit2: self 4AA31DF55B1E1998:E42C3C74CF0CF00E:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:07 linbit1 kernel: [504678.996022] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:07 linbit1 kernel: [504678.996037] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: State change 385003809: primary_nodes=0, weak_nodes=0
Mar 13 20:38:07 linbit1 kernel: [504678.996043] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 385003809 (52ms)
Mar 13 20:38:07 linbit1 kernel: [504678.996069] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:07 linbit1 kernel: [504678.996074] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:08 linbit1 kernel: [504679.080045] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 1893932606 (0->1 499/146)
Mar 13 20:38:08 linbit1 kernel: [504679.120071] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: drbd_sync_handshake:
Mar 13 20:38:08 linbit1 kernel: [504679.120076] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: self 4AA31DF55B1E1998:E42C3C74CF0CF00E:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:38:08 linbit1 kernel: [504679.120082] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:08 linbit1 kernel: [504679.120087] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:38:08 linbit1 kernel: [504679.120091] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:38:08 linbit1 kernel: [504679.122744] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: State change 1893932606: primary_nodes=0, weak_nodes=0
Mar 13 20:38:08 linbit1 kernel: [504679.122754] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 1893932606 (40ms)
Mar 13 20:38:08 linbit1 kernel: [504679.122806] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:08 linbit1 kernel: [504679.122810] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: quorum( no -> yes )
Mar 13 20:38:08 linbit1 kernel: [504679.122814] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:38:08 linbit1 kernel: [504679.124034] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:08 linbit1 kernel: [504679.126520] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:08 linbit1 kernel: [504679.126540] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: repl( WFBitMapS -> SyncSource )
Mar 13 20:38:08 linbit1 kernel: [504679.126805] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:08 linbit1 kernel: [504679.268107] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:08 linbit1 kernel: [504679.268117] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:08 linbit1 kernel: [504679.268133] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:08 linbit1 kernel: [504679.268142] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:08 linbit1 kernel: [504679.268382] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:08 linbit1 kernel: [504679.268426] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:08 linbit1 kernel: [504679.308069] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Preparing remote state change 101558060
Mar 13 20:38:08 linbit1 systemd-networkd[2297221]: calid3c598300ec: Gained IPv6LL
Mar 13 20:38:08 linbit1 kernel: [504679.340159] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: drbd_sync_handshake:
Mar 13 20:38:08 linbit1 kernel: [504679.340168] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: self 0000000000000004:0000000000000000:CA82BDB037AD25C0:0000000000000000 bits:0 flags:24
Mar 13 20:38:08 linbit1 kernel: [504679.340176] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: peer B7D22546AC13E266:CA82BDB037AD25C0:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:08 linbit1 kernel: [504679.340185] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: uuid_compare()=target-set-bitmap by rule=just-created-self
Mar 13 20:38:08 linbit1 kernel: [504679.340190] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: Setting and writing the whole bitmap, fresh node
Mar 13 20:38:08 linbit1 kernel: [504679.354206] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Committing remote state change 101558060 (primary_nodes=0)
Mar 13 20:38:08 linbit1 kernel: [504679.354230] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:08 linbit1 kernel: [504679.354235] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: quorum( no -> yes )
Mar 13 20:38:08 linbit1 kernel: [504679.354241] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:38:08 linbit1 kernel: [504679.354520] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: size = 10 GB (10485760 KB)
Mar 13 20:38:08 linbit1 kernel: [504679.356479] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:08 linbit1 kernel: [504679.357341] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:08 linbit1 kernel: [504679.357348] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: setting UUIDs to CA82BDB037AD25C0:0000000000000000:CA82BDB037AD25C0:0000000000000000
Mar 13 20:38:08 linbit1 kernel: [504679.357364] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: repl( WFBitMapT -> SyncTarget )
Mar 13 20:38:08 linbit1 kernel: [504679.357367] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit2: resync-susp( no -> connection dependency )
Mar 13 20:38:08 linbit1 kernel: [504679.357545] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: Began resync as SyncTarget (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:08 linbit1 kernel: [504679.416042] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Preparing cluster-wide state change 67295108 (1->2 499/146)
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.435948297Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.438656865Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:08 linbit1 kernel: [504679.456022] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit2: self B7D22546AC13E266:CA82BDB037AD25C0:CA82BDB037AD25C0:0000000000000000 bits:0 flags:0
Mar 13 20:38:08 linbit1 kernel: [504679.456033] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:08 linbit1 kernel: [504679.456051] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: State change 67295108: primary_nodes=0, weak_nodes=0
Mar 13 20:38:08 linbit1 kernel: [504679.456058] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Committing cluster-wide state change 67295108 (40ms)
Mar 13 20:38:08 linbit1 kernel: [504679.456103] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:08 linbit1 kernel: [504679.456109] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.444021722Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.448327175Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.450341221Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.450894622Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.453429230Z" level=info msg="CreateContainer within sandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.461037413Z" level=info msg="CreateContainer within sandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe\""
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.461444334Z" level=info msg="StartContainer for \"e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe\""
Mar 13 20:38:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:08.571078546Z" level=info msg="StartContainer for \"e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe\" returns successfully"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.074529787Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.077449276Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.082747572Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.086847264Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.088790350Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.089493152Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.092206520Z" level=info msg="CreateContainer within sandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.099011941Z" level=info msg="CreateContainer within sandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0\""
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.099498062Z" level=info msg="StartContainer for \"914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0\""
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.188763172Z" level=info msg="StartContainer for \"914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0\" returns successfully"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.676639606Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.679424855Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.685318513Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.690140887Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.692277774Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.695023302Z" level=info msg="CreateContainer within sandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.702918206Z" level=info msg="CreateContainer within sandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc\""
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.703383247Z" level=info msg="StartContainer for \"9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc\""
Mar 13 20:38:09 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-containerd-tmpmounts-containerd\x2dmount2827351162.mount: Deactivated successfully.
Mar 13 20:38:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:09.812604497Z" level=info msg="StartContainer for \"9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc\" returns successfully"
Mar 13 20:38:10 linbit1 kernel: [504681.915108] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Starting sender thread (from drbdsetup [2448900])
Mar 13 20:38:10 linbit1 kernel: [504681.916431] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Starting sender thread (from drbdsetup [2448902])
Mar 13 20:38:10 linbit1 kernel: [504681.957748] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: rs_discard_granularity feature disabled
Mar 13 20:38:10 linbit1 kernel: [504682.001464] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:10 linbit1 kernel: [504682.001470] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: quorum( yes -> no )
Mar 13 20:38:10 linbit1 kernel: [504682.001616] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Starting receiver thread (from drbd_w_pvc-4007 [2446824])
Mar 13 20:38:10 linbit1 kernel: [504682.001772] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:10 linbit1 kernel: [504682.002606] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:10 linbit1 kernel: [504682.002651] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Starting receiver thread (from drbd_w_pvc-4007 [2446824])
Mar 13 20:38:10 linbit1 kernel: [504682.002800] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:11 linbit1 kernel: [504682.016936] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578: Starting worker thread (from drbdsetup [2448920])
Mar 13 20:38:11 linbit1 kernel: [504682.021209] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Starting sender thread (from drbdsetup [2448925])
Mar 13 20:38:11 linbit1 kernel: [504682.022872] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Starting sender thread (from drbdsetup [2448930])
Mar 13 20:38:11 linbit1 kernel: [504682.026739] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:11 linbit1 kernel: [504682.026809] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Starting receiver thread (from drbd_w_pvc-b502 [2448921])
Mar 13 20:38:11 linbit1 kernel: [504682.026934] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:11 linbit1 kernel: [504682.028204] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:11 linbit1 kernel: [504682.028395] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Starting receiver thread (from drbd_w_pvc-b502 [2448921])
Mar 13 20:38:11 linbit1 systemd-udevd[2448927]: drbd1015: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1015' failed with exit code 1.
Mar 13 20:38:11 linbit1 kernel: [504682.028499] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:11 linbit1 kernel: [504682.532114] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:11 linbit1 kernel: [504682.532124] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:11 linbit1 kernel: [504682.532139] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:11 linbit1 kernel: [504682.532147] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:11 linbit1 kernel: [504682.532244] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:11 linbit1 kernel: [504682.532346] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:11 linbit1 kernel: [504682.550003] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:11 linbit1 kernel: [504682.550013] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:11 linbit1 kernel: [504682.550156] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:11 linbit1 kernel: [504682.554892] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:11 linbit1 kernel: [504682.554900] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:11 linbit1 kernel: [504682.555324] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:11 linbit1 kernel: [504682.608037] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 1543866673 (0->2 499/146)
Mar 13 20:38:11 linbit1 kernel: [504682.632015] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit3: self 9ED24C56D5CABD9E:5315BCF62A818892:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:11 linbit1 kernel: [504682.632027] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:38:11 linbit1 kernel: [504682.643064] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: State change 1543866673: primary_nodes=0, weak_nodes=0
Mar 13 20:38:11 linbit1 kernel: [504682.643073] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 1543866673 (32ms)
Mar 13 20:38:11 linbit1 kernel: [504682.643107] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:11 linbit1 kernel: [504682.643114] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:11 linbit1 kernel: [504682.686841] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Preparing remote state change 3744782767
Mar 13 20:38:11 linbit1 kernel: [504682.720010] drbd1015: detected capacity change from 0 to 20971520
Mar 13 20:38:11 linbit1 kernel: [504682.720018] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015: size = 10 GB (10485760 KB)
Mar 13 20:38:11 linbit1 kernel: [504682.720189] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:38:11 linbit1 kernel: [504682.720196] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit3: peer 8D39EF2D02272B3E:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:11 linbit1 kernel: [504682.724039] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 931456748 (0->1 499/146)
Mar 13 20:38:11 linbit1 kernel: [504682.730906] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Committing remote state change 3744782767 (primary_nodes=0)
Mar 13 20:38:11 linbit1 kernel: [504682.730925] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:11 linbit1 kernel: [504682.730930] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015: quorum( no -> yes )
Mar 13 20:38:11 linbit1 kernel: [504682.730935] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:38:11 linbit1 kernel: [504682.731202] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Preparing remote state change 2186200747
Mar 13 20:38:11 linbit1 kernel: [504682.752048] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: drbd_sync_handshake:
Mar 13 20:38:11 linbit1 kernel: [504682.752055] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: self 9ED24C56D5CABD9E:5315BCF62A818892:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:38:11 linbit1 kernel: [504682.752063] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:11 linbit1 kernel: [504682.752070] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:38:11 linbit1 kernel: [504682.752076] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:38:11 linbit1 kernel: [504682.768949] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: State change 931456748: primary_nodes=0, weak_nodes=0
Mar 13 20:38:11 linbit1 kernel: [504682.768960] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 931456748 (44ms)
Mar 13 20:38:11 linbit1 kernel: [504682.769021] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:11 linbit1 kernel: [504682.769026] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: quorum( no -> yes )
Mar 13 20:38:11 linbit1 kernel: [504682.769032] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:38:11 linbit1 kernel: [504682.770503] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:11 linbit1 kernel: [504682.772120] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:38:11 linbit1 kernel: [504682.772128] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit2: peer 8D39EF2D02272B3E:FFFFFFFFFFFFFFFF:C7BFAD55A6723A14:0000000000000000 bits:0 flags:1824
Mar 13 20:38:11 linbit1 kernel: [504682.772344] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Committing remote state change 2186200747 (primary_nodes=0)
Mar 13 20:38:11 linbit1 kernel: [504682.772358] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:11 linbit1 kernel: [504682.772365] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established ) resync-susp( no -> peer )
Mar 13 20:38:11 linbit1 kernel: [504682.773696] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:11 linbit1 kernel: [504682.773713] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:38:11 linbit1 kernel: [504682.773900] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:12 linbit1 kernel: [504683.189011] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: rs_discard_granularity feature disabled
Mar 13 20:38:12 linbit1 kernel: [504683.272835] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: rs_discard_granularity feature disabled
Mar 13 20:38:12 linbit1 kernel: [504683.324110] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Starting sender thread (from drbdsetup [2449053])
Mar 13 20:38:12 linbit1 kernel: [504683.325774] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Starting sender thread (from drbdsetup [2449055])
Mar 13 20:38:12 linbit1 kernel: [504683.365719] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: rs_discard_granularity feature disabled
Mar 13 20:38:12 linbit1 kernel: [504683.393565] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:12 linbit1 kernel: [504683.393631] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Starting receiver thread (from drbd_w_pvc-b01e [2447053])
Mar 13 20:38:12 linbit1 kernel: [504683.393818] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:12 linbit1 kernel: [504683.394801] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:12 linbit1 kernel: [504683.394807] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: quorum( yes -> no )
Mar 13 20:38:12 linbit1 kernel: [504683.394942] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Starting receiver thread (from drbd_w_pvc-b01e [2447053])
Mar 13 20:38:12 linbit1 kernel: [504683.395058] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:12 linbit1 kernel: [504683.428656] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Starting sender thread (from drbdsetup [2449083])
Mar 13 20:38:12 linbit1 kernel: [504683.430382] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Starting sender thread (from drbdsetup [2449085])
Mar 13 20:38:12 linbit1 kernel: [504683.457777] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: rs_discard_granularity feature disabled
Mar 13 20:38:12 linbit1 kernel: [504683.497479] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:12 linbit1 kernel: [504683.497537] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Starting receiver thread (from drbd_w_pvc-b7be [2447224])
Mar 13 20:38:12 linbit1 kernel: [504683.497685] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:12 linbit1 kernel: [504683.498588] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:12 linbit1 kernel: [504683.498594] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: quorum( yes -> no )
Mar 13 20:38:12 linbit1 kernel: [504683.498769] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Starting receiver thread (from drbd_w_pvc-b7be [2447224])
Mar 13 20:38:12 linbit1 kernel: [504683.498887] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:12 linbit1 kernel: [504683.512687] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148: Starting worker thread (from drbdsetup [2449102])
Mar 13 20:38:12 linbit1 kernel: [504683.516730] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Starting sender thread (from drbdsetup [2449107])
Mar 13 20:38:12 linbit1 kernel: [504683.518451] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Starting sender thread (from drbdsetup [2449110])
Mar 13 20:38:12 linbit1 kernel: [504683.522231] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:12 linbit1 kernel: [504683.522290] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Starting receiver thread (from drbd_w_pvc-e2bb [2449103])
Mar 13 20:38:12 linbit1 kernel: [504683.522402] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:12 linbit1 kernel: [504683.523652] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:12 linbit1 kernel: [504683.523687] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Starting receiver thread (from drbd_w_pvc-e2bb [2449103])
Mar 13 20:38:12 linbit1 kernel: [504683.523787] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:12 linbit1 systemd-udevd[2448926]: drbd1016: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1016' failed with exit code 1.
Mar 13 20:38:12 linbit1 kernel: [504683.544556] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: rs_discard_granularity feature disabled
Mar 13 20:38:12 linbit1 kernel: [504683.597271] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: rs_discard_granularity feature disabled
Mar 13 20:38:12 linbit1 kernel: [504683.922066] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:12 linbit1 kernel: [504683.922075] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:12 linbit1 kernel: [504683.922193] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:12 linbit1 kernel: [504683.940085] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:12 linbit1 kernel: [504683.940096] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:12 linbit1 kernel: [504683.940243] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:12 linbit1 kernel: [504683.944006] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 447020452 (0->1 499/146)
Mar 13 20:38:13 linbit1 kernel: [504684.022050] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:13 linbit1 kernel: [504684.022059] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:13 linbit1 kernel: [504684.022167] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:13 linbit1 kernel: [504684.024044] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: drbd_sync_handshake:
Mar 13 20:38:13 linbit1 kernel: [504684.024052] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: self 0D7C606C36409062:405E968F96DD2021:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:38:13 linbit1 kernel: [504684.024061] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:13 linbit1 kernel: [504684.024069] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:38:13 linbit1 kernel: [504684.024074] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:38:13 linbit1 kernel: [504684.026929] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change 447020452: primary_nodes=0, weak_nodes=0
Mar 13 20:38:13 linbit1 kernel: [504684.026938] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 447020452 (80ms)
Mar 13 20:38:13 linbit1 kernel: [504684.026973] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:13 linbit1 kernel: [504684.026977] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: quorum( no -> yes )
Mar 13 20:38:13 linbit1 kernel: [504684.026981] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:38:13 linbit1 kernel: [504684.027832] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:13 linbit1 kernel: [504684.027852] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Preparing remote state change 3953142659
Mar 13 20:38:13 linbit1 kernel: [504684.029797] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:13 linbit1 kernel: [504684.036067] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:13 linbit1 kernel: [504684.036077] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:13 linbit1 kernel: [504684.036253] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:13 linbit1 kernel: [504684.049993] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:13 linbit1 kernel: [504684.050002] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:13 linbit1 kernel: [504684.050105] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:13 linbit1 kernel: [504684.060041] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 2572083454 (0->2 499/146)
Mar 13 20:38:13 linbit1 kernel: [504684.091000] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Committing remote state change 3953142659 (primary_nodes=0)
Mar 13 20:38:13 linbit1 kernel: [504684.091021] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 2464127109 (0->2 499/146)
Mar 13 20:38:13 linbit1 kernel: [504684.095999] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit2: self E6C9CF72D8357D5C:C248611804888E0B:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:13 linbit1 kernel: [504684.096010] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:13 linbit1 kernel: [504684.110122] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: State change 2572083454: primary_nodes=0, weak_nodes=0
Mar 13 20:38:13 linbit1 kernel: [504684.110131] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 2572083454 (48ms)
Mar 13 20:38:13 linbit1 kernel: [504684.110173] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:13 linbit1 kernel: [504684.110179] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:13 linbit1 kernel: [504684.128015] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit2: self 0D7C606C36409062:405E968F96DD2021:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:13 linbit1 kernel: [504684.128025] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:13 linbit1 kernel: [504684.130064] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change 2464127109: primary_nodes=0, weak_nodes=0
Mar 13 20:38:13 linbit1 kernel: [504684.130071] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 2464127109 (40ms)
Mar 13 20:38:13 linbit1 kernel: [504684.130114] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:13 linbit1 kernel: [504684.130120] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:13 linbit1 kernel: [504684.141969] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Preparing remote state change 2214418561
Mar 13 20:38:13 linbit1 kernel: [504684.164014] drbd1016: detected capacity change from 0 to 20971520
Mar 13 20:38:13 linbit1 kernel: [504684.164022] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016: size = 10 GB (10485760 KB)
Mar 13 20:38:13 linbit1 kernel: [504684.164067] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:38:13 linbit1 kernel: [504684.164073] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: peer 16216ECF09A0CD20:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:13 linbit1 kernel: [504684.181998] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Committing remote state change 2214418561 (primary_nodes=0)
Mar 13 20:38:13 linbit1 kernel: [504684.182016] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:13 linbit1 kernel: [504684.182020] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016: quorum( no -> yes )
Mar 13 20:38:13 linbit1 kernel: [504684.182026] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:38:13 linbit1 kernel: [504684.185106] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Preparing remote state change 548701303
Mar 13 20:38:13 linbit1 kernel: [504684.220027] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:38:13 linbit1 kernel: [504684.220033] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit3: peer 16216ECF09A0CD20:FFFFFFFFFFFFFFFF:7F007D7488DE8B1A:0000000000000000 bits:0 flags:1824
Mar 13 20:38:13 linbit1 kernel: [504684.220191] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Committing remote state change 548701303 (primary_nodes=0)
Mar 13 20:38:13 linbit1 kernel: [504684.220206] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:13 linbit1 kernel: [504684.220212] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established ) resync-susp( no -> peer )
Mar 13 20:38:13 linbit1 kernel: [504684.232001] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: repl( WFBitMapS -> SyncSource )
Mar 13 20:38:13 linbit1 kernel: [504684.232247] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:13 linbit1 kernel: [504684.411013] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:13 linbit1 kernel: [504684.411024] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:13 linbit1 kernel: [504684.411139] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:13 linbit1 kernel: [504684.495022] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Preparing remote state change 876532820
Mar 13 20:38:13 linbit1 kernel: [504684.542297] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Committing remote state change 876532820 (primary_nodes=0)
Mar 13 20:38:13 linbit1 kernel: [504684.548027] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 1714667566 (0->1 499/146)
Mar 13 20:38:13 linbit1 kernel: [504684.588054] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: drbd_sync_handshake:
Mar 13 20:38:13 linbit1 kernel: [504684.588061] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: self E6C9CF72D8357D5C:C248611804888E0B:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:38:13 linbit1 kernel: [504684.588069] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:13 linbit1 kernel: [504684.588076] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:38:13 linbit1 kernel: [504684.588082] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:38:13 linbit1 kernel: [504684.590726] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: State change 1714667566: primary_nodes=0, weak_nodes=0
Mar 13 20:38:13 linbit1 kernel: [504684.590735] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 1714667566 (40ms)
Mar 13 20:38:13 linbit1 kernel: [504684.590784] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:13 linbit1 kernel: [504684.590788] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: quorum( no -> yes )
Mar 13 20:38:13 linbit1 kernel: [504684.590791] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:38:13 linbit1 kernel: [504684.592023] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:13 linbit1 kernel: [504684.595357] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:13 linbit1 kernel: [504684.595375] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: repl( WFBitMapS -> SyncSource )
Mar 13 20:38:13 linbit1 kernel: [504684.595594] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:14 linbit1 kernel: [504685.814162] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: rs_discard_granularity feature disabled
Mar 13 20:38:16 linbit1 kernel: [504687.076120] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: rs_discard_granularity feature disabled
Mar 13 20:38:16 linbit1 kernel: [504687.124397] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: rs_discard_granularity feature disabled
Mar 13 20:38:16 linbit1 kernel: [504687.207634] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: rs_discard_granularity feature disabled
Mar 13 20:38:16 linbit1 kernel: [504687.275263] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: rs_discard_granularity feature disabled
Mar 13 20:38:16 linbit1 kernel: [504687.332456] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: rs_discard_granularity feature disabled
Mar 13 20:38:18 linbit1 kernel: [504689.340917] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: rs_discard_granularity feature disabled
Mar 13 20:38:19 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:19.301358    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:38:19 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:19.468186    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sgnhk\" (UniqueName: \"kubernetes.io/projected/fa760197-68d3-48e7-abee-bc22cb31e820-kube-api-access-sgnhk\") pod \"fio-bench-r2-n4-4-mmrrc\" (UID: \"fa760197-68d3-48e7-abee-bc22cb31e820\") " pod="default/fio-bench-r2-n4-4-mmrrc"
Mar 13 20:38:19 linbit1 kernel: [504690.723199] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: rs_discard_granularity feature disabled
Mar 13 20:38:19 linbit1 kernel: [504690.771518] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: rs_discard_granularity feature disabled
Mar 13 20:38:19 linbit1 kernel: [504690.834836] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: rs_discard_granularity feature disabled
Mar 13 20:38:21 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:21.423598    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:38:21 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:21.583839    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-49t98\" (UniqueName: \"kubernetes.io/projected/1a7e1b25-e333-4558-9a33-7f7b07d5d61c-kube-api-access-49t98\") pod \"fio-bench-r2-n7-2-bnfd5\" (UID: \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\") " pod="default/fio-bench-r2-n7-2-bnfd5"
Mar 13 20:38:22 linbit1 kernel: [504693.445067] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: rs_discard_granularity feature disabled
Mar 13 20:38:23 linbit1 kernel: [504694.330771] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Preparing remote state change 1561582406
Mar 13 20:38:23 linbit1 kernel: [504694.331184] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Committing remote state change 1561582406 (primary_nodes=1)
Mar 13 20:38:23 linbit1 kernel: [504694.331200] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: peer( Secondary -> Primary )
Mar 13 20:38:23 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:23.369108    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:38:23 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:23.540161    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:38:23 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:23.598674    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sp4lb\" (UniqueName: \"kubernetes.io/projected/d9db77e8-50b0-4260-89f3-ddb5a0de385a-kube-api-access-sp4lb\") pod \"fio-bench-r2-n7-3-mfqd4\" (UID: \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\") " pod="default/fio-bench-r2-n7-3-mfqd4"
Mar 13 20:38:23 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:23.699235    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6mgjs\" (UniqueName: \"kubernetes.io/projected/05dd1efc-8486-47dd-89df-7ee739a2d7e8-kube-api-access-6mgjs\") pod \"fio-bench-r2-n7-4-ckbx6\" (UID: \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\") " pod="default/fio-bench-r2-n7-4-ckbx6"
Mar 13 20:38:24 linbit1 systemd-udevd[2449863]: dm-30: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-30' failed with exit code 1.
Mar 13 20:38:24 linbit1 systemd-udevd[2449862]: dm-31: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-31' failed with exit code 1.
Mar 13 20:38:24 linbit1 kernel: [504695.347880] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Starting worker thread (from drbdsetup [2449919])
Mar 13 20:38:24 linbit1 kernel: [504695.355120] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: meta-data IO uses: blk-bio
Mar 13 20:38:24 linbit1 kernel: [504695.355185] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: rs_discard_granularity feature disabled
Mar 13 20:38:24 linbit1 kernel: [504695.355206] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: disk( Diskless -> Attaching )
Mar 13 20:38:24 linbit1 kernel: [504695.355214] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: Maximum number of peer devices = 7
Mar 13 20:38:24 linbit1 kernel: [504695.355945] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Method to ensure write ordering: drain
Mar 13 20:38:24 linbit1 kernel: [504695.355968] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:24 linbit1 kernel: [504695.361803] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:24 linbit1 kernel: [504695.361810] drbd1024: detected capacity change from 0 to 20971520
Mar 13 20:38:24 linbit1 kernel: [504695.361813] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: size = 10 GB (10485760 KB)
Mar 13 20:38:24 linbit1 kernel: [504695.367800] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: recounting of set bits took additional 0ms
Mar 13 20:38:24 linbit1 kernel: [504695.367811] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: disk( Attaching -> Inconsistent )
Mar 13 20:38:24 linbit1 kernel: [504695.367813] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: attached to current UUID: 0000000000000004
Mar 13 20:38:24 linbit1 systemd-udevd[2449862]: drbd1024: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1024' failed with exit code 1.
Mar 13 20:38:24 linbit1 systemd-udevd[2449862]: dm-32: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-32' failed with exit code 1.
Mar 13 20:38:24 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:24.504985    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-f35dc493-2853-439d-84a0-6d25a62830f3\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3\") pod \"fio-bench-r2-n4-4-mmrrc\" (UID: \"fa760197-68d3-48e7-abee-bc22cb31e820\") " pod="default/fio-bench-r2-n4-4-mmrrc"
Mar 13 20:38:24 linbit1 systemd-udevd[2449862]: dm-33: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-33' failed with exit code 1.
Mar 13 20:38:24 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:24.517346    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-f35dc493-2853-439d-84a0-6d25a62830f3\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3\") pod \"fio-bench-r2-n4-4-mmrrc\" (UID: \"fa760197-68d3-48e7-abee-bc22cb31e820\") device path: \"\"" pod="default/fio-bench-r2-n4-4-mmrrc"
Mar 13 20:38:24 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:24.606826    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-f35dc493-2853-439d-84a0-6d25a62830f3\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3\") pod \"fio-bench-r2-n4-4-mmrrc\" (UID: \"fa760197-68d3-48e7-abee-bc22cb31e820\") DevicePath \"\"" pod="default/fio-bench-r2-n4-4-mmrrc"
Mar 13 20:38:24 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:24.613822    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-f35dc493-2853-439d-84a0-6d25a62830f3\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3\") pod \"fio-bench-r2-n4-4-mmrrc\" (UID: \"fa760197-68d3-48e7-abee-bc22cb31e820\") DevicePath \"csi-5e9b60a2d0119fea505052c6c7b0619b59fe8e5a278f09e924024ccbcd72f00d\"" pod="default/fio-bench-r2-n4-4-mmrrc"
Mar 13 20:38:24 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:24.639855    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:24 linbit1 kernel: [504695.712026] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Starting worker thread (from drbdsetup [2450000])
Mar 13 20:38:24 linbit1 systemd-udevd[2449862]: drbd1022: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1022' failed with exit code 1.
Mar 13 20:38:24 linbit1 kernel: [504695.717955] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: meta-data IO uses: blk-bio
Mar 13 20:38:24 linbit1 kernel: [504695.718029] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: rs_discard_granularity feature disabled
Mar 13 20:38:24 linbit1 kernel: [504695.718055] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: disk( Diskless -> Attaching )
Mar 13 20:38:24 linbit1 kernel: [504695.718066] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: Maximum number of peer devices = 7
Mar 13 20:38:24 linbit1 kernel: [504695.718976] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Method to ensure write ordering: drain
Mar 13 20:38:24 linbit1 kernel: [504695.718985] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:24 linbit1 kernel: [504695.726673] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:24 linbit1 kernel: [504695.726683] drbd1022: detected capacity change from 0 to 20971520
Mar 13 20:38:24 linbit1 kernel: [504695.726687] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: size = 10 GB (10485760 KB)
Mar 13 20:38:24 linbit1 kernel: [504695.735668] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: recounting of set bits took additional 4ms
Mar 13 20:38:24 linbit1 kernel: [504695.735688] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: disk( Attaching -> Inconsistent )
Mar 13 20:38:24 linbit1 kernel: [504695.735693] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: attached to current UUID: 0000000000000004
Mar 13 20:38:24 linbit1 kernel: [504695.738386] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 2340834722 (0->-1 3/1)
Mar 13 20:38:24 linbit1 kernel: [504695.738586] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: State change 2340834722: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:24 linbit1 kernel: [504695.738591] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 2340834722 (0ms)
Mar 13 20:38:24 linbit1 kernel: [504695.738600] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: role( Secondary -> Primary )
Mar 13 20:38:24 linbit1 kernel: [504695.739029] loop17: detected capacity change from 0 to 20971520
Mar 13 20:38:24 linbit1 systemd-udevd[2449863]: dm-34: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-34' failed with exit code 1.
Mar 13 20:38:24 linbit1 systemd-udevd[2449862]: dm-35: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-35' failed with exit code 1.
Mar 13 20:38:25 linbit1 kernel: [504696.035795] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Starting worker thread (from drbdsetup [2450089])
Mar 13 20:38:25 linbit1 kernel: [504696.043651] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: meta-data IO uses: blk-bio
Mar 13 20:38:25 linbit1 kernel: [504696.043749] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: rs_discard_granularity feature disabled
Mar 13 20:38:25 linbit1 kernel: [504696.043811] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: disk( Diskless -> Attaching )
Mar 13 20:38:25 linbit1 kernel: [504696.043822] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: Maximum number of peer devices = 7
Mar 13 20:38:25 linbit1 systemd-udevd[2450036]: drbd1023: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1023' failed with exit code 1.
Mar 13 20:38:25 linbit1 kernel: [504696.044818] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Method to ensure write ordering: drain
Mar 13 20:38:25 linbit1 kernel: [504696.044834] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:25 linbit1 kernel: [504696.052456] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:25 linbit1 kernel: [504696.052463] drbd1023: detected capacity change from 0 to 20971520
Mar 13 20:38:25 linbit1 kernel: [504696.052467] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: size = 10 GB (10485760 KB)
Mar 13 20:38:25 linbit1 kernel: [504696.062098] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: recounting of set bits took additional 8ms
Mar 13 20:38:25 linbit1 kernel: [504696.062115] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: disk( Attaching -> Inconsistent )
Mar 13 20:38:25 linbit1 kernel: [504696.062120] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: attached to current UUID: 0000000000000004
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25.061788422Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n4-4-mmrrc,Uid:fa760197-68d3-48e7-abee-bc22cb31e820,Namespace:default,Attempt:0,}"
Mar 13 20:38:25 linbit1 kernel: [504696.080525] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: rs_discard_granularity feature disabled
Mar 13 20:38:25 linbit1 kernel: [504696.314873] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:38:25 linbit1 kernel: [504696.315020] IPv6: ADDRCONF(NETDEV_CHANGE): cali23222f1245f: link becomes ready
Mar 13 20:38:25 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 101 seen, reloading interface list
Mar 13 20:38:25 linbit1 systemd-networkd[2297221]: cali23222f1245f: Link UP
Mar 13 20:38:25 linbit1 systemd-networkd[2297221]: cali23222f1245f: Gained carrier
Mar 13 20:38:25 linbit1 systemd-udevd[2449863]: Using default interface naming scheme 'v249'.
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.095 [INFO][2450118] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.126 [INFO][2450118] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0 fio-bench-r2-n4-4- default  fa760197-68d3-48e7-abee-bc22cb31e820 17524472 0 2023-03-13 20:37:27 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:12ffebee-34e4-41c4-bff7-9f98637928ac job-name:fio-bench-r2-n4-4 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n4-4-mmrrc eth0 default [] []   [kns.default ksa.default.default] cali23222f1245f  [] []}} ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Namespace="default" Pod="fio-bench-r2-n4-4-mmrrc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.126 [INFO][2450118] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Namespace="default" Pod="fio-bench-r2-n4-4-mmrrc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.174 [INFO][2450154] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.202 [INFO][2450154] ipam_plugin.go 264: Auto assigning IP ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x400043bb20), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n4-4-mmrrc", "timestamp":"2023-03-13 20:38:25.174801404 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.203 [INFO][2450154] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.218 [INFO][2450154] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.230 [INFO][2450154] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.242 [INFO][2450154] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.247 [INFO][2450154] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.255 [INFO][2450154] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.256 [INFO][2450154] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.263 [INFO][2450154] ipam.go 1680: Creating new handle: k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.274 [INFO][2450154] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.291 [INFO][2450154] ipam.go 1214: Successfully claimed IPs: [10.1.217.30/26] block=10.1.217.0/26 handle="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.292 [INFO][2450154] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.30/26] handle="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" host="linbit1"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.292 [INFO][2450154] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.30/26] IPv6=[] ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.296 [INFO][2450118] k8s.go 383: Populated endpoint ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Namespace="default" Pod="fio-bench-r2-n4-4-mmrrc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0", GenerateName:"fio-bench-r2-n4-4-", Namespace:"default", SelfLink:"", UID:"fa760197-68d3-48e7-abee-bc22cb31e820", ResourceVersion:"17524472", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 27, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"12ffebee-34e4-41c4-bff7-9f98637928ac", "job-name":"fio-bench-r2-n4-4", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n4-4-mmrrc", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.30/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali23222f1245f", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.296 [INFO][2450118] k8s.go 384: Calico CNI using IPs: [10.1.217.30/32] ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Namespace="default" Pod="fio-bench-r2-n4-4-mmrrc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.296 [INFO][2450118] dataplane_linux.go 68: Setting the host side veth name to cali23222f1245f ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Namespace="default" Pod="fio-bench-r2-n4-4-mmrrc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.299 [INFO][2450118] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Namespace="default" Pod="fio-bench-r2-n4-4-mmrrc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.340 [INFO][2450118] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Namespace="default" Pod="fio-bench-r2-n4-4-mmrrc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0", GenerateName:"fio-bench-r2-n4-4-", Namespace:"default", SelfLink:"", UID:"fa760197-68d3-48e7-abee-bc22cb31e820", ResourceVersion:"17524472", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 27, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"12ffebee-34e4-41c4-bff7-9f98637928ac", "job-name":"fio-bench-r2-n4-4", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa", Pod:"fio-bench-r2-n4-4-mmrrc", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.30/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali23222f1245f", MAC:"ce:e9:71:98:9f:bd", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:25.356 [INFO][2450118] k8s.go 489: Wrote updated endpoint to datastore ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Namespace="default" Pod="fio-bench-r2-n4-4-mmrrc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25.383873036Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25.383987716Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25.384015516Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25.384272117Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa pid=2450210 runtime=io.containerd.runc.v2
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25.477817360Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n4-4-mmrrc,Uid:fa760197-68d3-48e7-abee-bc22cb31e820,Namespace:default,Attempt:0,} returns sandbox id \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\""
Mar 13 20:38:25 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:25.479579445Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.229224471Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.232335760Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.237064335Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.240462425Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.241905949Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.245462520Z" level=info msg="CreateContainer within sandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.252526821Z" level=info msg="CreateContainer within sandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1\""
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.253103703Z" level=info msg="StartContainer for \"4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1\""
Mar 13 20:38:26 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:26.377708160Z" level=info msg="StartContainer for \"4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1\" returns successfully"
Mar 13 20:38:26 linbit1 systemd-networkd[2297221]: cali23222f1245f: Gained IPv6LL
Mar 13 20:38:28 linbit1 kernel: [504699.790670] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: rs_discard_granularity feature disabled
Mar 13 20:38:28 linbit1 kernel: [504699.853667] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: rs_discard_granularity feature disabled
Mar 13 20:38:28 linbit1 kernel: [504699.906054] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: rs_discard_granularity feature disabled
Mar 13 20:38:28 linbit1 kernel: [504699.953140] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 4179073816 (0->-1 7683/4609)
Mar 13 20:38:28 linbit1 kernel: [504699.953147] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 4179073816 (0ms)
Mar 13 20:38:28 linbit1 kernel: [504699.953151] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: role( Secondary -> Primary )
Mar 13 20:38:28 linbit1 kernel: [504699.953153] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: disk( Inconsistent -> UpToDate )
Mar 13 20:38:28 linbit1 kernel: [504699.953227] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: size = 10 GB (10485760 KB)
Mar 13 20:38:28 linbit1 kernel: [504699.953317] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Forced to consider local data as UpToDate!
Mar 13 20:38:28 linbit1 kernel: [504699.953373] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: new current UUID: AE53567CC443078F weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:28 linbit1 kernel: [504699.959130] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 1032593670 (0->-1 3/2)
Mar 13 20:38:28 linbit1 kernel: [504699.959138] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 1032593670 (0ms)
Mar 13 20:38:28 linbit1 kernel: [504699.959146] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: role( Primary -> Secondary )
Mar 13 20:38:28 linbit1 kernel: [504699.965505] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 3978725464 (0->-1 3/1)
Mar 13 20:38:28 linbit1 kernel: [504699.965515] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 3978725464 (0ms)
Mar 13 20:38:28 linbit1 kernel: [504699.965522] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: role( Secondary -> Primary )
Mar 13 20:38:28 linbit1 kernel: [504699.974331] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 3066441474 (0->-1 3/2)
Mar 13 20:38:28 linbit1 kernel: [504699.974342] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 3066441474 (0ms)
Mar 13 20:38:28 linbit1 kernel: [504699.974351] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: role( Primary -> Secondary )
Mar 13 20:38:28 linbit1 kernel: [504699.995004] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: rs_discard_granularity feature disabled
Mar 13 20:38:29 linbit1 systemd-udevd[2450513]: dm-36: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-36' failed with exit code 1.
Mar 13 20:38:29 linbit1 systemd-udevd[2450512]: dm-37: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-37' failed with exit code 1.
Mar 13 20:38:29 linbit1 kernel: [504700.351262] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Starting worker thread (from drbdsetup [2450569])
Mar 13 20:38:29 linbit1 kernel: [504700.354788] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Starting sender thread (from drbdsetup [2450574])
Mar 13 20:38:29 linbit1 kernel: [504700.356508] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Starting sender thread (from drbdsetup [2450577])
Mar 13 20:38:29 linbit1 systemd-udevd[2450512]: drbd1021: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1021' failed with exit code 1.
Mar 13 20:38:29 linbit1 kernel: [504700.408959] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: meta-data IO uses: blk-bio
Mar 13 20:38:29 linbit1 kernel: [504700.409029] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: rs_discard_granularity feature disabled
Mar 13 20:38:29 linbit1 kernel: [504700.409070] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: disk( Diskless -> Attaching )
Mar 13 20:38:29 linbit1 kernel: [504700.409079] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: Maximum number of peer devices = 7
Mar 13 20:38:29 linbit1 kernel: [504700.409753] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Method to ensure write ordering: drain
Mar 13 20:38:29 linbit1 kernel: [504700.409761] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:29 linbit1 kernel: [504700.414170] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:29 linbit1 kernel: [504700.414176] drbd1021: detected capacity change from 0 to 20971520
Mar 13 20:38:29 linbit1 kernel: [504700.414179] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: size = 10 GB (10485760 KB)
Mar 13 20:38:29 linbit1 kernel: [504700.419984] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: recounting of set bits took additional 4ms
Mar 13 20:38:29 linbit1 kernel: [504700.420000] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: disk( Attaching -> Inconsistent )
Mar 13 20:38:29 linbit1 kernel: [504700.420004] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: attached to current UUID: 0000000000000004
Mar 13 20:38:29 linbit1 kernel: [504700.421685] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:29 linbit1 kernel: [504700.421831] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Starting receiver thread (from drbd_w_pvc-a79c [2450570])
Mar 13 20:38:29 linbit1 kernel: [504700.421982] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:29 linbit1 kernel: [504700.423207] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:29 linbit1 kernel: [504700.423242] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Starting receiver thread (from drbd_w_pvc-a79c [2450570])
Mar 13 20:38:29 linbit1 kernel: [504700.423414] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:29 linbit1 kernel: [504700.480535] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: rs_discard_granularity feature disabled
Mar 13 20:38:29 linbit1 kernel: [504700.648947] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Preparing remote state change 1529901819
Mar 13 20:38:29 linbit1 kernel: [504700.649354] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Committing remote state change 1529901819 (primary_nodes=1)
Mar 13 20:38:29 linbit1 kernel: [504700.649370] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: peer( Secondary -> Primary )
Mar 13 20:38:29 linbit1 kernel: [504700.945917] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:29 linbit1 kernel: [504700.945927] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:29 linbit1 kernel: [504700.946155] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:29 linbit1 kernel: [504700.954863] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:29 linbit1 kernel: [504700.954874] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:29 linbit1 kernel: [504700.955006] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:29 linbit1 kernel: [504700.975968] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Preparing remote state change 553843336
Mar 13 20:38:29 linbit1 kernel: [504701.007994] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: drbd_sync_handshake:
Mar 13 20:38:29 linbit1 kernel: [504701.008002] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: self 0000000000000004:0000000000000000:0BBAA133EAF80B88:0000000000000000 bits:0 flags:24
Mar 13 20:38:29 linbit1 kernel: [504701.008011] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: peer 733662B8AC91036A:0BBAA133EAF80B88:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:29 linbit1 kernel: [504701.008020] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: uuid_compare()=target-set-bitmap by rule=just-created-self
Mar 13 20:38:29 linbit1 kernel: [504701.008025] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: Setting and writing the whole bitmap, fresh node
Mar 13 20:38:30 linbit1 kernel: [504701.017089] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Committing remote state change 553843336 (primary_nodes=0)
Mar 13 20:38:30 linbit1 kernel: [504701.017108] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:30 linbit1 kernel: [504701.017113] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: quorum( no -> yes )
Mar 13 20:38:30 linbit1 kernel: [504701.017119] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:38:30 linbit1 kernel: [504701.017262] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: size = 10 GB (10485760 KB)
Mar 13 20:38:30 linbit1 kernel: [504701.017289] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Preparing cluster-wide state change 2270234483 (1->2 499/146)
Mar 13 20:38:30 linbit1 kernel: [504701.019306] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:30 linbit1 kernel: [504701.020193] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:30 linbit1 kernel: [504701.020201] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: setting UUIDs to 0BBAA133EAF80B88:0000000000000000:0BBAA133EAF80B88:0000000000000000
Mar 13 20:38:30 linbit1 kernel: [504701.055986] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: self 0BBAA133EAF80B88:0000000000000000:0BBAA133EAF80B88:0000000000000000 bits:0 flags:0
Mar 13 20:38:30 linbit1 kernel: [504701.055997] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:38:30 linbit1 kernel: [504701.058951] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: State change 2270234483: primary_nodes=0, weak_nodes=0
Mar 13 20:38:30 linbit1 kernel: [504701.058959] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Committing cluster-wide state change 2270234483 (40ms)
Mar 13 20:38:30 linbit1 kernel: [504701.059007] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:30 linbit1 kernel: [504701.059013] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:30 linbit1 kernel: [504701.227950] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: resync-susp( no -> connection dependency )
Mar 13 20:38:30 linbit1 kernel: [504701.227958] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: repl( WFBitMapT -> SyncTarget )
Mar 13 20:38:30 linbit1 kernel: [504701.228251] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: Began resync as SyncTarget (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:31 linbit1 kernel: [504702.951038] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67: Starting worker thread (from drbdsetup [2450793])
Mar 13 20:38:31 linbit1 kernel: [504702.955153] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Starting sender thread (from drbdsetup [2450799])
Mar 13 20:38:31 linbit1 kernel: [504702.956838] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Starting sender thread (from drbdsetup [2450802])
Mar 13 20:38:31 linbit1 kernel: [504702.961431] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:31 linbit1 kernel: [504702.961500] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Starting receiver thread (from drbd_w_pvc-7a5e [2450794])
Mar 13 20:38:31 linbit1 kernel: [504702.961685] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:31 linbit1 kernel: [504702.962797] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:31 linbit1 kernel: [504702.962835] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Starting receiver thread (from drbd_w_pvc-7a5e [2450794])
Mar 13 20:38:31 linbit1 kernel: [504702.962965] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:31 linbit1 systemd-udevd[2450512]: drbd1025: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1025' failed with exit code 1.
Mar 13 20:38:31 linbit1 systemd-udevd[2450512]: dm-38: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-38' failed with exit code 1.
Mar 13 20:38:32 linbit1 systemd-udevd[2450512]: dm-39: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-39' failed with exit code 1.
Mar 13 20:38:32 linbit1 kernel: [504703.311307] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Starting worker thread (from drbdsetup [2450905])
Mar 13 20:38:32 linbit1 kernel: [504703.319191] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: meta-data IO uses: blk-bio
Mar 13 20:38:32 linbit1 kernel: [504703.319306] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: rs_discard_granularity feature disabled
Mar 13 20:38:32 linbit1 kernel: [504703.319338] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: disk( Diskless -> Attaching )
Mar 13 20:38:32 linbit1 kernel: [504703.319349] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: Maximum number of peer devices = 7
Mar 13 20:38:32 linbit1 kernel: [504703.320265] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Method to ensure write ordering: drain
Mar 13 20:38:32 linbit1 kernel: [504703.320274] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:32 linbit1 kernel: [504703.326745] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:32 linbit1 kernel: [504703.326753] drbd1028: detected capacity change from 0 to 20971520
Mar 13 20:38:32 linbit1 kernel: [504703.326757] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: size = 10 GB (10485760 KB)
Mar 13 20:38:32 linbit1 systemd-udevd[2450512]: drbd1028: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1028' failed with exit code 1.
Mar 13 20:38:32 linbit1 kernel: [504703.331373] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: recounting of set bits took additional 0ms
Mar 13 20:38:32 linbit1 kernel: [504703.331385] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: disk( Attaching -> Inconsistent )
Mar 13 20:38:32 linbit1 kernel: [504703.331388] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: attached to current UUID: 0000000000000004
Mar 13 20:38:32 linbit1 kernel: [504703.352215] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: rs_discard_granularity feature disabled
Mar 13 20:38:32 linbit1 kernel: [504703.402405] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 298516652 (0->-1 7683/4609)
Mar 13 20:38:32 linbit1 kernel: [504703.402416] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 298516652 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.402423] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: role( Secondary -> Primary )
Mar 13 20:38:32 linbit1 kernel: [504703.402427] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: disk( Inconsistent -> UpToDate )
Mar 13 20:38:32 linbit1 kernel: [504703.402500] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: size = 10 GB (10485760 KB)
Mar 13 20:38:32 linbit1 kernel: [504703.402592] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Forced to consider local data as UpToDate!
Mar 13 20:38:32 linbit1 kernel: [504703.402670] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: new current UUID: 65129730F932FD8B weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:32 linbit1 kernel: [504703.411657] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 330020966 (0->-1 3/2)
Mar 13 20:38:32 linbit1 kernel: [504703.411667] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 330020966 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.411674] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: role( Primary -> Secondary )
Mar 13 20:38:32 linbit1 kernel: [504703.418929] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 663904682 (0->-1 3/1)
Mar 13 20:38:32 linbit1 kernel: [504703.418936] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 663904682 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.418941] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: role( Secondary -> Primary )
Mar 13 20:38:32 linbit1 kernel: [504703.428174] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 3120435189 (0->-1 3/2)
Mar 13 20:38:32 linbit1 kernel: [504703.428182] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 3120435189 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.428189] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: role( Primary -> Secondary )
Mar 13 20:38:32 linbit1 kernel: [504703.448103] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: rs_discard_granularity feature disabled
Mar 13 20:38:32 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:32.469311    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-400778d2-ad89-410f-90fd-625de046658b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b\") pod \"fio-bench-r2-n7-2-bnfd5\" (UID: \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\") " pod="default/fio-bench-r2-n7-2-bnfd5"
Mar 13 20:38:32 linbit1 kernel: [504703.492026] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:32 linbit1 kernel: [504703.492037] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:32 linbit1 kernel: [504703.492170] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:32 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:32.485367    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-400778d2-ad89-410f-90fd-625de046658b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b\") pod \"fio-bench-r2-n7-2-bnfd5\" (UID: \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\") device path: \"\"" pod="default/fio-bench-r2-n7-2-bnfd5"
Mar 13 20:38:32 linbit1 kernel: [504703.502432] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Preparing remote state change 2512165113
Mar 13 20:38:32 linbit1 kernel: [504703.502818] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Committing remote state change 2512165113 (primary_nodes=1)
Mar 13 20:38:32 linbit1 kernel: [504703.502827] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: peer( Secondary -> Primary )
Mar 13 20:38:32 linbit1 kernel: [504703.516174] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: rs_discard_granularity feature disabled
Mar 13 20:38:32 linbit1 kernel: [504703.537034] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Preparing remote state change 1389091197
Mar 13 20:38:32 linbit1 kernel: [504703.537252] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Preparing remote state change 3462571132
Mar 13 20:38:32 linbit1 kernel: [504703.537285] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Committing remote state change 1389091197 (primary_nodes=1)
Mar 13 20:38:32 linbit1 kernel: [504703.537299] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: peer( Secondary -> Primary )
Mar 13 20:38:32 linbit1 kernel: [504703.537664] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Committing remote state change 3462571132 (primary_nodes=1)
Mar 13 20:38:32 linbit1 kernel: [504703.537676] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: peer( Secondary -> Primary )
Mar 13 20:38:32 linbit1 kernel: [504703.568589] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: rs_discard_granularity feature disabled
Mar 13 20:38:32 linbit1 kernel: [504703.577975] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Preparing remote state change 1120705700
Mar 13 20:38:32 linbit1 kernel: [504703.578018] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Preparing remote state change 3360031296
Mar 13 20:38:32 linbit1 kernel: [504703.578499] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Committing remote state change 3360031296 (primary_nodes=0)
Mar 13 20:38:32 linbit1 kernel: [504703.578597] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: peer( Primary -> Secondary )
Mar 13 20:38:32 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:32.571145    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-400778d2-ad89-410f-90fd-625de046658b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b\") pod \"fio-bench-r2-n7-2-bnfd5\" (UID: \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\") DevicePath \"\"" pod="default/fio-bench-r2-n7-2-bnfd5"
Mar 13 20:38:32 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:32.576558    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-400778d2-ad89-410f-90fd-625de046658b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b\") pod \"fio-bench-r2-n7-2-bnfd5\" (UID: \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\") DevicePath \"csi-1e782c823a6eb80c3d23e586f30a104fde76d3edc57908a3c11173ea5c2e0a05\"" pod="default/fio-bench-r2-n7-2-bnfd5"
Mar 13 20:38:32 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:32.582818    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:32 linbit1 kernel: [504703.615948] drbd1025: detected capacity change from 0 to 20971520
Mar 13 20:38:32 linbit1 kernel: [504703.615959] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025: size = 10 GB (10485760 KB)
Mar 13 20:38:32 linbit1 kernel: [504703.616016] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:38:32 linbit1 kernel: [504703.616023] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: peer B6CEB0CB01A622F8:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:32 linbit1 kernel: [504703.624531] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 4232616023 (0->-1 7683/4609)
Mar 13 20:38:32 linbit1 kernel: [504703.624541] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 4232616023 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.624548] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: role( Secondary -> Primary )
Mar 13 20:38:32 linbit1 kernel: [504703.624552] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: disk( Inconsistent -> UpToDate )
Mar 13 20:38:32 linbit1 kernel: [504703.624629] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: size = 10 GB (10485760 KB)
Mar 13 20:38:32 linbit1 kernel: [504703.624719] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Forced to consider local data as UpToDate!
Mar 13 20:38:32 linbit1 kernel: [504703.624778] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: new current UUID: 25005DEAB10165E1 weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:32 linbit1 kernel: [504703.625962] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Committing remote state change 1120705700 (primary_nodes=0)
Mar 13 20:38:32 linbit1 kernel: [504703.625981] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:32 linbit1 kernel: [504703.625986] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:38:32 linbit1 kernel: [504703.632695] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 2773413790 (0->-1 3/2)
Mar 13 20:38:32 linbit1 kernel: [504703.632702] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 2773413790 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.632707] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: role( Primary -> Secondary )
Mar 13 20:38:32 linbit1 kernel: [504703.638354] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 773693296 (0->-1 3/1)
Mar 13 20:38:32 linbit1 kernel: [504703.638549] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: State change 773693296: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:32 linbit1 kernel: [504703.638556] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 773693296 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.638567] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: role( Secondary -> Primary )
Mar 13 20:38:32 linbit1 kernel: [504703.638814] loop18: detected capacity change from 0 to 20971520
Mar 13 20:38:32 linbit1 kernel: [504703.640725] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 1805641715 (0->-1 3/1)
Mar 13 20:38:32 linbit1 kernel: [504703.640733] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 1805641715 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.640739] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: role( Secondary -> Primary )
Mar 13 20:38:32 linbit1 kernel: [504703.669434] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 2108476574 (0->-1 3/2)
Mar 13 20:38:32 linbit1 kernel: [504703.669444] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 2108476574 (0ms)
Mar 13 20:38:32 linbit1 kernel: [504703.669451] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: role( Primary -> Secondary )
Mar 13 20:38:32 linbit1 kernel: [504703.814507] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Preparing remote state change 3080844976
Mar 13 20:38:32 linbit1 kernel: [504703.814932] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Committing remote state change 3080844976 (primary_nodes=1)
Mar 13 20:38:32 linbit1 kernel: [504703.814937] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: peer( Secondary -> Primary )
Mar 13 20:38:32 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:32.870502625Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n7-2-bnfd5,Uid:1a7e1b25-e333-4558-9a33-7f7b07d5d61c,Namespace:default,Attempt:0,}"
Mar 13 20:38:33 linbit1 kernel: [504704.134545] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:38:33 linbit1 kernel: [504704.134696] IPv6: ADDRCONF(NETDEV_CHANGE): cali898463cd9b2: link becomes ready
Mar 13 20:38:33 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 102 seen, reloading interface list
Mar 13 20:38:33 linbit1 systemd-networkd[2297221]: cali898463cd9b2: Link UP
Mar 13 20:38:33 linbit1 systemd-networkd[2297221]: cali898463cd9b2: Gained carrier
Mar 13 20:38:33 linbit1 systemd-udevd[2450513]: Using default interface naming scheme 'v249'.
Mar 13 20:38:33 linbit1 kernel: [504704.202378] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Preparing remote state change 1790136433
Mar 13 20:38:33 linbit1 kernel: [504704.202603] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Committing remote state change 1790136433 (primary_nodes=1)
Mar 13 20:38:33 linbit1 kernel: [504704.202619] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: peer( Secondary -> Primary )
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:32.905 [INFO][2451084] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:32.934 [INFO][2451084] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0 fio-bench-r2-n7-2- default  1a7e1b25-e333-4558-9a33-7f7b07d5d61c 17524604 0 2023-03-13 20:37:28 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:352bff24-d8d3-4465-befe-eabb62bb9486 job-name:fio-bench-r2-n7-2 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n7-2-bnfd5 eth0 default [] []   [kns.default ksa.default.default] cali898463cd9b2  [] []}} ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Namespace="default" Pod="fio-bench-r2-n7-2-bnfd5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:32.934 [INFO][2451084] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Namespace="default" Pod="fio-bench-r2-n7-2-bnfd5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:32.989 [INFO][2451111] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.025 [INFO][2451111] ipam_plugin.go 264: Auto assigning IP ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x40005afbf0), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n7-2-bnfd5", "timestamp":"2023-03-13 20:38:32.989690305 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.025 [INFO][2451111] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.035 [INFO][2451111] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.047 [INFO][2451111] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.061 [INFO][2451111] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.066 [INFO][2451111] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.074 [INFO][2451111] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.074 [INFO][2451111] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.079 [INFO][2451111] ipam.go 1680: Creating new handle: k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.090 [INFO][2451111] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.111 [INFO][2451111] ipam.go 1214: Successfully claimed IPs: [10.1.217.3/26] block=10.1.217.0/26 handle="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.111 [INFO][2451111] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.3/26] handle="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" host="linbit1"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.111 [INFO][2451111] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.3/26] IPv6=[] ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.116 [INFO][2451084] k8s.go 383: Populated endpoint ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Namespace="default" Pod="fio-bench-r2-n7-2-bnfd5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0", GenerateName:"fio-bench-r2-n7-2-", Namespace:"default", SelfLink:"", UID:"1a7e1b25-e333-4558-9a33-7f7b07d5d61c", ResourceVersion:"17524604", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"352bff24-d8d3-4465-befe-eabb62bb9486", "job-name":"fio-bench-r2-n7-2", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n7-2-bnfd5", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.3/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali898463cd9b2", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.116 [INFO][2451084] k8s.go 384: Calico CNI using IPs: [10.1.217.3/32] ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Namespace="default" Pod="fio-bench-r2-n7-2-bnfd5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.116 [INFO][2451084] dataplane_linux.go 68: Setting the host side veth name to cali898463cd9b2 ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Namespace="default" Pod="fio-bench-r2-n7-2-bnfd5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.118 [INFO][2451084] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Namespace="default" Pod="fio-bench-r2-n7-2-bnfd5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.175 [INFO][2451084] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Namespace="default" Pod="fio-bench-r2-n7-2-bnfd5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0", GenerateName:"fio-bench-r2-n7-2-", Namespace:"default", SelfLink:"", UID:"1a7e1b25-e333-4558-9a33-7f7b07d5d61c", ResourceVersion:"17524604", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"352bff24-d8d3-4465-befe-eabb62bb9486", "job-name":"fio-bench-r2-n7-2", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5", Pod:"fio-bench-r2-n7-2-bnfd5", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.3/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali898463cd9b2", MAC:"3a:e9:39:89:30:99", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:33.199 [INFO][2451084] k8s.go 489: Wrote updated endpoint to datastore ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Namespace="default" Pod="fio-bench-r2-n7-2-bnfd5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.232454759Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.232562439Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.232589880Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.232949441Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5 pid=2451176 runtime=io.containerd.runc.v2
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.324052116Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n7-2-bnfd5,Uid:1a7e1b25-e333-4558-9a33-7f7b07d5d61c,Namespace:default,Attempt:0,} returns sandbox id \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\""
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.325964482Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:33 linbit1 kernel: [504704.484031] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:33 linbit1 kernel: [504704.484043] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:33 linbit1 kernel: [504704.484277] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:33 linbit1 kernel: [504704.561955] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Preparing remote state change 3429733721
Mar 13 20:38:33 linbit1 kernel: [504704.601911] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Committing remote state change 3429733721 (primary_nodes=0)
Mar 13 20:38:33 linbit1 kernel: [504704.602061] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Preparing remote state change 2855589078
Mar 13 20:38:33 linbit1 kernel: [504704.631976] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:38:33 linbit1 kernel: [504704.631985] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: peer 0000000000000004:0000000000000000:4B43951C0E42492A:0000000000000000 bits:0 flags:24
Mar 13 20:38:33 linbit1 kernel: [504704.638821] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Committing remote state change 2855589078 (primary_nodes=0)
Mar 13 20:38:33 linbit1 kernel: [504704.638843] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:33 linbit1 kernel: [504704.638848] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025: quorum( no -> yes )
Mar 13 20:38:33 linbit1 kernel: [504704.638854] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established )
Mar 13 20:38:33 linbit1 kernel: [504704.807056] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: resync-susp( no -> peer )
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.882554244Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.885593893Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.888967584Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.893410997Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.895443363Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.898811813Z" level=info msg="CreateContainer within sandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.904530951Z" level=info msg="CreateContainer within sandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29\""
Mar 13 20:38:33 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:33.904964952Z" level=info msg="StartContainer for \"a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29\""
Mar 13 20:38:34 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:34.014922444Z" level=info msg="StartContainer for \"a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29\" returns successfully"
Mar 13 20:38:34 linbit1 kernel: [504705.877939] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2: Starting worker thread (from drbdsetup [2451334])
Mar 13 20:38:34 linbit1 kernel: [504705.883028] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Starting sender thread (from drbdsetup [2451339])
Mar 13 20:38:34 linbit1 kernel: [504705.884570] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Starting sender thread (from drbdsetup [2451342])
Mar 13 20:38:34 linbit1 kernel: [504705.888483] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:34 linbit1 kernel: [504705.888532] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Starting receiver thread (from drbd_w_pvc-7e6b [2451335])
Mar 13 20:38:34 linbit1 kernel: [504705.888672] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:34 linbit1 kernel: [504705.889299] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:34 linbit1 kernel: [504705.889333] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Starting receiver thread (from drbd_w_pvc-7e6b [2451335])
Mar 13 20:38:34 linbit1 systemd-udevd[2450512]: drbd1026: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1026' failed with exit code 1.
Mar 13 20:38:34 linbit1 kernel: [504705.889447] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:34 linbit1 kernel: [504705.915135] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Starting sender thread (from drbdsetup [2451370])
Mar 13 20:38:34 linbit1 kernel: [504705.916290] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Starting sender thread (from drbdsetup [2451372])
Mar 13 20:38:34 linbit1 kernel: [504705.953324] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: rs_discard_granularity feature disabled
Mar 13 20:38:34 linbit1 kernel: [504705.993644] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:34 linbit1 kernel: [504705.993703] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Starting receiver thread (from drbd_w_pvc-8385 [2449920])
Mar 13 20:38:34 linbit1 kernel: [504705.993834] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:34 linbit1 kernel: [504705.995013] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:34 linbit1 kernel: [504705.995020] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: quorum( yes -> no )
Mar 13 20:38:34 linbit1 kernel: [504705.995112] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Starting receiver thread (from drbd_w_pvc-8385 [2449920])
Mar 13 20:38:34 linbit1 kernel: [504705.995187] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:35 linbit1 kernel: [504706.019351] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: rs_discard_granularity feature disabled
Mar 13 20:38:35 linbit1 kernel: [504706.073354] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: rs_discard_granularity feature disabled
Mar 13 20:38:35 linbit1 systemd-networkd[2297221]: cali898463cd9b2: Gained IPv6LL
Mar 13 20:38:35 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:35.092786    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-b01e907a-4035-4022-9701-ff7a07ac1588\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588\") pod \"fio-bench-r2-n7-3-mfqd4\" (UID: \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\") " pod="default/fio-bench-r2-n7-3-mfqd4"
Mar 13 20:38:35 linbit1 kernel: [504706.125370] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee: Starting worker thread (from drbdsetup [2451415])
Mar 13 20:38:35 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:35.109630    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-b01e907a-4035-4022-9701-ff7a07ac1588\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588\") pod \"fio-bench-r2-n7-3-mfqd4\" (UID: \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\") device path: \"\"" pod="default/fio-bench-r2-n7-3-mfqd4"
Mar 13 20:38:35 linbit1 kernel: [504706.129377] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Starting sender thread (from drbdsetup [2451420])
Mar 13 20:38:35 linbit1 kernel: [504706.130357] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Starting sender thread (from drbdsetup [2451423])
Mar 13 20:38:35 linbit1 kernel: [504706.133828] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:35 linbit1 kernel: [504706.133901] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Starting receiver thread (from drbd_w_pvc-dfca [2451416])
Mar 13 20:38:35 linbit1 kernel: [504706.134022] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:35 linbit1 kernel: [504706.135138] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:35 linbit1 kernel: [504706.135174] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Starting receiver thread (from drbd_w_pvc-dfca [2451416])
Mar 13 20:38:35 linbit1 kernel: [504706.135270] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:35 linbit1 systemd-udevd[2450512]: drbd1030: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1030' failed with exit code 1.
Mar 13 20:38:35 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:35.194252    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-b01e907a-4035-4022-9701-ff7a07ac1588\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588\") pod \"fio-bench-r2-n7-3-mfqd4\" (UID: \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\") DevicePath \"\"" pod="default/fio-bench-r2-n7-3-mfqd4"
Mar 13 20:38:35 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:35.200742    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-b01e907a-4035-4022-9701-ff7a07ac1588\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588\") pod \"fio-bench-r2-n7-3-mfqd4\" (UID: \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\") DevicePath \"csi-1b9a5fa3609c219ea21664bdad6ff21e6048031111994f2729e60e06bbf9d4f6\"" pod="default/fio-bench-r2-n7-3-mfqd4"
Mar 13 20:38:35 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:35.208324    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:35 linbit1 kernel: [504706.260994] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 1309079340 (0->-1 3/1)
Mar 13 20:38:35 linbit1 kernel: [504706.261208] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change 1309079340: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:35 linbit1 kernel: [504706.261214] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 1309079340 (0ms)
Mar 13 20:38:35 linbit1 kernel: [504706.261224] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: role( Secondary -> Primary )
Mar 13 20:38:35 linbit1 kernel: [504706.261463] loop19: detected capacity change from 0 to 20971520
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35.425891310Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n7-3-mfqd4,Uid:d9db77e8-50b0-4260-89f3-ddb5a0de385a,Namespace:default,Attempt:0,}"
Mar 13 20:38:35 linbit1 kernel: [504706.513956] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:35 linbit1 kernel: [504706.513968] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:35 linbit1 kernel: [504706.514080] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:35 linbit1 kernel: [504706.635960] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 4163673972 (0->2 499/146)
Mar 13 20:38:35 linbit1 kernel: [504706.660018] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:35 linbit1 kernel: [504706.660028] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:35 linbit1 kernel: [504706.660272] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:35 linbit1 kernel: [504706.679953] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit2: self AE53567CC443078E:7F8F8AC31FD72FD6:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:35 linbit1 kernel: [504706.679963] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:35 linbit1 kernel: [504706.679975] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: State change 4163673972: primary_nodes=0, weak_nodes=0
Mar 13 20:38:35 linbit1 kernel: [504706.679981] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 4163673972 (44ms)
Mar 13 20:38:35 linbit1 kernel: [504706.680008] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:35 linbit1 kernel: [504706.680014] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:35 linbit1 kernel: [504706.684327] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:38:35 linbit1 kernel: [504706.684473] IPv6: ADDRCONF(NETDEV_CHANGE): calide24a04cc7c: link becomes ready
Mar 13 20:38:35 linbit1 kernel: [504706.717904] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Preparing remote state change 1534013124
Mar 13 20:38:35 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 103 seen, reloading interface list
Mar 13 20:38:35 linbit1 systemd-networkd[2297221]: calide24a04cc7c: Link UP
Mar 13 20:38:35 linbit1 systemd-networkd[2297221]: calide24a04cc7c: Gained carrier
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.462 [INFO][2451473] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.490 [INFO][2451473] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0 fio-bench-r2-n7-3- default  d9db77e8-50b0-4260-89f3-ddb5a0de385a 17524795 0 2023-03-13 20:37:28 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:19942729-e70c-406e-82f8-a1ec419111c7 job-name:fio-bench-r2-n7-3 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n7-3-mfqd4 eth0 default [] []   [kns.default ksa.default.default] calide24a04cc7c  [] []}} ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Namespace="default" Pod="fio-bench-r2-n7-3-mfqd4" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.490 [INFO][2451473] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Namespace="default" Pod="fio-bench-r2-n7-3-mfqd4" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.538 [INFO][2451502] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.566 [INFO][2451502] ipam_plugin.go 264: Auto assigning IP ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x4000cfea90), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n7-3-mfqd4", "timestamp":"2023-03-13 20:38:35.538057489 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.566 [INFO][2451502] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.577 [INFO][2451502] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.589 [INFO][2451502] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.606 [INFO][2451502] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.612 [INFO][2451502] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.620 [INFO][2451502] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.620 [INFO][2451502] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.625 [INFO][2451502] ipam.go 1680: Creating new handle: k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.639 [INFO][2451502] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.661 [INFO][2451502] ipam.go 1214: Successfully claimed IPs: [10.1.217.34/26] block=10.1.217.0/26 handle="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.661 [INFO][2451502] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.34/26] handle="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" host="linbit1"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.661 [INFO][2451502] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.34/26] IPv6=[] ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.665 [INFO][2451473] k8s.go 383: Populated endpoint ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Namespace="default" Pod="fio-bench-r2-n7-3-mfqd4" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0", GenerateName:"fio-bench-r2-n7-3-", Namespace:"default", SelfLink:"", UID:"d9db77e8-50b0-4260-89f3-ddb5a0de385a", ResourceVersion:"17524795", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"19942729-e70c-406e-82f8-a1ec419111c7", "job-name":"fio-bench-r2-n7-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n7-3-mfqd4", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.34/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"calide24a04cc7c", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.666 [INFO][2451473] k8s.go 384: Calico CNI using IPs: [10.1.217.34/32] ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Namespace="default" Pod="fio-bench-r2-n7-3-mfqd4" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.666 [INFO][2451473] dataplane_linux.go 68: Setting the host side veth name to calide24a04cc7c ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Namespace="default" Pod="fio-bench-r2-n7-3-mfqd4" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.668 [INFO][2451473] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Namespace="default" Pod="fio-bench-r2-n7-3-mfqd4" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.717 [INFO][2451473] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Namespace="default" Pod="fio-bench-r2-n7-3-mfqd4" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0", GenerateName:"fio-bench-r2-n7-3-", Namespace:"default", SelfLink:"", UID:"d9db77e8-50b0-4260-89f3-ddb5a0de385a", ResourceVersion:"17524795", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"19942729-e70c-406e-82f8-a1ec419111c7", "job-name":"fio-bench-r2-n7-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61", Pod:"fio-bench-r2-n7-3-mfqd4", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.34/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"calide24a04cc7c", MAC:"aa:9b:20:46:09:6e", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:35.739 [INFO][2451473] k8s.go 489: Wrote updated endpoint to datastore ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Namespace="default" Pod="fio-bench-r2-n7-3-mfqd4" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:38:35 linbit1 kernel: [504706.779938] drbd1030: detected capacity change from 0 to 20971520
Mar 13 20:38:35 linbit1 kernel: [504706.779948] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030: size = 10 GB (10485760 KB)
Mar 13 20:38:35 linbit1 kernel: [504706.780100] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:38:35 linbit1 kernel: [504706.780109] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit2: peer D57C92C0E41C71AC:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:35 linbit1 kernel: [504706.780342] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Committing remote state change 1534013124 (primary_nodes=0)
Mar 13 20:38:35 linbit1 kernel: [504706.780364] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:35 linbit1 kernel: [504706.780369] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35.768356945Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35.768493465Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35.768526465Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35.768762226Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61 pid=2451558 runtime=io.containerd.runc.v2
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35.872845261Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n7-3-mfqd4,Uid:d9db77e8-50b0-4260-89f3-ddb5a0de385a,Namespace:default,Attempt:0,} returns sandbox id \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\""
Mar 13 20:38:35 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:35.874623786Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:35 linbit1 kernel: [504706.933945] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:35 linbit1 kernel: [504706.933957] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:35 linbit1 kernel: [504706.934129] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:36 linbit1 kernel: [504707.030039] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Preparing remote state change 931647862
Mar 13 20:38:36 linbit1 kernel: [504707.063965] drbd1026: detected capacity change from 0 to 20971520
Mar 13 20:38:36 linbit1 kernel: [504707.063974] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026: size = 10 GB (10485760 KB)
Mar 13 20:38:36 linbit1 kernel: [504707.064021] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:38:36 linbit1 kernel: [504707.064025] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit2: peer 0000000000000004:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:36 linbit1 kernel: [504707.081915] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Committing remote state change 931647862 (primary_nodes=0)
Mar 13 20:38:36 linbit1 kernel: [504707.081934] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:36 linbit1 kernel: [504707.081938] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established )
Mar 13 20:38:36 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:36.200674    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\") pod \"fio-bench-r2-n7-4-ckbx6\" (UID: \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\") " pod="default/fio-bench-r2-n7-4-ckbx6"
Mar 13 20:38:36 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:36.216888    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\") pod \"fio-bench-r2-n7-4-ckbx6\" (UID: \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\") device path: \"\"" pod="default/fio-bench-r2-n7-4-ckbx6"
Mar 13 20:38:36 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:36.301552    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\") pod \"fio-bench-r2-n7-4-ckbx6\" (UID: \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\") DevicePath \"\"" pod="default/fio-bench-r2-n7-4-ckbx6"
Mar 13 20:38:36 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:36.307511    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\") pod \"fio-bench-r2-n7-4-ckbx6\" (UID: \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\") DevicePath \"csi-2e473281e88099d916d9856d6a2f4d937743ce847859a00e367a6591a0e8278a\"" pod="default/fio-bench-r2-n7-4-ckbx6"
Mar 13 20:38:36 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:36.314050    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.447540038Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.450353446Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.455082101Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.459968835Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.462799724Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.465614733Z" level=info msg="CreateContainer within sandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.472627794Z" level=info msg="CreateContainer within sandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe\""
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.473068555Z" level=info msg="StartContainer for \"a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe\""
Mar 13 20:38:36 linbit1 kernel: [504707.536025] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 2307077733 (0->-1 3/1)
Mar 13 20:38:36 linbit1 kernel: [504707.536301] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: State change 2307077733: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:38:36 linbit1 kernel: [504707.536307] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 2307077733 (0ms)
Mar 13 20:38:36 linbit1 kernel: [504707.536318] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: role( Secondary -> Primary )
Mar 13 20:38:36 linbit1 kernel: [504707.536621] loop20: detected capacity change from 0 to 20971520
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.569266446Z" level=info msg="StartContainer for \"a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe\" returns successfully"
Mar 13 20:38:36 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36.778380518Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n7-4-ckbx6,Uid:05dd1efc-8486-47dd-89df-7ee739a2d7e8,Namespace:default,Attempt:0,}"
Mar 13 20:38:36 linbit1 systemd-udevd[2450512]: dm-40: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-40' failed with exit code 1.
Mar 13 20:38:37 linbit1 kernel: [504708.026037] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:38:37 linbit1 kernel: [504708.026193] IPv6: ADDRCONF(NETDEV_CHANGE): cali03e68263c68: link becomes ready
Mar 13 20:38:37 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 104 seen, reloading interface list
Mar 13 20:38:37 linbit1 systemd-networkd[2297221]: cali03e68263c68: Link UP
Mar 13 20:38:37 linbit1 systemd-networkd[2297221]: cali03e68263c68: Gained carrier
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.810 [INFO][2451716] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.837 [INFO][2451716] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0 fio-bench-r2-n7-4- default  05dd1efc-8486-47dd-89df-7ee739a2d7e8 17524804 0 2023-03-13 20:37:28 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:bbdc38de-0a9a-42f8-8694-8fd2cd8df641 job-name:fio-bench-r2-n7-4 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n7-4-ckbx6 eth0 default [] []   [kns.default ksa.default.default] cali03e68263c68  [] []}} ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Namespace="default" Pod="fio-bench-r2-n7-4-ckbx6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.837 [INFO][2451716] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Namespace="default" Pod="fio-bench-r2-n7-4-ckbx6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.885 [INFO][2451746] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.911 [INFO][2451746] ipam_plugin.go 264: Auto assigning IP ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x4000a10ff0), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n7-4-ckbx6", "timestamp":"2023-03-13 20:38:36.885690482 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:36Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.911 [INFO][2451746] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.920 [INFO][2451746] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.932 [INFO][2451746] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.944 [INFO][2451746] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.949 [INFO][2451746] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.958 [INFO][2451746] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.958 [INFO][2451746] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.963 [INFO][2451746] ipam.go 1680: Creating new handle: k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:36.973 [INFO][2451746] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.002 [INFO][2451746] ipam.go 1214: Successfully claimed IPs: [10.1.217.53/26] block=10.1.217.0/26 handle="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.002 [INFO][2451746] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.53/26] handle="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" host="linbit1"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.003 [INFO][2451746] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.53/26] IPv6=[] ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.007 [INFO][2451716] k8s.go 383: Populated endpoint ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Namespace="default" Pod="fio-bench-r2-n7-4-ckbx6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0", GenerateName:"fio-bench-r2-n7-4-", Namespace:"default", SelfLink:"", UID:"05dd1efc-8486-47dd-89df-7ee739a2d7e8", ResourceVersion:"17524804", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"bbdc38de-0a9a-42f8-8694-8fd2cd8df641", "job-name":"fio-bench-r2-n7-4", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n7-4-ckbx6", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.53/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali03e68263c68", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.007 [INFO][2451716] k8s.go 384: Calico CNI using IPs: [10.1.217.53/32] ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Namespace="default" Pod="fio-bench-r2-n7-4-ckbx6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.007 [INFO][2451716] dataplane_linux.go 68: Setting the host side veth name to cali03e68263c68 ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Namespace="default" Pod="fio-bench-r2-n7-4-ckbx6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.010 [INFO][2451716] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Namespace="default" Pod="fio-bench-r2-n7-4-ckbx6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.073 [INFO][2451716] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Namespace="default" Pod="fio-bench-r2-n7-4-ckbx6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0", GenerateName:"fio-bench-r2-n7-4-", Namespace:"default", SelfLink:"", UID:"05dd1efc-8486-47dd-89df-7ee739a2d7e8", ResourceVersion:"17524804", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"bbdc38de-0a9a-42f8-8694-8fd2cd8df641", "job-name":"fio-bench-r2-n7-4", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557", Pod:"fio-bench-r2-n7-4-ckbx6", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.53/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali03e68263c68", MAC:"32:05:f3:79:15:34", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:38:37.094 [INFO][2451716] k8s.go 489: Wrote updated endpoint to datastore ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Namespace="default" Pod="fio-bench-r2-n7-4-ckbx6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:38:37 linbit1 systemd-udevd[2450512]: dm-41: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-41' failed with exit code 1.
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.122041117Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.122150637Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.122178957Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.122432678Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557 pid=2451826 runtime=io.containerd.runc.v2
Mar 13 20:38:37 linbit1 kernel: [504708.244650] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: Starting worker thread (from drbdsetup [2451887])
Mar 13 20:38:37 linbit1 kernel: [504708.248861] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Starting sender thread (from drbdsetup [2451892])
Mar 13 20:38:37 linbit1 kernel: [504708.250462] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Starting sender thread (from drbdsetup [2451895])
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.236294502Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n7-4-ckbx6,Uid:05dd1efc-8486-47dd-89df-7ee739a2d7e8,Namespace:default,Attempt:0,} returns sandbox id \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\""
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.238010067Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:38:37 linbit1 systemd-udevd[2450512]: drbd1029: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1029' failed with exit code 1.
Mar 13 20:38:37 linbit1 kernel: [504708.292585] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: meta-data IO uses: blk-bio
Mar 13 20:38:37 linbit1 kernel: [504708.292686] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: rs_discard_granularity feature disabled
Mar 13 20:38:37 linbit1 kernel: [504708.292729] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: disk( Diskless -> Attaching )
Mar 13 20:38:37 linbit1 kernel: [504708.292742] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: Maximum number of peer devices = 7
Mar 13 20:38:37 linbit1 kernel: [504708.293695] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: Method to ensure write ordering: drain
Mar 13 20:38:37 linbit1 kernel: [504708.293705] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:37 linbit1 kernel: [504708.300724] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:37 linbit1 kernel: [504708.300732] drbd1029: detected capacity change from 0 to 20971520
Mar 13 20:38:37 linbit1 kernel: [504708.300736] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: size = 10 GB (10485760 KB)
Mar 13 20:38:37 linbit1 kernel: [504708.309738] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: recounting of set bits took additional 8ms
Mar 13 20:38:37 linbit1 kernel: [504708.309759] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: disk( Attaching -> Inconsistent )
Mar 13 20:38:37 linbit1 kernel: [504708.309764] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: attached to current UUID: 0000000000000004
Mar 13 20:38:37 linbit1 kernel: [504708.311460] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:37 linbit1 kernel: [504708.311634] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Starting receiver thread (from drbd_w_pvc-7803 [2451888])
Mar 13 20:38:37 linbit1 kernel: [504708.311780] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:37 linbit1 kernel: [504708.313082] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:37 linbit1 kernel: [504708.313253] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Starting receiver thread (from drbd_w_pvc-7803 [2451888])
Mar 13 20:38:37 linbit1 kernel: [504708.313421] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:37 linbit1 kernel: [504708.354640] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: rs_discard_granularity feature disabled
Mar 13 20:38:37 linbit1 systemd-networkd[2297221]: calide24a04cc7c: Gained IPv6LL
Mar 13 20:38:37 linbit1 kernel: [504708.393312] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 653765672 (0->-1 7683/4609)
Mar 13 20:38:37 linbit1 kernel: [504708.393318] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 653765672 (0ms)
Mar 13 20:38:37 linbit1 kernel: [504708.393321] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: role( Secondary -> Primary )
Mar 13 20:38:37 linbit1 kernel: [504708.393323] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: disk( Inconsistent -> UpToDate )
Mar 13 20:38:37 linbit1 kernel: [504708.393397] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: size = 10 GB (10485760 KB)
Mar 13 20:38:37 linbit1 kernel: [504708.393483] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Forced to consider local data as UpToDate!
Mar 13 20:38:37 linbit1 kernel: [504708.393526] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: new current UUID: 49EB060E1205E2B5 weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:37 linbit1 kernel: [504708.402959] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 80852693 (0->-1 3/2)
Mar 13 20:38:37 linbit1 kernel: [504708.402967] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 80852693 (0ms)
Mar 13 20:38:37 linbit1 kernel: [504708.402975] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: role( Primary -> Secondary )
Mar 13 20:38:37 linbit1 kernel: [504708.413481] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 1426701487 (0->-1 3/1)
Mar 13 20:38:37 linbit1 kernel: [504708.413490] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 1426701487 (0ms)
Mar 13 20:38:37 linbit1 kernel: [504708.413498] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: role( Secondary -> Primary )
Mar 13 20:38:37 linbit1 kernel: [504708.422908] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 3526333043 (0->-1 3/2)
Mar 13 20:38:37 linbit1 kernel: [504708.422913] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 3526333043 (0ms)
Mar 13 20:38:37 linbit1 kernel: [504708.422916] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: role( Primary -> Secondary )
Mar 13 20:38:37 linbit1 kernel: [504708.445648] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Starting sender thread (from drbdsetup [2451967])
Mar 13 20:38:37 linbit1 kernel: [504708.447240] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Starting sender thread (from drbdsetup [2451969])
Mar 13 20:38:37 linbit1 kernel: [504708.501095] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: rs_discard_granularity feature disabled
Mar 13 20:38:37 linbit1 kernel: [504708.533537] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:37 linbit1 kernel: [504708.533609] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Starting receiver thread (from drbd_w_pvc-9428 [2450001])
Mar 13 20:38:37 linbit1 kernel: [504708.533782] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:37 linbit1 kernel: [504708.534818] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:37 linbit1 kernel: [504708.534824] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: quorum( yes -> no )
Mar 13 20:38:37 linbit1 kernel: [504708.535033] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Starting receiver thread (from drbd_w_pvc-9428 [2450001])
Mar 13 20:38:37 linbit1 kernel: [504708.535138] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:37 linbit1 kernel: [504708.560211] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Starting sender thread (from drbdsetup [2451996])
Mar 13 20:38:37 linbit1 kernel: [504708.561702] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Starting sender thread (from drbdsetup [2451998])
Mar 13 20:38:37 linbit1 kernel: [504708.621800] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: rs_discard_granularity feature disabled
Mar 13 20:38:37 linbit1 kernel: [504708.661451] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:37 linbit1 kernel: [504708.661524] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Starting receiver thread (from drbd_w_pvc-c917 [2450090])
Mar 13 20:38:37 linbit1 kernel: [504708.661687] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:37 linbit1 kernel: [504708.662764] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:37 linbit1 kernel: [504708.662770] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: quorum( yes -> no )
Mar 13 20:38:37 linbit1 kernel: [504708.663003] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Starting receiver thread (from drbd_w_pvc-c917 [2450090])
Mar 13 20:38:37 linbit1 kernel: [504708.663155] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:37 linbit1 kernel: [504708.675993] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:37 linbit1 kernel: [504708.676000] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:37 linbit1 kernel: [504708.676234] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:37 linbit1 kernel: [504708.758891] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Preparing remote state change 892068495
Mar 13 20:38:37 linbit1 kernel: [504708.783976] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:38:37 linbit1 kernel: [504708.783985] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit3: peer DD0C7B1BDAF9C81C:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:37 linbit1 kernel: [504708.786815] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Committing remote state change 892068495 (primary_nodes=0)
Mar 13 20:38:37 linbit1 kernel: [504708.786836] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:37 linbit1 kernel: [504708.786840] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026: quorum( no -> yes )
Mar 13 20:38:37 linbit1 kernel: [504708.786846] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.785590243Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.788676732Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.794289469Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.798256361Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.800125927Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.802467734Z" level=info msg="CreateContainer within sandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.809424755Z" level=info msg="CreateContainer within sandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba\""
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.809922516Z" level=info msg="StartContainer for \"b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba\""
Mar 13 20:38:37 linbit1 kernel: [504708.836001] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:37 linbit1 kernel: [504708.836010] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:37 linbit1 kernel: [504708.836177] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:37 linbit1 kernel: [504708.838875] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Preparing remote state change 82349089
Mar 13 20:38:37 linbit1 kernel: [504708.885128] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Committing remote state change 82349089 (primary_nodes=0)
Mar 13 20:38:37 linbit1 kernel: [504708.888771] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit2: resync-susp( no -> peer )
Mar 13 20:38:37 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:38:37.916583359Z" level=info msg="StartContainer for \"b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba\" returns successfully"
Mar 13 20:38:37 linbit1 kernel: [504708.999947] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: Preparing cluster-wide state change 2386236043 (1->2 499/146)
Mar 13 20:38:38 linbit1 kernel: [504709.027941] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit2: self 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:38 linbit1 kernel: [504709.027952] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:38 linbit1 kernel: [504709.027968] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: State change 2386236043: primary_nodes=0, weak_nodes=0
Mar 13 20:38:38 linbit1 kernel: [504709.027975] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: Committing cluster-wide state change 2386236043 (28ms)
Mar 13 20:38:38 linbit1 kernel: [504709.028006] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:38 linbit1 kernel: [504709.028013] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:38 linbit1 kernel: [504709.063980] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:38 linbit1 kernel: [504709.063992] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:38 linbit1 kernel: [504709.064204] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:38 linbit1 kernel: [504709.082974] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:38 linbit1 kernel: [504709.082989] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:38 linbit1 kernel: [504709.083104] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:38 linbit1 kernel: [504709.107941] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 3499449271 (0->2 499/146)
Mar 13 20:38:38 linbit1 kernel: [504709.135938] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: self 65129730F932FD8A:F137059BC44F14E1:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:38 linbit1 kernel: [504709.135950] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:38 linbit1 kernel: [504709.145982] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change 3499449271: primary_nodes=0, weak_nodes=0
Mar 13 20:38:38 linbit1 kernel: [504709.145992] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 3499449271 (36ms)
Mar 13 20:38:38 linbit1 kernel: [504709.146027] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:38 linbit1 kernel: [504709.146032] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:38 linbit1 kernel: [504709.187987] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:38 linbit1 kernel: [504709.187995] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:38 linbit1 kernel: [504709.188243] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:38 linbit1 kernel: [504709.191946] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 3135528482 (0->1 499/146)
Mar 13 20:38:38 linbit1 kernel: [504709.247946] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 891689961 (0->2 499/146)
Mar 13 20:38:38 linbit1 kernel: [504709.251987] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: drbd_sync_handshake:
Mar 13 20:38:38 linbit1 kernel: [504709.251996] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: self AE53567CC443078E:7F8F8AC31FD72FD6:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:38:38 linbit1 kernel: [504709.252005] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:38 linbit1 kernel: [504709.252012] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:38:38 linbit1 kernel: [504709.252018] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:38:38 linbit1 kernel: [504709.254896] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: State change 3135528482: primary_nodes=0, weak_nodes=0
Mar 13 20:38:38 linbit1 kernel: [504709.254906] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 3135528482 (60ms)
Mar 13 20:38:38 linbit1 kernel: [504709.254956] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:38 linbit1 kernel: [504709.254960] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: quorum( no -> yes )
Mar 13 20:38:38 linbit1 kernel: [504709.254965] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:38:38 linbit1 kernel: [504709.256384] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:38 linbit1 kernel: [504709.256412] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Preparing remote state change 583674469
Mar 13 20:38:38 linbit1 kernel: [504709.259167] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:38 linbit1 kernel: [504709.259190] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: repl( WFBitMapS -> SyncSource )
Mar 13 20:38:38 linbit1 kernel: [504709.259489] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:38 linbit1 kernel: [504709.287933] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit2: self 25005DEAB10165E0:FC45BA0DD84D9E4F:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:38 linbit1 kernel: [504709.287948] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:38 linbit1 kernel: [504709.287970] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: State change 891689961: primary_nodes=0, weak_nodes=0
Mar 13 20:38:38 linbit1 kernel: [504709.287977] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 891689961 (40ms)
Mar 13 20:38:38 linbit1 kernel: [504709.288023] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:38 linbit1 kernel: [504709.288030] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:38 linbit1 kernel: [504709.290884] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Committing remote state change 583674469 (primary_nodes=0)
Mar 13 20:38:38 linbit1 kernel: [504709.476025] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:38 linbit1 kernel: [504709.476037] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:38 linbit1 kernel: [504709.476193] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:38 linbit1 kernel: [504709.614801] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Preparing remote state change 3809853840
Mar 13 20:38:38 linbit1 kernel: [504709.655976] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:38:38 linbit1 kernel: [504709.655985] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit3: peer 0000000000000004:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:38 linbit1 kernel: [504709.656169] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Committing remote state change 3809853840 (primary_nodes=0)
Mar 13 20:38:38 linbit1 kernel: [504709.656186] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:38 linbit1 kernel: [504709.656191] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030: quorum( no -> yes )
Mar 13 20:38:38 linbit1 kernel: [504709.656197] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established )
Mar 13 20:38:38 linbit1 kernel: [504709.657873] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Preparing remote state change 1469616673
Mar 13 20:38:38 linbit1 kernel: [504709.708748] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Committing remote state change 1469616673 (primary_nodes=0)
Mar 13 20:38:38 linbit1 kernel: [504709.712454] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit3: resync-susp( no -> peer )
Mar 13 20:38:39 linbit1 systemd-networkd[2297221]: cali03e68263c68: Gained IPv6LL
Mar 13 20:38:40 linbit1 kernel: [504711.884269] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: rs_discard_granularity feature disabled
Mar 13 20:38:40 linbit1 kernel: [504711.950728] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: rs_discard_granularity feature disabled
Mar 13 20:38:41 linbit1 kernel: [504712.164013] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:41 linbit1 kernel: [504712.164025] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:41 linbit1 kernel: [504712.164234] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:41 linbit1 kernel: [504712.246795] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Preparing remote state change 4222694426
Mar 13 20:38:41 linbit1 kernel: [504712.272018] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: drbd_sync_handshake:
Mar 13 20:38:41 linbit1 kernel: [504712.272027] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: self 0000000000000004:0000000000000000:3C1CC23E444EC1D4:0000000000000000 bits:0 flags:24
Mar 13 20:38:41 linbit1 kernel: [504712.272036] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: peer 010864CFD47DAB04:3C1CC23E444EC1D4:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:41 linbit1 kernel: [504712.272044] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: uuid_compare()=target-set-bitmap by rule=just-created-self
Mar 13 20:38:41 linbit1 kernel: [504712.272050] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: Setting and writing the whole bitmap, fresh node
Mar 13 20:38:41 linbit1 kernel: [504712.289923] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Committing remote state change 4222694426 (primary_nodes=0)
Mar 13 20:38:41 linbit1 kernel: [504712.289947] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:41 linbit1 kernel: [504712.289952] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: quorum( no -> yes )
Mar 13 20:38:41 linbit1 kernel: [504712.289957] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:38:41 linbit1 kernel: [504712.290342] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: size = 10 GB (10485760 KB)
Mar 13 20:38:41 linbit1 kernel: [504712.292355] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:41 linbit1 kernel: [504712.293584] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:41 linbit1 kernel: [504712.293594] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: setting UUIDs to 3C1CC23E444EC1D4:0000000000000000:3C1CC23E444EC1D4:0000000000000000
Mar 13 20:38:41 linbit1 kernel: [504712.293616] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: repl( WFBitMapT -> SyncTarget )
Mar 13 20:38:41 linbit1 kernel: [504712.293621] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit2: resync-susp( no -> connection dependency )
Mar 13 20:38:41 linbit1 kernel: [504712.293931] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: Began resync as SyncTarget (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:41 linbit1 kernel: [504712.326791] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Preparing remote state change 3911387276
Mar 13 20:38:41 linbit1 kernel: [504712.358803] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Committing remote state change 3911387276 (primary_nodes=0)
Mar 13 20:38:41 linbit1 kernel: [504712.616088] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:41 linbit1 kernel: [504712.616098] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:41 linbit1 kernel: [504712.616319] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:41 linbit1 kernel: [504712.743924] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 2043012460 (0->1 499/146)
Mar 13 20:38:41 linbit1 kernel: [504712.803952] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: drbd_sync_handshake:
Mar 13 20:38:41 linbit1 kernel: [504712.803958] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: self 65129730F932FD8A:F137059BC44F14E1:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:38:41 linbit1 kernel: [504712.803961] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:41 linbit1 kernel: [504712.803964] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:38:41 linbit1 kernel: [504712.803966] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:38:41 linbit1 kernel: [504712.805202] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change 2043012460: primary_nodes=0, weak_nodes=0
Mar 13 20:38:41 linbit1 kernel: [504712.805207] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 2043012460 (60ms)
Mar 13 20:38:41 linbit1 kernel: [504712.805244] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:41 linbit1 kernel: [504712.805246] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: quorum( no -> yes )
Mar 13 20:38:41 linbit1 kernel: [504712.805248] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:38:41 linbit1 kernel: [504712.805918] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:41 linbit1 kernel: [504712.805934] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Preparing remote state change 4064760313
Mar 13 20:38:41 linbit1 kernel: [504712.808526] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:41 linbit1 kernel: [504712.808543] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: repl( WFBitMapS -> SyncSource )
Mar 13 20:38:41 linbit1 kernel: [504712.808729] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:41 linbit1 kernel: [504712.842782] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Committing remote state change 4064760313 (primary_nodes=0)
Mar 13 20:38:42 linbit1 kernel: [504713.018816] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:42 linbit1 kernel: [504713.018826] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:42 linbit1 kernel: [504713.019059] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:42 linbit1 kernel: [504713.073897] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Preparing remote state change 3802184390
Mar 13 20:38:42 linbit1 kernel: [504713.106170] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Committing remote state change 3802184390 (primary_nodes=0)
Mar 13 20:38:42 linbit1 kernel: [504713.106194] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 4138964839 (0->1 499/146)
Mar 13 20:38:42 linbit1 kernel: [504713.132000] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: drbd_sync_handshake:
Mar 13 20:38:42 linbit1 kernel: [504713.132007] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: self 25005DEAB10165E0:FC45BA0DD84D9E4F:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:38:42 linbit1 kernel: [504713.132016] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:42 linbit1 kernel: [504713.132023] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:38:42 linbit1 kernel: [504713.132029] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:38:42 linbit1 kernel: [504713.149802] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: State change 4138964839: primary_nodes=0, weak_nodes=0
Mar 13 20:38:42 linbit1 kernel: [504713.149815] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 4138964839 (44ms)
Mar 13 20:38:42 linbit1 kernel: [504713.149888] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:42 linbit1 kernel: [504713.149893] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: quorum( no -> yes )
Mar 13 20:38:42 linbit1 kernel: [504713.149899] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:38:42 linbit1 kernel: [504713.151672] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:42 linbit1 kernel: [504713.155171] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:42 linbit1 kernel: [504713.155198] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: repl( WFBitMapS -> SyncSource )
Mar 13 20:38:42 linbit1 kernel: [504713.155737] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:42 linbit1 kernel: [504713.219037] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Starting sender thread (from drbdsetup [2452358])
Mar 13 20:38:42 linbit1 kernel: [504713.220340] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Starting sender thread (from drbdsetup [2452360])
Mar 13 20:38:42 linbit1 kernel: [504713.321947] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: rs_discard_granularity feature disabled
Mar 13 20:38:42 linbit1 kernel: [504713.345706] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:42 linbit1 kernel: [504713.345715] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: quorum( yes -> no )
Mar 13 20:38:42 linbit1 kernel: [504713.345881] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Starting receiver thread (from drbd_w_pvc-8f12 [2450906])
Mar 13 20:38:42 linbit1 kernel: [504713.346037] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:42 linbit1 kernel: [504713.347189] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:42 linbit1 kernel: [504713.347243] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Starting receiver thread (from drbd_w_pvc-8f12 [2450906])
Mar 13 20:38:42 linbit1 kernel: [504713.347362] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:42 linbit1 systemd-udevd[2452413]: dm-42: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-42' failed with exit code 1.
Mar 13 20:38:42 linbit1 systemd-udevd[2452413]: dm-43: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-43' failed with exit code 1.
Mar 13 20:38:42 linbit1 kernel: [504713.703877] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: Starting worker thread (from drbdsetup [2452496])
Mar 13 20:38:42 linbit1 kernel: [504713.707847] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Starting sender thread (from drbdsetup [2452507])
Mar 13 20:38:42 linbit1 kernel: [504713.709407] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Starting sender thread (from drbdsetup [2452510])
Mar 13 20:38:42 linbit1 systemd-udevd[2452413]: drbd1027: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1027' failed with exit code 1.
Mar 13 20:38:42 linbit1 kernel: [504713.753767] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: meta-data IO uses: blk-bio
Mar 13 20:38:42 linbit1 kernel: [504713.753891] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: rs_discard_granularity feature disabled
Mar 13 20:38:42 linbit1 kernel: [504713.753958] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: disk( Diskless -> Attaching )
Mar 13 20:38:42 linbit1 kernel: [504713.753974] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: Maximum number of peer devices = 7
Mar 13 20:38:42 linbit1 kernel: [504713.754973] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: Method to ensure write ordering: drain
Mar 13 20:38:42 linbit1 kernel: [504713.754996] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:42 linbit1 kernel: [504713.762020] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:42 linbit1 kernel: [504713.762028] drbd1027: detected capacity change from 0 to 20971520
Mar 13 20:38:42 linbit1 kernel: [504713.762032] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: size = 10 GB (10485760 KB)
Mar 13 20:38:42 linbit1 kernel: [504713.773185] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: recounting of set bits took additional 8ms
Mar 13 20:38:42 linbit1 kernel: [504713.773208] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: disk( Attaching -> Inconsistent )
Mar 13 20:38:42 linbit1 kernel: [504713.773213] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: attached to current UUID: 0000000000000004
Mar 13 20:38:42 linbit1 kernel: [504713.774750] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:42 linbit1 kernel: [504713.774853] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Starting receiver thread (from drbd_w_pvc-a2d2 [2452499])
Mar 13 20:38:42 linbit1 kernel: [504713.774982] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:42 linbit1 kernel: [504713.775626] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:42 linbit1 kernel: [504713.775658] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Starting receiver thread (from drbd_w_pvc-a2d2 [2452499])
Mar 13 20:38:42 linbit1 kernel: [504713.775791] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:42 linbit1 kernel: [504713.874027] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:38:42 linbit1 kernel: [504713.874036] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:42 linbit1 kernel: [504713.874275] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:42 linbit1 kernel: [504713.931947] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 2356707290 (0->1 499/146)
Mar 13 20:38:42 linbit1 kernel: [504713.979965] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: drbd_sync_handshake:
Mar 13 20:38:42 linbit1 kernel: [504713.979973] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: self 49EB060E1205E2B4:FE91709162BFE132:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:38:42 linbit1 kernel: [504713.979982] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:38:42 linbit1 kernel: [504713.979990] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:38:42 linbit1 kernel: [504713.979995] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:38:42 linbit1 kernel: [504713.982980] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: State change 2356707290: primary_nodes=0, weak_nodes=0
Mar 13 20:38:42 linbit1 kernel: [504713.982990] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 2356707290 (48ms)
Mar 13 20:38:42 linbit1 kernel: [504713.983036] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:42 linbit1 kernel: [504713.983041] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: quorum( no -> yes )
Mar 13 20:38:42 linbit1 kernel: [504713.983045] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:38:42 linbit1 kernel: [504713.984436] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:42 linbit1 kernel: [504713.987824] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:42 linbit1 kernel: [504713.987850] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:38:42 linbit1 kernel: [504713.988175] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:43 linbit1 kernel: [504714.290033] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:43 linbit1 kernel: [504714.290042] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:43 linbit1 kernel: [504714.290304] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:43 linbit1 kernel: [504714.415943] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: Preparing cluster-wide state change 2010250438 (1->2 499/146)
Mar 13 20:38:43 linbit1 kernel: [504714.455908] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit2: self 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:43 linbit1 kernel: [504714.455917] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:38:43 linbit1 kernel: [504714.457998] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: State change 2010250438: primary_nodes=0, weak_nodes=0
Mar 13 20:38:43 linbit1 kernel: [504714.458004] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: Committing cluster-wide state change 2010250438 (40ms)
Mar 13 20:38:43 linbit1 kernel: [504714.458033] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:43 linbit1 kernel: [504714.458038] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:44 linbit1 kernel: [504715.571935] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: rs_discard_granularity feature disabled
Mar 13 20:38:45 linbit1 kernel: [504716.519987] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:38:45 linbit1 kernel: [504716.519996] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:45 linbit1 kernel: [504716.520181] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:45 linbit1 kernel: [504716.587904] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 1386106167 (0->2 499/146)
Mar 13 20:38:45 linbit1 kernel: [504716.631910] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: self 49EB060E1205E2B4:FE91709162BFE132:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:38:45 linbit1 kernel: [504716.631923] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:38:45 linbit1 kernel: [504716.631940] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: State change 1386106167: primary_nodes=0, weak_nodes=0
Mar 13 20:38:45 linbit1 kernel: [504716.631947] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 1386106167 (44ms)
Mar 13 20:38:45 linbit1 kernel: [504716.631995] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:45 linbit1 kernel: [504716.632003] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:38:45 linbit1 kernel: [504716.632131] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Preparing remote state change 1929110200
Mar 13 20:38:45 linbit1 kernel: [504716.634808] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:45 linbit1 kernel: [504716.634818] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:45 linbit1 kernel: [504716.634925] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:45 linbit1 kernel: [504716.673888] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Committing remote state change 1929110200 (primary_nodes=0)
Mar 13 20:38:45 linbit1 kernel: [504716.678813] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Preparing remote state change 22975479
Mar 13 20:38:45 linbit1 kernel: [504716.708049] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: drbd_sync_handshake:
Mar 13 20:38:45 linbit1 kernel: [504716.708058] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: self 0000000000000004:0000000000000000:DC5637792E5DB850:0000000000000000 bits:0 flags:24
Mar 13 20:38:45 linbit1 kernel: [504716.708067] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: peer 2CB4450069C887E6:DC5637792E5DB851:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:45 linbit1 kernel: [504716.708075] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: uuid_compare()=target-set-bitmap by rule=just-created-self
Mar 13 20:38:45 linbit1 kernel: [504716.708081] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: Setting and writing the whole bitmap, fresh node
Mar 13 20:38:45 linbit1 kernel: [504716.722619] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Committing remote state change 22975479 (primary_nodes=0)
Mar 13 20:38:45 linbit1 kernel: [504716.722638] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:45 linbit1 kernel: [504716.722641] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: quorum( no -> yes )
Mar 13 20:38:45 linbit1 kernel: [504716.722644] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:38:45 linbit1 kernel: [504716.722901] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: size = 10 GB (10485760 KB)
Mar 13 20:38:45 linbit1 kernel: [504716.724905] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:45 linbit1 kernel: [504716.725779] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:38:45 linbit1 kernel: [504716.725787] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: setting UUIDs to DC5637792E5DB850:0000000000000000:DC5637792E5DB850:0000000000000000
Mar 13 20:38:45 linbit1 kernel: [504716.725807] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: repl( WFBitMapT -> SyncTarget )
Mar 13 20:38:45 linbit1 kernel: [504716.725810] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit2: resync-susp( no -> connection dependency )
Mar 13 20:38:45 linbit1 kernel: [504716.726078] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: Began resync as SyncTarget (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:38:45 linbit1 kernel: [504716.742731] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Preparing remote state change 2821805218
Mar 13 20:38:45 linbit1 kernel: [504716.794147] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Committing remote state change 2821805218 (primary_nodes=0)
Mar 13 20:38:45 linbit1 kernel: [504716.935084] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: rs_discard_granularity feature disabled
Mar 13 20:38:45 linbit1 kernel: [504717.001024] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: rs_discard_granularity feature disabled
Mar 13 20:38:47 linbit1 kernel: [504718.856544] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: rs_discard_granularity feature disabled
Mar 13 20:38:49 linbit1 kernel: [504720.214982] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: rs_discard_granularity feature disabled
Mar 13 20:38:49 linbit1 kernel: [504720.274635] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: rs_discard_granularity feature disabled
Mar 13 20:38:51 linbit1 kernel: [504722.071195] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: rs_discard_granularity feature disabled
Mar 13 20:38:51 linbit1 kernel: [504722.120434] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: rs_discard_granularity feature disabled
Mar 13 20:38:51 linbit1 kernel: [504722.153815] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Preparing remote state change 2658646378
Mar 13 20:38:51 linbit1 kernel: [504722.177282] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: rs_discard_granularity feature disabled
Mar 13 20:38:51 linbit1 kernel: [504722.227678] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Committing remote state change 2658646378 (primary_nodes=1)
Mar 13 20:38:52 linbit1 kernel: [504723.657887] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Preparing remote state change 1163538975
Mar 13 20:38:52 linbit1 kernel: [504723.701829] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Committing remote state change 1163538975 (primary_nodes=1)
Mar 13 20:38:52 linbit1 kernel: [504723.903997] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: rs_discard_granularity feature disabled
Mar 13 20:38:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:54.077057    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:38:54 linbit1 kernel: [504725.121867] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: sock was shut down by peer
Mar 13 20:38:54 linbit1 kernel: [504725.121882] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:38:54 linbit1 kernel: [504725.121890] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( Connected -> BrokenPipe ) peer( Primary -> Unknown )
Mar 13 20:38:54 linbit1 kernel: [504725.121896] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:38:54 linbit1 kernel: [504725.122123] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Terminating sender thread
Mar 13 20:38:54 linbit1 kernel: [504725.122169] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Starting sender thread (from drbd_r_pvc-e2bb [2449117])
Mar 13 20:38:54 linbit1 kernel: [504725.220017] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Connection closed
Mar 13 20:38:54 linbit1 kernel: [504725.220033] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:38:54 linbit1 kernel: [504725.220044] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Restarting receiver thread
Mar 13 20:38:54 linbit1 kernel: [504725.220053] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:54.223819    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6xtsx\" (UniqueName: \"kubernetes.io/projected/f49ecb96-a6f0-48a0-b80c-53dd0806643d-kube-api-access-6xtsx\") pod \"fio-bench-r2-n8-4-vfll8\" (UID: \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\") " pod="default/fio-bench-r2-n8-4-vfll8"
Mar 13 20:38:54 linbit1 kernel: [504725.306375] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: rs_discard_granularity feature disabled
Mar 13 20:38:54 linbit1 kernel: [504725.373764] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: rs_discard_granularity feature disabled
Mar 13 20:38:54 linbit1 kernel: [504725.745838] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:54 linbit1 kernel: [504725.745849] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:54 linbit1 kernel: [504725.745956] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:54 linbit1 kernel: [504725.791871] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Preparing remote state change 1875037161
Mar 13 20:38:54 linbit1 kernel: [504725.827915] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:38:54 linbit1 kernel: [504725.827924] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: peer 16216ECF09A0CD20:7F007D7488DE8B1B:0000000000000000:0000000000000000 bits:0 flags:1120
Mar 13 20:38:54 linbit1 kernel: [504725.829879] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Committing remote state change 1875037161 (primary_nodes=1)
Mar 13 20:38:54 linbit1 kernel: [504725.829894] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Primary )
Mar 13 20:38:54 linbit1 kernel: [504725.829901] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:38:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:55.756788    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:38:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:55.838631    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hczfq\" (UniqueName: \"kubernetes.io/projected/f3b42727-fd59-4af5-a4e2-dc97ffd00b2d-kube-api-access-hczfq\") pod \"fio-bench-r2-n8-1-5nnlm\" (UID: \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\") " pod="default/fio-bench-r2-n8-1-5nnlm"
Mar 13 20:38:56 linbit1 kernel: [504727.056518] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Preparing remote state change 299514945
Mar 13 20:38:56 linbit1 kernel: [504727.056855] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Committing remote state change 299514945 (primary_nodes=1)
Mar 13 20:38:56 linbit1 kernel: [504727.056871] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: peer( Secondary -> Primary )
Mar 13 20:38:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:56.070210    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:38:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:38:56.141429    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-4bvsj\" (UniqueName: \"kubernetes.io/projected/cc6de1a1-21c4-4733-838a-eb8fcbbe552d-kube-api-access-4bvsj\") pod \"fio-bench-r2-n8-3-8hwhw\" (UID: \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\") " pod="default/fio-bench-r2-n8-3-8hwhw"
Mar 13 20:38:57 linbit1 systemd-udevd[2453667]: dm-44: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-44' failed with exit code 1.
Mar 13 20:38:57 linbit1 systemd-udevd[2453666]: dm-45: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-45' failed with exit code 1.
Mar 13 20:38:57 linbit1 kernel: [504728.351958] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Starting worker thread (from drbdsetup [2453723])
Mar 13 20:38:57 linbit1 kernel: [504728.360071] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: meta-data IO uses: blk-bio
Mar 13 20:38:57 linbit1 kernel: [504728.360131] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: rs_discard_granularity feature disabled
Mar 13 20:38:57 linbit1 kernel: [504728.360157] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: disk( Diskless -> Attaching )
Mar 13 20:38:57 linbit1 kernel: [504728.360167] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: Maximum number of peer devices = 7
Mar 13 20:38:57 linbit1 kernel: [504728.361102] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Method to ensure write ordering: drain
Mar 13 20:38:57 linbit1 kernel: [504728.361111] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: drbd_bm_resize called with capacity == 20971520
Mar 13 20:38:57 linbit1 kernel: [504728.367507] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:38:57 linbit1 kernel: [504728.367515] drbd1034: detected capacity change from 0 to 20971520
Mar 13 20:38:57 linbit1 kernel: [504728.367518] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: size = 10 GB (10485760 KB)
Mar 13 20:38:57 linbit1 systemd-udevd[2453666]: drbd1034: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1034' failed with exit code 1.
Mar 13 20:38:57 linbit1 kernel: [504728.372532] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: recounting of set bits took additional 4ms
Mar 13 20:38:57 linbit1 kernel: [504728.372544] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: disk( Attaching -> Inconsistent )
Mar 13 20:38:57 linbit1 kernel: [504728.372546] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: attached to current UUID: 0000000000000004
Mar 13 20:38:57 linbit1 kernel: [504728.409826] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Preparing remote state change 3111158456
Mar 13 20:38:57 linbit1 kernel: [504728.421831] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Preparing remote state change 3584682473
Mar 13 20:38:57 linbit1 kernel: [504728.454901] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Committing remote state change 3111158456 (primary_nodes=1)
Mar 13 20:38:57 linbit1 kernel: [504728.474866] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Committing remote state change 3584682473 (primary_nodes=1)
Mar 13 20:38:58 linbit1 kernel: [504729.726675] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: rs_discard_granularity feature disabled
Mar 13 20:38:58 linbit1 kernel: [504729.782142] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 291161871 (0->-1 7683/4609)
Mar 13 20:38:58 linbit1 kernel: [504729.782153] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 291161871 (0ms)
Mar 13 20:38:58 linbit1 kernel: [504729.782160] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: role( Secondary -> Primary )
Mar 13 20:38:58 linbit1 kernel: [504729.782164] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: disk( Inconsistent -> UpToDate )
Mar 13 20:38:58 linbit1 kernel: [504729.782249] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: size = 10 GB (10485760 KB)
Mar 13 20:38:58 linbit1 kernel: [504729.782357] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Forced to consider local data as UpToDate!
Mar 13 20:38:58 linbit1 kernel: [504729.782420] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: new current UUID: 128417CE8BF51BA9 weak: FFFFFFFFFFFFFFFE
Mar 13 20:38:58 linbit1 kernel: [504729.792369] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 2667841959 (0->-1 3/2)
Mar 13 20:38:58 linbit1 kernel: [504729.792374] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 2667841959 (0ms)
Mar 13 20:38:58 linbit1 kernel: [504729.792378] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: role( Primary -> Secondary )
Mar 13 20:38:58 linbit1 kernel: [504729.802781] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 1035994746 (0->-1 3/1)
Mar 13 20:38:58 linbit1 kernel: [504729.802790] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 1035994746 (0ms)
Mar 13 20:38:58 linbit1 kernel: [504729.802798] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: role( Secondary -> Primary )
Mar 13 20:38:58 linbit1 kernel: [504729.811450] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 1283243275 (0->-1 3/2)
Mar 13 20:38:58 linbit1 kernel: [504729.811460] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 1283243275 (0ms)
Mar 13 20:38:58 linbit1 kernel: [504729.811467] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: role( Primary -> Secondary )
Mar 13 20:38:58 linbit1 kernel: [504729.833963] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a: Starting worker thread (from drbdsetup [2453889])
Mar 13 20:38:58 linbit1 kernel: [504729.838194] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Starting sender thread (from drbdsetup [2453894])
Mar 13 20:38:58 linbit1 kernel: [504729.839908] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Starting sender thread (from drbdsetup [2453897])
Mar 13 20:38:58 linbit1 kernel: [504729.844135] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:38:58 linbit1 kernel: [504729.844207] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Starting receiver thread (from drbd_w_pvc-0f95 [2453890])
Mar 13 20:38:58 linbit1 kernel: [504729.844344] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: conn( Unconnected -> Connecting )
Mar 13 20:38:58 linbit1 kernel: [504729.845607] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:38:58 linbit1 kernel: [504729.845644] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Starting receiver thread (from drbd_w_pvc-0f95 [2453890])
Mar 13 20:38:58 linbit1 kernel: [504729.845755] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Unconnected -> Connecting )
Mar 13 20:38:58 linbit1 systemd-udevd[2453666]: drbd1031: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1031' failed with exit code 1.
Mar 13 20:38:58 linbit1 kernel: [504729.871705] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: rs_discard_granularity feature disabled
Mar 13 20:38:58 linbit1 kernel: [504729.954268] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: rs_discard_granularity feature disabled
Mar 13 20:38:58 linbit1 kernel: [504730.014230] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: rs_discard_granularity feature disabled
Mar 13 20:38:59 linbit1 kernel: [504730.073707] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: rs_discard_granularity feature disabled
Mar 13 20:38:59 linbit1 kernel: [504730.371957] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:38:59 linbit1 kernel: [504730.371968] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:38:59 linbit1 kernel: [504730.372165] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:38:59 linbit1 kernel: [504730.409858] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Preparing remote state change 1994677593
Mar 13 20:38:59 linbit1 kernel: [504730.459856] drbd1031: detected capacity change from 0 to 20971520
Mar 13 20:38:59 linbit1 kernel: [504730.459864] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031: size = 10 GB (10485760 KB)
Mar 13 20:38:59 linbit1 kernel: [504730.459909] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:38:59 linbit1 kernel: [504730.459914] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit2: peer AE7361DB2B2FCAB8:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:38:59 linbit1 kernel: [504730.460080] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Committing remote state change 1994677593 (primary_nodes=0)
Mar 13 20:38:59 linbit1 kernel: [504730.460099] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:38:59 linbit1 kernel: [504730.460105] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:38:59 linbit1 kernel: [504730.614249] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Preparing remote state change 1298690569
Mar 13 20:38:59 linbit1 kernel: [504730.614546] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Committing remote state change 1298690569 (primary_nodes=1)
Mar 13 20:38:59 linbit1 kernel: [504730.614559] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: peer( Secondary -> Primary )
Mar 13 20:39:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:00.056846    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:39:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:00.174178    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cltvg\" (UniqueName: \"kubernetes.io/projected/23c28465-eb55-4375-9bd1-5fd757745f89-kube-api-access-cltvg\") pod \"fio-bench-r2-n3-0-zq6bl\" (UID: \"23c28465-eb55-4375-9bd1-5fd757745f89\") " pod="default/fio-bench-r2-n3-0-zq6bl"
Mar 13 20:39:01 linbit1 kernel: [504732.144534] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: rs_discard_granularity feature disabled
Mar 13 20:39:02 linbit1 kernel: [504733.027974] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:39:02 linbit1 kernel: [504733.027986] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:02 linbit1 kernel: [504733.028117] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:02 linbit1 kernel: [504733.059848] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Preparing remote state change 1936270084
Mar 13 20:39:02 linbit1 kernel: [504733.096005] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:39:02 linbit1 kernel: [504733.096018] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: peer 0000000000000004:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:39:02 linbit1 kernel: [504733.114770] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Committing remote state change 1936270084 (primary_nodes=0)
Mar 13 20:39:02 linbit1 kernel: [504733.114795] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:02 linbit1 kernel: [504733.114800] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031: quorum( no -> yes )
Mar 13 20:39:02 linbit1 kernel: [504733.114807] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established )
Mar 13 20:39:02 linbit1 kernel: [504733.115173] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Preparing remote state change 67688683
Mar 13 20:39:02 linbit1 kernel: [504733.157618] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Committing remote state change 67688683 (primary_nodes=0)
Mar 13 20:39:02 linbit1 kernel: [504733.161247] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: resync-susp( no -> peer )
Mar 13 20:39:02 linbit1 kernel: [504733.382580] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Starting sender thread (from drbdsetup [2454239])
Mar 13 20:39:02 linbit1 kernel: [504733.384242] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Starting sender thread (from drbdsetup [2454241])
Mar 13 20:39:02 linbit1 kernel: [504733.421752] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: rs_discard_granularity feature disabled
Mar 13 20:39:02 linbit1 kernel: [504733.461629] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:39:02 linbit1 kernel: [504733.461638] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: quorum( yes -> no )
Mar 13 20:39:02 linbit1 kernel: [504733.461778] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Starting receiver thread (from drbd_w_pvc-0845 [2453724])
Mar 13 20:39:02 linbit1 kernel: [504733.461937] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:39:02 linbit1 kernel: [504733.463069] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:39:02 linbit1 kernel: [504733.463116] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Starting receiver thread (from drbd_w_pvc-0845 [2453724])
Mar 13 20:39:02 linbit1 kernel: [504733.463256] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:39:02 linbit1 systemd-udevd[2453666]: dm-46: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-46' failed with exit code 1.
Mar 13 20:39:02 linbit1 systemd-udevd[2453666]: dm-47: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-47' failed with exit code 1.
Mar 13 20:39:02 linbit1 kernel: [504733.768580] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Starting worker thread (from drbdsetup [2454363])
Mar 13 20:39:02 linbit1 kernel: [504733.772765] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Starting sender thread (from drbdsetup [2454370])
Mar 13 20:39:02 linbit1 kernel: [504733.774656] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Starting sender thread (from drbdsetup [2454373])
Mar 13 20:39:02 linbit1 systemd-udevd[2453666]: drbd1033: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1033' failed with exit code 1.
Mar 13 20:39:02 linbit1 kernel: [504733.829045] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: meta-data IO uses: blk-bio
Mar 13 20:39:02 linbit1 kernel: [504733.829112] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: rs_discard_granularity feature disabled
Mar 13 20:39:02 linbit1 kernel: [504733.829171] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( Diskless -> Attaching )
Mar 13 20:39:02 linbit1 kernel: [504733.829182] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: Maximum number of peer devices = 7
Mar 13 20:39:02 linbit1 kernel: [504733.830116] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Method to ensure write ordering: drain
Mar 13 20:39:02 linbit1 kernel: [504733.830125] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: drbd_bm_resize called with capacity == 20971520
Mar 13 20:39:02 linbit1 kernel: [504733.836565] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: resync bitmap: bits=2621440 words=286720 pages=560
Mar 13 20:39:02 linbit1 kernel: [504733.836572] drbd1033: detected capacity change from 0 to 20971520
Mar 13 20:39:02 linbit1 kernel: [504733.836575] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: size = 10 GB (10485760 KB)
Mar 13 20:39:02 linbit1 kernel: [504733.844905] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: recounting of set bits took additional 8ms
Mar 13 20:39:02 linbit1 kernel: [504733.844924] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( Attaching -> Inconsistent )
Mar 13 20:39:02 linbit1 kernel: [504733.844929] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: attached to current UUID: 0000000000000004
Mar 13 20:39:02 linbit1 kernel: [504733.846547] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:39:02 linbit1 kernel: [504733.846762] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Starting receiver thread (from drbd_w_pvc-329d [2454364])
Mar 13 20:39:02 linbit1 kernel: [504733.846876] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( Unconnected -> Connecting )
Mar 13 20:39:02 linbit1 kernel: [504733.847769] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:39:02 linbit1 kernel: [504733.847873] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Starting receiver thread (from drbd_w_pvc-329d [2454364])
Mar 13 20:39:02 linbit1 kernel: [504733.848006] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Unconnected -> Connecting )
Mar 13 20:39:02 linbit1 kernel: [504733.874370] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: rs_discard_granularity feature disabled
Mar 13 20:39:04 linbit1 kernel: [504735.249952] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:39:04 linbit1 kernel: [504735.249964] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:04 linbit1 kernel: [504735.250062] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:04 linbit1 kernel: [504735.345894] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:39:04 linbit1 kernel: [504735.345904] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:04 linbit1 kernel: [504735.346241] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:04 linbit1 kernel: [504735.431894] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 3573181939 (0->1 499/146)
Mar 13 20:39:04 linbit1 kernel: [504735.445816] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Preparing remote state change 3093148230
Mar 13 20:39:04 linbit1 kernel: [504735.456009] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: drbd_sync_handshake:
Mar 13 20:39:04 linbit1 kernel: [504735.456017] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: self 128417CE8BF51BA8:05201496CD5E36C5:0000000000000000:0000000000000000 bits:0 flags:20
Mar 13 20:39:04 linbit1 kernel: [504735.456027] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: peer 0000000000000004:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:39:04 linbit1 kernel: [504735.456035] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: uuid_compare()=source-set-bitmap by rule=just-created-peer
Mar 13 20:39:04 linbit1 kernel: [504735.456040] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: Setting and writing one bitmap slot, after drbd_sync_handshake
Mar 13 20:39:04 linbit1 kernel: [504735.471907] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: drbd_sync_handshake:
Mar 13 20:39:04 linbit1 kernel: [504735.471915] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: self 0000000000000004:0000000000000000:EB77D5EEDF720B64:0000000000000000 bits:0 flags:24
Mar 13 20:39:04 linbit1 kernel: [504735.471925] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: peer C5D73FB57C5CBE18:EB77D5EEDF720B64:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:39:04 linbit1 kernel: [504735.471933] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: uuid_compare()=target-set-bitmap by rule=just-created-self
Mar 13 20:39:04 linbit1 kernel: [504735.471939] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: Setting and writing the whole bitmap, fresh node
Mar 13 20:39:04 linbit1 kernel: [504735.472828] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: State change 3573181939: primary_nodes=0, weak_nodes=0
Mar 13 20:39:04 linbit1 kernel: [504735.472842] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 3573181939 (40ms)
Mar 13 20:39:04 linbit1 kernel: [504735.472898] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:04 linbit1 kernel: [504735.472904] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: quorum( no -> yes )
Mar 13 20:39:04 linbit1 kernel: [504735.472910] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> WFBitMapS )
Mar 13 20:39:04 linbit1 kernel: [504735.474489] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:39:04 linbit1 kernel: [504735.477598] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:39:04 linbit1 kernel: [504735.477635] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: repl( WFBitMapS -> SyncSource )
Mar 13 20:39:04 linbit1 kernel: [504735.477942] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: Began resync as SyncSource (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:39:04 linbit1 kernel: [504735.493012] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Committing remote state change 3093148230 (primary_nodes=0)
Mar 13 20:39:04 linbit1 kernel: [504735.493037] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:04 linbit1 kernel: [504735.493043] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: quorum( no -> yes )
Mar 13 20:39:04 linbit1 kernel: [504735.493048] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:39:04 linbit1 kernel: [504735.493396] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: size = 10 GB (10485760 KB)
Mar 13 20:39:04 linbit1 kernel: [504735.495515] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:39:04 linbit1 kernel: [504735.496447] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 23(1), total 23; compression: 100.0%
Mar 13 20:39:04 linbit1 kernel: [504735.496455] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: setting UUIDs to EB77D5EEDF720B64:0000000000000000:EB77D5EEDF720B64:0000000000000000
Mar 13 20:39:04 linbit1 kernel: [504735.496475] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit3: resync-susp( no -> connection dependency )
Mar 13 20:39:04 linbit1 kernel: [504735.496478] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: repl( WFBitMapT -> SyncTarget )
Mar 13 20:39:04 linbit1 kernel: [504735.496825] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: Began resync as SyncTarget (will sync 10485760 KB [2621440 bits set]).
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.016000    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\") pod \"fio-bench-r2-n8-3-8hwhw\" (UID: \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\") " pod="default/fio-bench-r2-n8-3-8hwhw"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.029773    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\") pod \"fio-bench-r2-n8-3-8hwhw\" (UID: \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\") device path: \"\"" pod="default/fio-bench-r2-n8-3-8hwhw"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.117058    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\") pod \"fio-bench-r2-n8-3-8hwhw\" (UID: \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\") DevicePath \"\"" pod="default/fio-bench-r2-n8-3-8hwhw"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.122030    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\") pod \"fio-bench-r2-n8-3-8hwhw\" (UID: \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\") DevicePath \"csi-e54556835af685a8d1001268f5c406fccf8e08371006015218936816e2528e78\"" pod="default/fio-bench-r2-n8-3-8hwhw"
Mar 13 20:39:05 linbit1 kernel: [504736.142859] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Preparing remote state change 1426077543
Mar 13 20:39:05 linbit1 kernel: [504736.143180] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Committing remote state change 1426077543 (primary_nodes=1)
Mar 13 20:39:05 linbit1 kernel: [504736.143196] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: peer( Secondary -> Primary )
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.128087    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.217259    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-942882e3-6d43-415f-8f08-18cb1792333b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b\") pod \"fio-bench-r2-n8-1-5nnlm\" (UID: \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\") " pod="default/fio-bench-r2-n8-1-5nnlm"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.217349    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\") pod \"fio-bench-r2-n8-4-vfll8\" (UID: \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\") " pod="default/fio-bench-r2-n8-4-vfll8"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.232176    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-942882e3-6d43-415f-8f08-18cb1792333b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b\") pod \"fio-bench-r2-n8-1-5nnlm\" (UID: \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\") device path: \"\"" pod="default/fio-bench-r2-n8-1-5nnlm"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.234589    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\") pod \"fio-bench-r2-n8-4-vfll8\" (UID: \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\") device path: \"\"" pod="default/fio-bench-r2-n8-4-vfll8"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.319004    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-942882e3-6d43-415f-8f08-18cb1792333b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b\") pod \"fio-bench-r2-n8-1-5nnlm\" (UID: \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\") DevicePath \"\"" pod="default/fio-bench-r2-n8-1-5nnlm"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.319303    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\") pod \"fio-bench-r2-n8-4-vfll8\" (UID: \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\") DevicePath \"\"" pod="default/fio-bench-r2-n8-4-vfll8"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.324422    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-942882e3-6d43-415f-8f08-18cb1792333b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b\") pod \"fio-bench-r2-n8-1-5nnlm\" (UID: \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\") DevicePath \"csi-02e45a11a63ee2ef001a33b7967585144a12d345e31f8d0142a5f8a586627af0\"" pod="default/fio-bench-r2-n8-1-5nnlm"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.326807    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\") pod \"fio-bench-r2-n8-4-vfll8\" (UID: \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\") DevicePath \"csi-e4fdc6bf22a68ad78b5ec5133b5a6b05905b5a0a68a72f60c81dc272cb19b598\"" pod="default/fio-bench-r2-n8-4-vfll8"
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.426677    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:39:05 linbit1 kernel: [504736.515811] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 3083890169 (0->-1 3/1)
Mar 13 20:39:05 linbit1 kernel: [504736.516038] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: State change 3083890169: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:39:05 linbit1 kernel: [504736.516045] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 3083890169 (4ms)
Mar 13 20:39:05 linbit1 kernel: [504736.516055] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: role( Secondary -> Primary )
Mar 13 20:39:05 linbit1 kernel: [504736.516146] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Preparing remote state change 3145474557
Mar 13 20:39:05 linbit1 kernel: [504736.516276] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Preparing remote state change 3490353522
Mar 13 20:39:05 linbit1 kernel: [504736.516437] loop21: detected capacity change from 0 to 20971520
Mar 13 20:39:05 linbit1 kernel: [504736.516465] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Committing remote state change 3145474557 (primary_nodes=1)
Mar 13 20:39:05 linbit1 kernel: [504736.516476] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: peer( Secondary -> Primary )
Mar 13 20:39:05 linbit1 kernel: [504736.516545] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Committing remote state change 3490353522 (primary_nodes=1)
Mar 13 20:39:05 linbit1 kernel: [504736.516556] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: peer( Secondary -> Primary )
Mar 13 20:39:05 linbit1 kernel: [504736.518113] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c: Starting worker thread (from drbdsetup [2454586])
Mar 13 20:39:05 linbit1 kernel: [504736.522309] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Starting sender thread (from drbdsetup [2454591])
Mar 13 20:39:05 linbit1 kernel: [504736.524106] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Starting sender thread (from drbdsetup [2454593])
Mar 13 20:39:05 linbit1 kernel: [504736.528862] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:39:05 linbit1 kernel: [504736.528923] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Starting receiver thread (from drbd_w_pvc-2262 [2454587])
Mar 13 20:39:05 linbit1 kernel: [504736.529094] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: conn( Unconnected -> Connecting )
Mar 13 20:39:05 linbit1 kernel: [504736.530405] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:39:05 linbit1 kernel: [504736.530439] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Starting receiver thread (from drbd_w_pvc-2262 [2454587])
Mar 13 20:39:05 linbit1 kernel: [504736.530596] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( Unconnected -> Connecting )
Mar 13 20:39:05 linbit1 kernel: [504736.570744] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7: Starting worker thread (from drbdsetup [2454613])
Mar 13 20:39:05 linbit1 systemd-udevd[2454609]: drbd1035: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1035' failed with exit code 1.
Mar 13 20:39:05 linbit1 kernel: [504736.574877] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Starting sender thread (from drbdsetup [2454620])
Mar 13 20:39:05 linbit1 kernel: [504736.576722] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Starting sender thread (from drbdsetup [2454622])
Mar 13 20:39:05 linbit1 kernel: [504736.581782] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: conn( StandAlone -> Unconnected )
Mar 13 20:39:05 linbit1 kernel: [504736.581856] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Starting receiver thread (from drbd_w_pvc-d2f1 [2454615])
Mar 13 20:39:05 linbit1 kernel: [504736.582040] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:39:05 linbit1 kernel: [504736.583036] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( StandAlone -> Unconnected )
Mar 13 20:39:05 linbit1 kernel: [504736.583078] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Starting receiver thread (from drbd_w_pvc-d2f1 [2454615])
Mar 13 20:39:05 linbit1 kernel: [504736.583233] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:39:05 linbit1 systemd-udevd[2454619]: drbd1032: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/drbd1032' failed with exit code 1.
Mar 13 20:39:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:05.624119    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:39:05 linbit1 kernel: [504736.867015] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 1313902424 (0->-1 3/1)
Mar 13 20:39:05 linbit1 kernel: [504736.867197] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change 1313902424: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:39:05 linbit1 kernel: [504736.867201] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 1313902424 (0ms)
Mar 13 20:39:05 linbit1 kernel: [504736.867208] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: role( Secondary -> Primary )
Mar 13 20:39:05 linbit1 kernel: [504736.867505] loop22: detected capacity change from 0 to 20971520
Mar 13 20:39:06 linbit1 kernel: [504737.274777] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:39:06 linbit1 kernel: [504737.274789] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:06 linbit1 kernel: [504737.274899] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:06 linbit1 kernel: [504737.276509] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 4248243823 (0->-1 3/1)
Mar 13 20:39:06 linbit1 kernel: [504737.276685] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: State change 4248243823: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:39:06 linbit1 kernel: [504737.276689] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 4248243823 (0ms)
Mar 13 20:39:06 linbit1 kernel: [504737.276695] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: role( Secondary -> Primary )
Mar 13 20:39:06 linbit1 kernel: [504737.277005] loop23: detected capacity change from 0 to 20971520
Mar 13 20:39:06 linbit1 kernel: [504737.306743] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:39:06 linbit1 kernel: [504737.306752] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:06 linbit1 kernel: [504737.306914] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.314757182Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n8-3-8hwhw,Uid:cc6de1a1-21c4-4733-838a-eb8fcbbe552d,Namespace:default,Attempt:0,}"
Mar 13 20:39:06 linbit1 kernel: [504737.367876] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Preparing cluster-wide state change 1607676465 (1->2 499/146)
Mar 13 20:39:06 linbit1 kernel: [504737.375873] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 578209191 (0->2 499/146)
Mar 13 20:39:06 linbit1 kernel: [504737.395858] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit3: self C5D73FB57C5CBE18:EB77D5EEDF720B64:EB77D5EEDF720B64:0000000000000000 bits:0 flags:0
Mar 13 20:39:06 linbit1 kernel: [504737.395873] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:39:06 linbit1 kernel: [504737.406757] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: State change 1607676465: primary_nodes=0, weak_nodes=0
Mar 13 20:39:06 linbit1 kernel: [504737.406766] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Committing cluster-wide state change 1607676465 (36ms)
Mar 13 20:39:06 linbit1 kernel: [504737.406828] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:06 linbit1 kernel: [504737.406834] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:39:06 linbit1 kernel: [504737.415860] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit3: self 128417CE8BF51BA8:05201496CD5E36C5:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:39:06 linbit1 kernel: [504737.415872] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:39:06 linbit1 kernel: [504737.415886] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: State change 578209191: primary_nodes=0, weak_nodes=0
Mar 13 20:39:06 linbit1 kernel: [504737.415893] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 578209191 (40ms)
Mar 13 20:39:06 linbit1 kernel: [504737.415938] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:06 linbit1 kernel: [504737.415944] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:39:06 linbit1 kernel: [504737.429834] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Preparing remote state change 1861359466
Mar 13 20:39:06 linbit1 kernel: [504737.462956] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Committing remote state change 1861359466 (primary_nodes=0)
Mar 13 20:39:06 linbit1 kernel: [504737.493819] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Preparing remote state change 2237992366
Mar 13 20:39:06 linbit1 kernel: [504737.533844] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Committing remote state change 2237992366 (primary_nodes=0)
Mar 13 20:39:06 linbit1 kernel: [504737.577757] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:39:06 linbit1 kernel: [504737.577919] IPv6: ADDRCONF(NETDEV_CHANGE): caliead4891e62b: link becomes ready
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.601776490Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n8-1-5nnlm,Uid:f3b42727-fd59-4af5-a4e2-dc97ffd00b2d,Namespace:default,Attempt:0,}"
Mar 13 20:39:06 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 105 seen, reloading interface list
Mar 13 20:39:06 linbit1 systemd-networkd[2297221]: caliead4891e62b: Link UP
Mar 13 20:39:06 linbit1 systemd-networkd[2297221]: caliead4891e62b: Gained carrier
Mar 13 20:39:06 linbit1 systemd-udevd[2454609]: Using default interface naming scheme 'v249'.
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.348 [INFO][2454699] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.384 [INFO][2454699] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0 fio-bench-r2-n8-3- default  cc6de1a1-21c4-4733-838a-eb8fcbbe552d 17525727 0 2023-03-13 20:37:28 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:7472d986-389c-4adf-9fe6-61f15b8567f6 job-name:fio-bench-r2-n8-3 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n8-3-8hwhw eth0 default [] []   [kns.default ksa.default.default] caliead4891e62b  [] []}} ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Namespace="default" Pod="fio-bench-r2-n8-3-8hwhw" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.384 [INFO][2454699] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Namespace="default" Pod="fio-bench-r2-n8-3-8hwhw" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.426 [INFO][2454731] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.455 [INFO][2454731] ipam_plugin.go 264: Auto assigning IP ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x40005b9920), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n8-3-8hwhw", "timestamp":"2023-03-13 20:39:06.426854121 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.455 [INFO][2454731] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.472 [INFO][2454731] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.484 [INFO][2454731] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.497 [INFO][2454731] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.502 [INFO][2454731] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.511 [INFO][2454731] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.511 [INFO][2454731] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.518 [INFO][2454731] ipam.go 1680: Creating new handle: k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.530 [INFO][2454731] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.554 [INFO][2454731] ipam.go 1214: Successfully claimed IPs: [10.1.217.15/26] block=10.1.217.0/26 handle="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.554 [INFO][2454731] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.15/26] handle="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.555 [INFO][2454731] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.15/26] IPv6=[] ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.559 [INFO][2454699] k8s.go 383: Populated endpoint ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Namespace="default" Pod="fio-bench-r2-n8-3-8hwhw" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0", GenerateName:"fio-bench-r2-n8-3-", Namespace:"default", SelfLink:"", UID:"cc6de1a1-21c4-4733-838a-eb8fcbbe552d", ResourceVersion:"17525727", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"7472d986-389c-4adf-9fe6-61f15b8567f6", "job-name":"fio-bench-r2-n8-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n8-3-8hwhw", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.15/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"caliead4891e62b", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.559 [INFO][2454699] k8s.go 384: Calico CNI using IPs: [10.1.217.15/32] ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Namespace="default" Pod="fio-bench-r2-n8-3-8hwhw" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.559 [INFO][2454699] dataplane_linux.go 68: Setting the host side veth name to caliead4891e62b ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Namespace="default" Pod="fio-bench-r2-n8-3-8hwhw" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.562 [INFO][2454699] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Namespace="default" Pod="fio-bench-r2-n8-3-8hwhw" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.609 [INFO][2454699] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Namespace="default" Pod="fio-bench-r2-n8-3-8hwhw" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0", GenerateName:"fio-bench-r2-n8-3-", Namespace:"default", SelfLink:"", UID:"cc6de1a1-21c4-4733-838a-eb8fcbbe552d", ResourceVersion:"17525727", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"7472d986-389c-4adf-9fe6-61f15b8567f6", "job-name":"fio-bench-r2-n8-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba", Pod:"fio-bench-r2-n8-3-8hwhw", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.15/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"caliead4891e62b", MAC:"e6:58:bf:8c:55:dc", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.630 [INFO][2454699] k8s.go 489: Wrote updated endpoint to datastore ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Namespace="default" Pod="fio-bench-r2-n8-3-8hwhw" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.661760271Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.661878471Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.661902952Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.662185392Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba pid=2454814 runtime=io.containerd.runc.v2
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.717968881Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n8-4-vfll8,Uid:f49ecb96-a6f0-48a0-b80c-53dd0806643d,Namespace:default,Attempt:0,}"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.774817693Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n8-3-8hwhw,Uid:cc6de1a1-21c4-4733-838a-eb8fcbbe552d,Namespace:default,Attempt:0,} returns sandbox id \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\""
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.776519418Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:39:06 linbit1 kernel: [504737.864580] IPv6: ADDRCONF(NETDEV_CHANGE): cali929a2f4f008: link becomes ready
Mar 13 20:39:06 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 106 seen, reloading interface list
Mar 13 20:39:06 linbit1 systemd-networkd[2297221]: cali929a2f4f008: Link UP
Mar 13 20:39:06 linbit1 systemd-networkd[2297221]: cali929a2f4f008: Gained carrier
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.640 [INFO][2454765] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.668 [INFO][2454765] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0 fio-bench-r2-n8-1- default  f3b42727-fd59-4af5-a4e2-dc97ffd00b2d 17525697 0 2023-03-13 20:37:28 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:e0345956-1751-4eae-976d-8aa6a2fad7f1 job-name:fio-bench-r2-n8-1 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n8-1-5nnlm eth0 default [] []   [kns.default ksa.default.default] cali929a2f4f008  [] []}} ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Namespace="default" Pod="fio-bench-r2-n8-1-5nnlm" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.668 [INFO][2454765] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Namespace="default" Pod="fio-bench-r2-n8-1-5nnlm" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.717 [INFO][2454831] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.749 [INFO][2454831] ipam_plugin.go 264: Auto assigning IP ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x400078fc30), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n8-1-5nnlm", "timestamp":"2023-03-13 20:39:06.717978201 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.749 [INFO][2454831] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.763 [INFO][2454831] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.777 [INFO][2454831] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.792 [INFO][2454831] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.797 [INFO][2454831] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.805 [INFO][2454831] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.805 [INFO][2454831] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.810 [INFO][2454831] ipam.go 1680: Creating new handle: k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.819 [INFO][2454831] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.841 [INFO][2454831] ipam.go 1214: Successfully claimed IPs: [10.1.217.38/26] block=10.1.217.0/26 handle="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.841 [INFO][2454831] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.38/26] handle="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" host="linbit1"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.841 [INFO][2454831] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.38/26] IPv6=[] ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.845 [INFO][2454765] k8s.go 383: Populated endpoint ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Namespace="default" Pod="fio-bench-r2-n8-1-5nnlm" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0", GenerateName:"fio-bench-r2-n8-1-", Namespace:"default", SelfLink:"", UID:"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d", ResourceVersion:"17525697", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"e0345956-1751-4eae-976d-8aa6a2fad7f1", "job-name":"fio-bench-r2-n8-1", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n8-1-5nnlm", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.38/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali929a2f4f008", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.846 [INFO][2454765] k8s.go 384: Calico CNI using IPs: [10.1.217.38/32] ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Namespace="default" Pod="fio-bench-r2-n8-1-5nnlm" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.846 [INFO][2454765] dataplane_linux.go 68: Setting the host side veth name to cali929a2f4f008 ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Namespace="default" Pod="fio-bench-r2-n8-1-5nnlm" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.848 [INFO][2454765] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Namespace="default" Pod="fio-bench-r2-n8-1-5nnlm" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.893 [INFO][2454765] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Namespace="default" Pod="fio-bench-r2-n8-1-5nnlm" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0", GenerateName:"fio-bench-r2-n8-1-", Namespace:"default", SelfLink:"", UID:"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d", ResourceVersion:"17525697", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"e0345956-1751-4eae-976d-8aa6a2fad7f1", "job-name":"fio-bench-r2-n8-1", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7", Pod:"fio-bench-r2-n8-1-5nnlm", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.38/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali929a2f4f008", MAC:"1e:5e:da:f6:6f:dd", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.923 [INFO][2454765] k8s.go 489: Wrote updated endpoint to datastore ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Namespace="default" Pod="fio-bench-r2-n8-1-5nnlm" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.958613209Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.958733529Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.958760289Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:39:06 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06.959013250Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7 pid=2454963 runtime=io.containerd.runc.v2
Mar 13 20:39:06 linbit1 kernel: [504737.994261] IPv6: ADDRCONF(NETDEV_CHANGE): cali375d3b08313: link becomes ready
Mar 13 20:39:07 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 107 seen, reloading interface list
Mar 13 20:39:07 linbit1 systemd-networkd[2297221]: cali375d3b08313: Link UP
Mar 13 20:39:07 linbit1 systemd-networkd[2297221]: cali375d3b08313: Gained carrier
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.749 [INFO][2454867] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.780 [INFO][2454867] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0 fio-bench-r2-n8-4- default  f49ecb96-a6f0-48a0-b80c-53dd0806643d 17525642 0 2023-03-13 20:37:28 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:04b97d17-b88b-4792-b1e9-1c664445a4a6 job-name:fio-bench-r2-n8-4 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n8-4-vfll8 eth0 default [] []   [kns.default ksa.default.default] cali375d3b08313  [] []}} ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Namespace="default" Pod="fio-bench-r2-n8-4-vfll8" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.780 [INFO][2454867] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Namespace="default" Pod="fio-bench-r2-n8-4-vfll8" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.832 [INFO][2454909] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.862 [INFO][2454909] ipam_plugin.go 264: Auto assigning IP ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x400003a540), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n8-4-vfll8", "timestamp":"2023-03-13 20:39:06.832220707 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.862 [INFO][2454909] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.871 [INFO][2454909] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.890 [INFO][2454909] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.904 [INFO][2454909] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.912 [INFO][2454909] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.923 [INFO][2454909] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.923 [INFO][2454909] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.934 [INFO][2454909] ipam.go 1680: Creating new handle: k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.951 [INFO][2454909] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.971 [INFO][2454909] ipam.go 1214: Successfully claimed IPs: [10.1.217.13/26] block=10.1.217.0/26 handle="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.971 [INFO][2454909] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.13/26] handle="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" host="linbit1"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:06Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.971 [INFO][2454909] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.13/26] IPv6=[] ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.975 [INFO][2454867] k8s.go 383: Populated endpoint ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Namespace="default" Pod="fio-bench-r2-n8-4-vfll8" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0", GenerateName:"fio-bench-r2-n8-4-", Namespace:"default", SelfLink:"", UID:"f49ecb96-a6f0-48a0-b80c-53dd0806643d", ResourceVersion:"17525642", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"04b97d17-b88b-4792-b1e9-1c664445a4a6", "job-name":"fio-bench-r2-n8-4", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n8-4-vfll8", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.13/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali375d3b08313", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.976 [INFO][2454867] k8s.go 384: Calico CNI using IPs: [10.1.217.13/32] ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Namespace="default" Pod="fio-bench-r2-n8-4-vfll8" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.976 [INFO][2454867] dataplane_linux.go 68: Setting the host side veth name to cali375d3b08313 ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Namespace="default" Pod="fio-bench-r2-n8-4-vfll8" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:06.978 [INFO][2454867] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Namespace="default" Pod="fio-bench-r2-n8-4-vfll8" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:07.045 [INFO][2454867] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Namespace="default" Pod="fio-bench-r2-n8-4-vfll8" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0", GenerateName:"fio-bench-r2-n8-4-", Namespace:"default", SelfLink:"", UID:"f49ecb96-a6f0-48a0-b80c-53dd0806643d", ResourceVersion:"17525642", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 28, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"04b97d17-b88b-4792-b1e9-1c664445a4a6", "job-name":"fio-bench-r2-n8-4", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc", Pod:"fio-bench-r2-n8-4-vfll8", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.13/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali375d3b08313", MAC:"ee:90:ec:4a:68:27", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:07.060 [INFO][2454867] k8s.go 489: Wrote updated endpoint to datastore ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Namespace="default" Pod="fio-bench-r2-n8-4-vfll8" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.086277195Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n8-1-5nnlm,Uid:f3b42727-fd59-4af5-a4e2-dc97ffd00b2d,Namespace:default,Attempt:0,} returns sandbox id \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\""
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.090371447Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.090433327Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.090443768Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.090654568Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc pid=2455028 runtime=io.containerd.runc.v2
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.151421432Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n8-4-vfll8,Uid:f49ecb96-a6f0-48a0-b80c-53dd0806643d,Namespace:default,Attempt:0,} returns sandbox id \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\""
Mar 13 20:39:07 linbit1 kernel: [504738.220815] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: rs_discard_granularity feature disabled
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.402200750Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.404378997Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.408544689Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.412753142Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.415553151Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.416188073Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.418848961Z" level=info msg="CreateContainer within sandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.426515904Z" level=info msg="CreateContainer within sandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd\""
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.427056265Z" level=info msg="StartContainer for \"2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd\""
Mar 13 20:39:07 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:07.521836752Z" level=info msg="StartContainer for \"2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd\" returns successfully"
Mar 13 20:39:07 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:07.852447    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\") pod \"fio-bench-r2-n3-0-zq6bl\" (UID: \"23c28465-eb55-4375-9bd1-5fd757745f89\") " pod="default/fio-bench-r2-n3-0-zq6bl"
Mar 13 20:39:07 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:07.862153    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\") pod \"fio-bench-r2-n3-0-zq6bl\" (UID: \"23c28465-eb55-4375-9bd1-5fd757745f89\") device path: \"\"" pod="default/fio-bench-r2-n3-0-zq6bl"
Mar 13 20:39:07 linbit1 kernel: [504738.883932] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:39:07 linbit1 kernel: [504738.883945] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:07 linbit1 kernel: [504738.884190] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:07 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:07.953671    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\") pod \"fio-bench-r2-n3-0-zq6bl\" (UID: \"23c28465-eb55-4375-9bd1-5fd757745f89\") DevicePath \"\"" pod="default/fio-bench-r2-n3-0-zq6bl"
Mar 13 20:39:07 linbit1 kernel: [504739.011943] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:39:07 linbit1 kernel: [504739.011955] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:07 linbit1 kernel: [504739.012108] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.004765933Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.012964397Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.015475965Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.016591568Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.017067650Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.019514017Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.020493020Z" level=info msg="CreateContainer within sandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:39:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:08.022408    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\") pod \"fio-bench-r2-n3-0-zq6bl\" (UID: \"23c28465-eb55-4375-9bd1-5fd757745f89\") DevicePath \"csi-56ea1a8b42d8e21e9aa4d14718c7fd76830441163c7032d0b12cb0a611113629\"" pod="default/fio-bench-r2-n3-0-zq6bl"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.027199040Z" level=info msg="CreateContainer within sandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65\""
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.027611802Z" level=info msg="StartContainer for \"dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65\""
Mar 13 20:39:08 linbit1 kernel: [504739.047916] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:39:08 linbit1 kernel: [504739.047926] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:08 linbit1 kernel: [504739.048081] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:08 linbit1 kernel: [504739.083946] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Preparing remote state change 814299493
Mar 13 20:39:08 linbit1 systemd-networkd[2297221]: cali929a2f4f008: Gained IPv6LL
Mar 13 20:39:08 linbit1 kernel: [504739.102673] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Preparing remote state change 2508897175
Mar 13 20:39:08 linbit1 kernel: [504739.115847] drbd1035: detected capacity change from 0 to 20971520
Mar 13 20:39:08 linbit1 kernel: [504739.115857] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035: size = 10 GB (10485760 KB)
Mar 13 20:39:08 linbit1 kernel: [504739.115918] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:39:08 linbit1 kernel: [504739.115924] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit2: peer 0000000000000004:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:24
Mar 13 20:39:08 linbit1 kernel: [504739.115944] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Aborting remote state change 814299493
Mar 13 20:39:08 linbit1 kernel: [504739.135828] drbd1032: detected capacity change from 0 to 20971520
Mar 13 20:39:08 linbit1 kernel: [504739.135835] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032: size = 10 GB (10485760 KB)
Mar 13 20:39:08 linbit1 kernel: [504739.135904] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:39:08 linbit1 kernel: [504739.135911] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit3: peer EF73B8240ED384C2:FFFFFFFFFFFFFFFF:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.125775538Z" level=info msg="StartContainer for \"dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65\" returns successfully"
Mar 13 20:39:08 linbit1 kernel: [504739.142716] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Committing remote state change 2508897175 (primary_nodes=0)
Mar 13 20:39:08 linbit1 kernel: [504739.142737] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:08 linbit1 kernel: [504739.142743] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:39:08 linbit1 kernel: [504739.165822] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Preparing remote state change 4270293865
Mar 13 20:39:08 linbit1 kernel: [504739.215891] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:39:08 linbit1 kernel: [504739.215902] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit2: peer 3A856CD82155D872:D7CB34FBCF2CFEB4:D7CB34FBCF2CFEB4:0000000000000000 bits:0 flags:1824
Mar 13 20:39:08 linbit1 kernel: [504739.216086] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Committing remote state change 4270293865 (primary_nodes=0)
Mar 13 20:39:08 linbit1 kernel: [504739.216104] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:08 linbit1 kernel: [504739.216111] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established ) resync-susp( no -> peer )
Mar 13 20:39:08 linbit1 kernel: [504739.216581] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Preparing remote state change 3066111332
Mar 13 20:39:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:08.224714    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:39:08 linbit1 kernel: [504739.259906] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:39:08 linbit1 kernel: [504739.259913] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit3: peer 3A856CD82155D872:D7CB34FBCF2CFEB5:0000000000000000:0000000000000000 bits:0 flags:1020
Mar 13 20:39:08 linbit1 kernel: [504739.260087] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Committing remote state change 3066111332 (primary_nodes=0)
Mar 13 20:39:08 linbit1 kernel: [504739.260119] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:08 linbit1 kernel: [504739.260123] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035: quorum( no -> yes )
Mar 13 20:39:08 linbit1 kernel: [504739.260127] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:39:08 linbit1 kernel: [504739.326782] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Preparing remote state change 3434851653
Mar 13 20:39:08 linbit1 kernel: [504739.327193] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Committing remote state change 3434851653 (primary_nodes=1)
Mar 13 20:39:08 linbit1 kernel: [504739.327208] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: peer( Secondary -> Primary )
Mar 13 20:39:08 linbit1 kernel: [504739.463909] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:39:08 linbit1 kernel: [504739.463920] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:08 linbit1 kernel: [504739.464137] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:08 linbit1 kernel: [504739.474882] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 1134139827 (0->-1 3/1)
Mar 13 20:39:08 linbit1 kernel: [504739.475370] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: State change 1134139827: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:39:08 linbit1 kernel: [504739.475376] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 1134139827 (0ms)
Mar 13 20:39:08 linbit1 kernel: [504739.475386] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: role( Secondary -> Primary )
Mar 13 20:39:08 linbit1 kernel: [504739.475692] loop24: detected capacity change from 0 to 20971520
Mar 13 20:39:08 linbit1 systemd-networkd[2297221]: cali375d3b08313: Gained IPv6LL
Mar 13 20:39:08 linbit1 systemd-networkd[2297221]: caliead4891e62b: Gained IPv6LL
Mar 13 20:39:08 linbit1 kernel: [504739.570688] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Preparing remote state change 2903839957
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.566431231Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.569384720Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.575092257Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:08 linbit1 kernel: [504739.593170] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: rs_discard_granularity feature disabled
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.579870872Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.581655077Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.584774167Z" level=info msg="CreateContainer within sandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.593711594Z" level=info msg="CreateContainer within sandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e\""
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.594203715Z" level=info msg="StartContainer for \"93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e\""
Mar 13 20:39:08 linbit1 kernel: [504739.612893] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Committing remote state change 2903839957 (primary_nodes=0)
Mar 13 20:39:08 linbit1 kernel: [504739.637834] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Preparing remote state change 3744957615
Mar 13 20:39:08 linbit1 kernel: [504739.687894] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:39:08 linbit1 kernel: [504739.687903] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit2: peer EF73B8240ED384C2:8C4600F2F9B55210:8C4600F2F9B55210:0000000000000000 bits:0 flags:1824
Mar 13 20:39:08 linbit1 kernel: [504739.689900] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Committing remote state change 3744957615 (primary_nodes=0)
Mar 13 20:39:08 linbit1 kernel: [504739.689915] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:08 linbit1 kernel: [504739.689919] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032: quorum( no -> yes )
Mar 13 20:39:08 linbit1 kernel: [504739.689924] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit2: pdsk( DUnknown -> Inconsistent ) repl( Off -> Established ) resync-susp( no -> peer )
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.711867551Z" level=info msg="StartContainer for \"93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e\" returns successfully"
Mar 13 20:39:08 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08.794590201Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n3-0-zq6bl,Uid:23c28465-eb55-4375-9bd1-5fd757745f89,Namespace:default,Attempt:0,}"
Mar 13 20:39:09 linbit1 kernel: [504740.057403] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:39:09 linbit1 kernel: [504740.057558] IPv6: ADDRCONF(NETDEV_CHANGE): cali97811183456: link becomes ready
Mar 13 20:39:09 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 108 seen, reloading interface list
Mar 13 20:39:09 linbit1 systemd-networkd[2297221]: cali97811183456: Link UP
Mar 13 20:39:09 linbit1 systemd-networkd[2297221]: cali97811183456: Gained carrier
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.821 [INFO][2455325] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.849 [INFO][2455325] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0 fio-bench-r2-n3-0- default  23c28465-eb55-4375-9bd1-5fd757745f89 17525873 0 2023-03-13 20:37:26 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:c53f902d-0fa9-4fdd-a375-f3863115309e job-name:fio-bench-r2-n3-0 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n3-0-zq6bl eth0 default [] []   [kns.default ksa.default.default] cali97811183456  [] []}} ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Namespace="default" Pod="fio-bench-r2-n3-0-zq6bl" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.850 [INFO][2455325] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Namespace="default" Pod="fio-bench-r2-n3-0-zq6bl" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.897 [INFO][2455363] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.924 [INFO][2455363] ipam_plugin.go 264: Auto assigning IP ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x40006a3920), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n3-0-zq6bl", "timestamp":"2023-03-13 20:39:08.897630593 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:08Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.925 [INFO][2455363] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.935 [INFO][2455363] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.948 [INFO][2455363] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.965 [INFO][2455363] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.970 [INFO][2455363] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.980 [INFO][2455363] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.980 [INFO][2455363] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.985 [INFO][2455363] ipam.go 1680: Creating new handle: k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:08.999 [INFO][2455363] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.033 [INFO][2455363] ipam.go 1214: Successfully claimed IPs: [10.1.217.5/26] block=10.1.217.0/26 handle="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.033 [INFO][2455363] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.5/26] handle="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" host="linbit1"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.033 [INFO][2455363] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.5/26] IPv6=[] ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.038 [INFO][2455325] k8s.go 383: Populated endpoint ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Namespace="default" Pod="fio-bench-r2-n3-0-zq6bl" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0", GenerateName:"fio-bench-r2-n3-0-", Namespace:"default", SelfLink:"", UID:"23c28465-eb55-4375-9bd1-5fd757745f89", ResourceVersion:"17525873", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"c53f902d-0fa9-4fdd-a375-f3863115309e", "job-name":"fio-bench-r2-n3-0", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n3-0-zq6bl", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.5/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali97811183456", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.039 [INFO][2455325] k8s.go 384: Calico CNI using IPs: [10.1.217.5/32] ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Namespace="default" Pod="fio-bench-r2-n3-0-zq6bl" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.039 [INFO][2455325] dataplane_linux.go 68: Setting the host side veth name to cali97811183456 ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Namespace="default" Pod="fio-bench-r2-n3-0-zq6bl" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.041 [INFO][2455325] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Namespace="default" Pod="fio-bench-r2-n3-0-zq6bl" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.109 [INFO][2455325] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Namespace="default" Pod="fio-bench-r2-n3-0-zq6bl" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0", GenerateName:"fio-bench-r2-n3-0-", Namespace:"default", SelfLink:"", UID:"23c28465-eb55-4375-9bd1-5fd757745f89", ResourceVersion:"17525873", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 26, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"c53f902d-0fa9-4fdd-a375-f3863115309e", "job-name":"fio-bench-r2-n3-0", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6", Pod:"fio-bench-r2-n3-0-zq6bl", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.5/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali97811183456", MAC:"ee:44:21:b0:f5:04", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:09.124 [INFO][2455325] k8s.go 489: Wrote updated endpoint to datastore ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Namespace="default" Pod="fio-bench-r2-n3-0-zq6bl" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.154357489Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.154464609Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.154498450Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.154776450Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6 pid=2455427 runtime=io.containerd.runc.v2
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.269915879Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n3-0-zq6bl,Uid:23c28465-eb55-4375-9bd1-5fd757745f89,Namespace:default,Attempt:0,} returns sandbox id \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\""
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.271998965Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.880223324Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.882791492Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.887907308Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.892006320Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.894356567Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.897908338Z" level=info msg="CreateContainer within sandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.905200760Z" level=info msg="CreateContainer within sandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1\""
Mar 13 20:39:09 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:09.905596641Z" level=info msg="StartContainer for \"a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1\""
Mar 13 20:39:10 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:10.013796848Z" level=info msg="StartContainer for \"a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1\" returns successfully"
Mar 13 20:39:10 linbit1 systemd-networkd[2297221]: cali97811183456: Gained IPv6LL
Mar 13 20:39:10 linbit1 kernel: [504741.768452] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: rs_discard_granularity feature disabled
Mar 13 20:39:12 linbit1 kernel: [504743.060011] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: rs_discard_granularity feature disabled
Mar 13 20:39:14 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:14.684959    2448 topology_manager.go:210] "Topology Admit Handler"
Mar 13 20:39:14 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:14.808302    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sk5cf\" (UniqueName: \"kubernetes.io/projected/6352c9ef-1397-436e-b40b-a40ae673de66-kube-api-access-sk5cf\") pod \"fio-bench-r2-n5-3-5zdjt\" (UID: \"6352c9ef-1397-436e-b40b-a40ae673de66\") " pod="default/fio-bench-r2-n5-3-5zdjt"
Mar 13 20:39:15 linbit1 kernel: [504746.639563] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: rs_discard_granularity feature disabled
Mar 13 20:39:16 linbit1 kernel: [504747.985272] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: rs_discard_granularity feature disabled
Mar 13 20:39:20 linbit1 kernel: [504751.758744] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Preparing remote state change 152642958
Mar 13 20:39:20 linbit1 kernel: [504751.759072] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Committing remote state change 152642958 (primary_nodes=1)
Mar 13 20:39:20 linbit1 kernel: [504751.759084] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: peer( Secondary -> Primary )
Mar 13 20:39:20 linbit1 kernel: [504751.761684] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Preparing remote state change 2593348746
Mar 13 20:39:20 linbit1 kernel: [504751.762019] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Committing remote state change 2593348746 (primary_nodes=1)
Mar 13 20:39:20 linbit1 kernel: [504751.762033] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: peer( Secondary -> Primary )
Mar 13 20:39:21 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:21.861486    2448 reconciler_common.go:253] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\") pod \"fio-bench-r2-n5-3-5zdjt\" (UID: \"6352c9ef-1397-436e-b40b-a40ae673de66\") " pod="default/fio-bench-r2-n5-3-5zdjt"
Mar 13 20:39:21 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:21.886158    2448 operation_generator.go:1582] "Controller attach succeeded for volume \"pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\") pod \"fio-bench-r2-n5-3-5zdjt\" (UID: \"6352c9ef-1397-436e-b40b-a40ae673de66\") device path: \"\"" pod="default/fio-bench-r2-n5-3-5zdjt"
Mar 13 20:39:21 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:21.963028    2448 operation_generator.go:1103] "MapVolume.WaitForAttach entering for volume \"pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\") pod \"fio-bench-r2-n5-3-5zdjt\" (UID: \"6352c9ef-1397-436e-b40b-a40ae673de66\") DevicePath \"\"" pod="default/fio-bench-r2-n5-3-5zdjt"
Mar 13 20:39:21 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:21.971162    2448 operation_generator.go:1113] "MapVolume.WaitForAttach succeeded for volume \"pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\") pod \"fio-bench-r2-n5-3-5zdjt\" (UID: \"6352c9ef-1397-436e-b40b-a40ae673de66\") DevicePath \"csi-864c5d003109b5b1e7318a92264bc8dc244bcbf017ea207dc9c8752ccb7b3506\"" pod="default/fio-bench-r2-n5-3-5zdjt"
Mar 13 20:39:21 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:39:21.977015    2448 csi_block.go:163] kubernetes.io/csi: blockMapper.stageVolumeForBlock STAGE_UNSTAGE_VOLUME capability not set. Skipping MountDevice...
Mar 13 20:39:22 linbit1 kernel: [504753.030611] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 34757710 (0->-1 3/1)
Mar 13 20:39:22 linbit1 kernel: [504753.031086] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: State change 34757710: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:39:22 linbit1 kernel: [504753.031092] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 34757710 (0ms)
Mar 13 20:39:22 linbit1 kernel: [504753.031103] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: role( Secondary -> Primary )
Mar 13 20:39:22 linbit1 kernel: [504753.031449] loop25: detected capacity change from 0 to 20971520
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22.234678370Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n5-3-5zdjt,Uid:6352c9ef-1397-436e-b40b-a40ae673de66,Namespace:default,Attempt:0,}"
Mar 13 20:39:22 linbit1 kernel: [504753.498618] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Mar 13 20:39:22 linbit1 kernel: [504753.498770] IPv6: ADDRCONF(NETDEV_CHANGE): cali7414e773585: link becomes ready
Mar 13 20:39:22 linbit1 networkd-dispatcher[2346]: WARNING:Unknown index 109 seen, reloading interface list
Mar 13 20:39:22 linbit1 systemd-networkd[2297221]: cali7414e773585: Link UP
Mar 13 20:39:22 linbit1 systemd-networkd[2297221]: cali7414e773585: Gained carrier
Mar 13 20:39:22 linbit1 systemd-udevd[2456392]: Using default interface naming scheme 'v249'.
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.268 [INFO][2456433] utils.go 108: File /var/lib/calico/mtu does not exist
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.301 [INFO][2456433] plugin.go 324: Calico CNI found existing endpoint: &{{WorkloadEndpoint projectcalico.org/v3} {linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0 fio-bench-r2-n5-3- default  6352c9ef-1397-436e-b40b-a40ae673de66 17526238 0 2023-03-13 20:37:27 +0000 UTC <nil> <nil> map[app:linstorbench controller-uid:8c0516e2-2dac-443b-b345-b8732aab1ee5 job-name:fio-bench-r2-n5-3 projectcalico.org/namespace:default projectcalico.org/orchestrator:k8s projectcalico.org/serviceaccount:default] map[] [] []  []} {k8s  linbit1  fio-bench-r2-n5-3-5zdjt eth0 default [] []   [kns.default ksa.default.default] cali7414e773585  [] []}} ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Namespace="default" Pod="fio-bench-r2-n5-3-5zdjt" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.302 [INFO][2456433] k8s.go 74: Extracted identifiers for CmdAddK8s ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Namespace="default" Pod="fio-bench-r2-n5-3-5zdjt" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.350 [INFO][2456477] ipam_plugin.go 224: Calico CNI IPAM request count IPv4=1 IPv6=0 ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.387 [INFO][2456477] ipam_plugin.go 264: Auto assigning IP ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0" assignArgs=ipam.AutoAssignArgs{Num4:1, Num6:0, HandleID:(*string)(0x400071c540), Attrs:map[string]string{"namespace":"default", "node":"linbit1", "pod":"fio-bench-r2-n5-3-5zdjt", "timestamp":"2023-03-13 20:39:22.350004799 +0000 UTC"}, Hostname:"linbit1", IPv4Pools:[]net.IPNet{}, IPv6Pools:[]net.IPNet{}, MaxBlocksPerHost:0, HostReservedAttrIPv4s:(*ipam.HostReservedAttr)(nil), HostReservedAttrIPv6s:(*ipam.HostReservedAttr)(nil), IntendedUse:"Workload"}
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.387 [INFO][2456477] ipam.go 105: Auto-assign 1 ipv4, 0 ipv6 addrs for host 'linbit1'
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.396 [INFO][2456477] ipam.go 658: Looking up existing affinities for host handle="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.408 [INFO][2456477] ipam.go 370: Looking up existing affinities for host host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.426 [INFO][2456477] ipam.go 487: Trying affinity for 10.1.217.0/26 host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.432 [INFO][2456477] ipam.go 153: Attempting to load block cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.440 [INFO][2456477] ipam.go 230: Affinity is confirmed and block has been loaded cidr=10.1.217.0/26 host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.440 [INFO][2456477] ipam.go 1178: Attempting to assign 1 addresses from block block=10.1.217.0/26 handle="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.446 [INFO][2456477] ipam.go 1680: Creating new handle: k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.455 [INFO][2456477] ipam.go 1201: Writing block in order to claim IPs block=10.1.217.0/26 handle="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.475 [INFO][2456477] ipam.go 1214: Successfully claimed IPs: [10.1.217.37/26] block=10.1.217.0/26 handle="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.475 [INFO][2456477] ipam.go 845: Auto-assigned 1 out of 1 IPv4s: [10.1.217.37/26] handle="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" host="linbit1"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.475 [INFO][2456477] ipam_plugin.go 282: Calico CNI IPAM assigned addresses IPv4=[10.1.217.37/26] IPv6=[] ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.480 [INFO][2456433] k8s.go 383: Populated endpoint ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Namespace="default" Pod="fio-bench-r2-n5-3-5zdjt" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0", GenerateName:"fio-bench-r2-n5-3-", Namespace:"default", SelfLink:"", UID:"6352c9ef-1397-436e-b40b-a40ae673de66", ResourceVersion:"17526238", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 27, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"8c0516e2-2dac-443b-b345-b8732aab1ee5", "job-name":"fio-bench-r2-n5-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"", Pod:"fio-bench-r2-n5-3-5zdjt", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.37/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali7414e773585", MAC:"", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.480 [INFO][2456433] k8s.go 384: Calico CNI using IPs: [10.1.217.37/32] ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Namespace="default" Pod="fio-bench-r2-n5-3-5zdjt" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.480 [INFO][2456433] dataplane_linux.go 68: Setting the host side veth name to cali7414e773585 ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Namespace="default" Pod="fio-bench-r2-n5-3-5zdjt" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.483 [INFO][2456433] dataplane_linux.go 453: Disabling IPv4 forwarding ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Namespace="default" Pod="fio-bench-r2-n5-3-5zdjt" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.529 [INFO][2456433] k8s.go 411: Added Mac, interface name, and active container ID to endpoint ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Namespace="default" Pod="fio-bench-r2-n5-3-5zdjt" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0" endpoint=&v3.WorkloadEndpoint{TypeMeta:v1.TypeMeta{Kind:"WorkloadEndpoint", APIVersion:"projectcalico.org/v3"}, ObjectMeta:v1.ObjectMeta{Name:"linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0", GenerateName:"fio-bench-r2-n5-3-", Namespace:"default", SelfLink:"", UID:"6352c9ef-1397-436e-b40b-a40ae673de66", ResourceVersion:"17526238", Generation:0, CreationTimestamp:time.Date(2023, time.March, 13, 20, 37, 27, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"linstorbench", "controller-uid":"8c0516e2-2dac-443b-b345-b8732aab1ee5", "job-name":"fio-bench-r2-n5-3", "projectcalico.org/namespace":"default", "projectcalico.org/orchestrator":"k8s", "projectcalico.org/serviceaccount":"default"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v3.WorkloadEndpointSpec{Orchestrator:"k8s", Workload:"", Node:"linbit1", ContainerID:"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61", Pod:"fio-bench-r2-n5-3-5zdjt", Endpoint:"eth0", ServiceAccountName:"default", IPNetworks:[]string{"10.1.217.37/32"}, IPNATs:[]v3.IPNAT(nil), IPv4Gateway:"", IPv6Gateway:"", Profiles:[]string{"kns.default", "ksa.default.default"}, InterfaceName:"cali7414e773585", MAC:"12:8f:0d:0a:7d:e7", Ports:[]v3.WorkloadEndpointPort(nil), AllowSpoofedSourcePrefixes:[]string(nil)}}
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:39:22.544 [INFO][2456433] k8s.go 489: Wrote updated endpoint to datastore ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Namespace="default" Pod="fio-bench-r2-n5-3-5zdjt" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22.570841347Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22.570970907Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22.570996467Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22.571240428Z" level=info msg="starting signal loop" namespace=k8s.io path=/var/snap/microk8s/common/run/containerd/io.containerd.runtime.v2.task/k8s.io/e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61 pid=2456541 runtime=io.containerd.runc.v2
Mar 13 20:39:22 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:39:22.651115    2448 cadvisor_stats_provider.go:442] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods/besteffort/pod6352c9ef-1397-436e-b40b-a40ae673de66/e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\": RecentStats: unable to find data in memory cache]"
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22.666708557Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:fio-bench-r2-n5-3-5zdjt,Uid:6352c9ef-1397-436e-b40b-a40ae673de66,Namespace:default,Attempt:0,} returns sandbox id \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\""
Mar 13 20:39:22 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:22.668502602Z" level=info msg="PullImage \"curlimages/curl:latest\""
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.224273203Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.226603250Z" level=info msg="ImageUpdate event &ImageUpdate{Name:sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.231877506Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl:latest,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.236323679Z" level=info msg="ImageUpdate event &ImageUpdate{Name:docker.io/curlimages/curl@sha256:48318407b8d98e8c7d5bd4741c88e8e1a5442de660b47f63ba656e5c910bc3da,Labels:map[string]string{io.cri-containerd.image: managed,},XXX_unrecognized:[],}"
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.238199485Z" level=info msg="PullImage \"curlimages/curl:latest\" returns image reference \"sha256:5880c2bc66d5c2fdec926f2a8d65fee338d703f99f0d08739d2b6605b3e919de\""
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.241639096Z" level=info msg="CreateContainer within sandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" for container &ContainerMetadata{Name:wait-for-sync,Attempt:0,}"
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.248622237Z" level=info msg="CreateContainer within sandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" for &ContainerMetadata{Name:wait-for-sync,Attempt:0,} returns container id \"738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c\""
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.249041278Z" level=info msg="StartContainer for \"738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c\""
Mar 13 20:39:23 linbit1 kernel: [504754.301806] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: sock was shut down by peer
Mar 13 20:39:23 linbit1 kernel: [504754.301822] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:39:23 linbit1 kernel: [504754.301830] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Connected -> BrokenPipe ) peer( Primary -> Unknown )
Mar 13 20:39:23 linbit1 kernel: [504754.301834] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:39:23 linbit1 kernel: [504754.301994] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Terminating sender thread
Mar 13 20:39:23 linbit1 kernel: [504754.302033] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Starting sender thread (from drbd_r_pvc-b5e1 [2448489])
Mar 13 20:39:23 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:39:23.343804285Z" level=info msg="StartContainer for \"738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c\" returns successfully"
Mar 13 20:39:23 linbit1 kernel: [504754.371902] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Connection closed
Mar 13 20:39:23 linbit1 kernel: [504754.371916] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:39:23 linbit1 kernel: [504754.371924] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Restarting receiver thread
Mar 13 20:39:23 linbit1 kernel: [504754.371932] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Unconnected -> Connecting )
Mar 13 20:39:23 linbit1 kernel: [504754.407547] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:39:23 linbit1 kernel: [504754.407560] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: updated UUIDs 67E6FE2E8F370A4C:0000000000000000:10FC5CC0C6FEC7C8:0000000000000000
Mar 13 20:39:23 linbit1 kernel: [504754.407574] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: disk( Inconsistent -> UpToDate )
Mar 13 20:39:23 linbit1 kernel: [504754.407578] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit3: resync-susp( connection dependency -> no )
Mar 13 20:39:23 linbit1 kernel: [504754.407581] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: repl( SyncTarget -> Established )
Mar 13 20:39:23 linbit1 kernel: [504754.411241] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit3: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:39:23 linbit1 kernel: [504754.413891] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007: receiver updated UUIDs to exposed data uuid: 8F1D52E9CE37BBF6
Mar 13 20:39:23 linbit1 kernel: [504754.544759] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Preparing remote state change 4064279097
Mar 13 20:39:23 linbit1 kernel: [504754.545127] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Committing remote state change 4064279097 (primary_nodes=1)
Mar 13 20:39:23 linbit1 kernel: [504754.545143] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: peer( Secondary -> Primary )
Mar 13 20:39:23 linbit1 kernel: [504754.546318] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Preparing remote state change 2691073699
Mar 13 20:39:23 linbit1 kernel: [504754.546581] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Committing remote state change 2691073699 (primary_nodes=1)
Mar 13 20:39:23 linbit1 kernel: [504754.546596] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: peer( Secondary -> Primary )
Mar 13 20:39:23 linbit1 kernel: [504754.897818] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:39:23 linbit1 kernel: [504754.897827] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:23 linbit1 kernel: [504754.897940] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:24 linbit1 kernel: [504755.061817] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Preparing remote state change 2008498712
Mar 13 20:39:24 linbit1 kernel: [504755.103993] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: updated UUIDs 365522F30E0CAB98:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:24 linbit1 kernel: [504755.104008] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: Clearing bitmap UUID for node 2
Mar 13 20:39:24 linbit1 kernel: [504755.104437] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: Resync done (total 106 sec; paused 0 sec; 98920 K/sec)
Mar 13 20:39:24 linbit1 kernel: [504755.104457] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:24 linbit1 kernel: [504755.111865] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: my exposed UUID: 0000000000000000
Mar 13 20:39:24 linbit1 kernel: [504755.111875] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: peer 8CECECC5242E4A6E:2E5F4E1213C623B8:0000000000000000:0000000000000000 bits:0 flags:1120
Mar 13 20:39:24 linbit1 kernel: [504755.112045] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Committing remote state change 2008498712 (primary_nodes=1)
Mar 13 20:39:24 linbit1 kernel: [504755.112063] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Connecting -> Connected ) peer( Unknown -> Primary )
Mar 13 20:39:24 linbit1 kernel: [504755.112069] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:39:24 linbit1 systemd-networkd[2297221]: cali7414e773585: Gained IPv6LL
Mar 13 20:39:24 linbit1 kernel: [504755.982016] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit2: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:39:24 linbit1 kernel: [504755.984426] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008: receiver updated UUIDs to exposed data uuid: EEF0D87FF1D965AA
Mar 13 20:39:25 linbit1 kernel: [504756.357791] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: sock was shut down by peer
Mar 13 20:39:25 linbit1 kernel: [504756.357812] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:39:25 linbit1 kernel: [504756.357819] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit2: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 20:39:25 linbit1 kernel: [504756.357949] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Terminating sender thread
Mar 13 20:39:25 linbit1 kernel: [504756.357999] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Starting sender thread (from drbd_r_pvc-b01e [2449062])
Mar 13 20:39:25 linbit1 kernel: [504756.426767] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Connection closed
Mar 13 20:39:25 linbit1 kernel: [504756.426929] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:39:25 linbit1 kernel: [504756.426940] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Restarting receiver thread
Mar 13 20:39:25 linbit1 kernel: [504756.426949] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:39:25 linbit1 kernel: [504756.945856] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:39:25 linbit1 kernel: [504756.945867] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:25 linbit1 kernel: [504756.945971] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:25 linbit1 kernel: [504756.971776] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 2376340786 (0->2 499/145)
Mar 13 20:39:26 linbit1 kernel: [504757.043792] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit2: self 0D7C606C36409062:405E968F96DD2021:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:39:26 linbit1 kernel: [504757.043807] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:39:26 linbit1 kernel: [504757.043824] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change 2376340786: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:39:26 linbit1 kernel: [504757.043832] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 2376340786 (72ms)
Mar 13 20:39:26 linbit1 kernel: [504757.043877] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:26 linbit1 kernel: [504757.043884] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:39:26 linbit1 kernel: [504757.244977] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: updated UUIDs D5FDCB3B6CEDFD0E:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:26 linbit1 kernel: [504757.245358] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:39:26 linbit1 kernel: [504757.245378] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:27 linbit1 kernel: [504758.401520] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: updated UUIDs 6E639916C4467B00:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:27 linbit1 kernel: [504758.401531] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: Clearing bitmap UUID for node 2
Mar 13 20:39:27 linbit1 kernel: [504758.401872] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: Resync done (total 106 sec; paused 0 sec; 98920 K/sec)
Mar 13 20:39:27 linbit1 kernel: [504758.401885] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:28 linbit1 kernel: [504759.053780] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: updated UUIDs 1CE8657E0B4F4A9A:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:28 linbit1 kernel: [504759.053795] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: Clearing bitmap UUID for node 2
Mar 13 20:39:28 linbit1 kernel: [504759.054147] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:39:28 linbit1 kernel: [504759.054167] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:28 linbit1 kernel: [504759.318615] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: updated UUIDs 666FBF2D8784402C:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:28 linbit1 kernel: [504759.318630] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: Clearing bitmap UUID for node 2
Mar 13 20:39:28 linbit1 kernel: [504759.318965] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:39:28 linbit1 kernel: [504759.318984] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:28 linbit1 kernel: [504759.541618] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: updated UUIDs AECE884230265E80:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:28 linbit1 kernel: [504759.541631] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: Clearing bitmap UUID for node 2
Mar 13 20:39:28 linbit1 kernel: [504759.541867] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:39:28 linbit1 kernel: [504759.541883] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:29 linbit1 kernel: [504760.579940] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: updated UUIDs C6D5DA1F69944504:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:29 linbit1 kernel: [504760.579954] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: Clearing bitmap UUID for node 2
Mar 13 20:39:29 linbit1 kernel: [504760.580369] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:39:29 linbit1 kernel: [504760.580388] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:45 linbit1 kernel: [504776.738451] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:39:45 linbit1 kernel: [504776.738467] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: updated UUIDs CACE1D3C53DDAC64:0000000000000000:C0DAB66176F1E84C:0000000000000000
Mar 13 20:39:45 linbit1 kernel: [504776.738486] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: disk( Inconsistent -> UpToDate )
Mar 13 20:39:45 linbit1 kernel: [504776.738490] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: repl( SyncTarget -> Established )
Mar 13 20:39:45 linbit1 kernel: [504776.738495] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit2: resync-susp( connection dependency -> no )
Mar 13 20:39:47 linbit1 kernel: [504778.403717] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: sock_recvmsg returned -11
Mar 13 20:39:47 linbit1 kernel: [504778.403744] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:39:47 linbit1 kernel: [504778.403750] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 20:39:47 linbit1 kernel: [504778.403895] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Terminating sender thread
Mar 13 20:39:47 linbit1 kernel: [504778.403937] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Starting sender thread (from drbd_r_pvc-a79c [2450592])
Mar 13 20:39:47 linbit1 kernel: [504778.479882] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Connection closed
Mar 13 20:39:47 linbit1 kernel: [504778.479891] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 20:39:47 linbit1 kernel: [504778.479896] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Restarting receiver thread
Mar 13 20:39:47 linbit1 kernel: [504778.479900] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:39:48 linbit1 kernel: [504779.043829] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:39:48 linbit1 kernel: [504779.043840] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:48 linbit1 kernel: [504779.044831] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:48 linbit1 kernel: [504779.063714] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Preparing cluster-wide state change 907016605 (1->2 499/146)
Mar 13 20:39:48 linbit1 kernel: [504779.119718] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: self 733662B8AC91036A:0000000000000000:0BBAA133EAF80B88:0000000000000000 bits:0 flags:0
Mar 13 20:39:48 linbit1 kernel: [504779.119729] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: peer's exposed UUID: 0000000000000000
Mar 13 20:39:48 linbit1 kernel: [504779.119743] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: State change 907016605: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:39:48 linbit1 kernel: [504779.119749] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Committing cluster-wide state change 907016605 (56ms)
Mar 13 20:39:48 linbit1 kernel: [504779.119798] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:48 linbit1 kernel: [504779.119803] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:39:48 linbit1 kernel: [504779.561728] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Preparing remote state change 3279346691
Mar 13 20:39:48 linbit1 kernel: [504779.598884] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Committing remote state change 3279346691 (primary_nodes=1)
Mar 13 20:39:48 linbit1 kernel: [504779.601749] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Preparing remote state change 2650683870
Mar 13 20:39:48 linbit1 kernel: [504779.634727] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Committing remote state change 2650683870 (primary_nodes=1)
Mar 13 20:39:50 linbit1 kernel: [504781.081989] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit3: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:39:50 linbit1 kernel: [504781.082407] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010: receiver updated UUIDs to exposed data uuid: 8CECECC5242E4A6E
Mar 13 20:39:51 linbit1 kernel: [504782.250557] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: sock was shut down by peer
Mar 13 20:39:51 linbit1 kernel: [504782.250578] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:39:51 linbit1 kernel: [504782.250585] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: repl( Established -> Off ) resync-susp( peer -> no )
Mar 13 20:39:51 linbit1 kernel: [504782.250974] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Terminating sender thread
Mar 13 20:39:51 linbit1 kernel: [504782.251016] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Starting sender thread (from drbd_r_pvc-7a5e [2450811])
Mar 13 20:39:51 linbit1 kernel: [504782.295812] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Connection closed
Mar 13 20:39:51 linbit1 kernel: [504782.295823] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 20:39:51 linbit1 kernel: [504782.295831] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Restarting receiver thread
Mar 13 20:39:51 linbit1 kernel: [504782.295839] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:39:51 linbit1 kernel: [504782.819804] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:39:51 linbit1 kernel: [504782.819814] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:51 linbit1 kernel: [504782.820490] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:51 linbit1 kernel: [504782.859697] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Preparing remote state change 1550055894
Mar 13 20:39:51 linbit1 kernel: [504782.891730] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:39:51 linbit1 kernel: [504782.891737] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: peer B6CEB0CB01A622F8:0000000000000000:4B43951C0E42492A:0000000000000000 bits:0 flags:1904
Mar 13 20:39:51 linbit1 kernel: [504782.906603] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Committing remote state change 1550055894 (primary_nodes=1)
Mar 13 20:39:51 linbit1 kernel: [504782.906622] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:51 linbit1 kernel: [504782.906628] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: repl( Off -> Established ) resync-susp( no -> peer )
Mar 13 20:39:52 linbit1 kernel: [504783.074923] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:39:52 linbit1 kernel: [504783.074938] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: updated UUIDs B08AE663A5D508EC:0000000000000000:9878D4B3AC0CDFE6:0000000000000000
Mar 13 20:39:52 linbit1 kernel: [504783.074953] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: disk( Inconsistent -> UpToDate )
Mar 13 20:39:52 linbit1 kernel: [504783.074957] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit3: resync-susp( connection dependency -> no )
Mar 13 20:39:52 linbit1 kernel: [504783.074961] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: repl( SyncTarget -> Established )
Mar 13 20:39:52 linbit1 kernel: [504783.433417] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:39:52 linbit1 kernel: [504783.433427] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: updated UUIDs B7D22546AC13E266:0000000000000000:CA82BDB037AD25C0:0000000000000000
Mar 13 20:39:52 linbit1 kernel: [504783.433436] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: disk( Inconsistent -> UpToDate )
Mar 13 20:39:52 linbit1 kernel: [504783.433439] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: repl( SyncTarget -> Established )
Mar 13 20:39:52 linbit1 kernel: [504783.433441] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit2: resync-susp( connection dependency -> no )
Mar 13 20:39:53 linbit1 kernel: [504784.288930] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit2: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:39:53 linbit1 kernel: [504784.292124] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015: receiver updated UUIDs to exposed data uuid: 8D39EF2D02272B3E
Mar 13 20:39:54 linbit1 kernel: [504785.026714] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: updated UUIDs 4AA31DF55B1E1998:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:54 linbit1 kernel: [504785.026729] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: Clearing bitmap UUID for node 2
Mar 13 20:39:54 linbit1 kernel: [504785.027156] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:39:54 linbit1 kernel: [504785.027176] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:55 linbit1 kernel: [504786.595780] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: sock_recvmsg returned -11
Mar 13 20:39:55 linbit1 kernel: [504786.595805] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:39:55 linbit1 kernel: [504786.595812] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit2: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 20:39:55 linbit1 kernel: [504786.595961] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Terminating sender thread
Mar 13 20:39:55 linbit1 kernel: [504786.596011] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Starting sender thread (from drbd_r_pvc-c917 [2452006])
Mar 13 20:39:55 linbit1 kernel: [504786.677755] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Connection closed
Mar 13 20:39:55 linbit1 kernel: [504786.677859] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:39:55 linbit1 kernel: [504786.677867] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Restarting receiver thread
Mar 13 20:39:55 linbit1 kernel: [504786.677872] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:39:56 linbit1 kernel: [504787.249738] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:39:56 linbit1 kernel: [504787.249749] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:39:56 linbit1 kernel: [504787.249853] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:39:56 linbit1 kernel: [504787.419698] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 621237436 (0->2 499/145)
Mar 13 20:39:56 linbit1 kernel: [504787.447689] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit2: self 25005DEAB10165E0:FC45BA0DD84D9E4F:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:39:56 linbit1 kernel: [504787.447700] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit2: peer's exposed UUID: 0000000000000000
Mar 13 20:39:56 linbit1 kernel: [504787.447712] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: State change 621237436: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:39:56 linbit1 kernel: [504787.447718] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 621237436 (28ms)
Mar 13 20:39:56 linbit1 kernel: [504787.447754] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:39:56 linbit1 kernel: [504787.447759] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:39:56 linbit1 kernel: [504787.495225] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit3: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:39:56 linbit1 kernel: [504787.496947] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016: receiver updated UUIDs to exposed data uuid: 16216ECF09A0CD20
Mar 13 20:39:57 linbit1 kernel: [504788.037770] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: updated UUIDs 9ED24C56D5CABD9E:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:57 linbit1 kernel: [504788.037785] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: Clearing bitmap UUID for node 2
Mar 13 20:39:57 linbit1 kernel: [504788.038131] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:39:57 linbit1 kernel: [504788.038150] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:58 linbit1 kernel: [504789.038533] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Preparing remote state change 1942289998
Mar 13 20:39:58 linbit1 kernel: [504789.070528] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Preparing remote state change 2069498682
Mar 13 20:39:58 linbit1 kernel: [504789.082570] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Committing remote state change 1942289998 (primary_nodes=1)
Mar 13 20:39:58 linbit1 kernel: [504789.110573] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Committing remote state change 2069498682 (primary_nodes=1)
Mar 13 20:39:59 linbit1 kernel: [504790.693057] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: updated UUIDs 0D7C606C36409062:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:59 linbit1 kernel: [504790.693067] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: Clearing bitmap UUID for node 2
Mar 13 20:39:59 linbit1 kernel: [504790.693325] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: Resync done (total 106 sec; paused 0 sec; 98920 K/sec)
Mar 13 20:39:59 linbit1 kernel: [504790.693338] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:39:59 linbit1 kernel: [504790.761004] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: updated UUIDs E6C9CF72D8357D5C:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:39:59 linbit1 kernel: [504790.761018] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: Clearing bitmap UUID for node 2
Mar 13 20:39:59 linbit1 kernel: [504790.761367] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: Resync done (total 106 sec; paused 0 sec; 98920 K/sec)
Mar 13 20:39:59 linbit1 kernel: [504790.761387] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:40:11 linbit1 kernel: [504802.979676] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: sock_recvmsg returned -11
Mar 13 20:40:11 linbit1 kernel: [504802.979696] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:40:11 linbit1 kernel: [504802.979705] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( Connected -> BrokenPipe ) peer( Primary -> Unknown )
Mar 13 20:40:11 linbit1 kernel: [504802.979711] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit3: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:40:11 linbit1 kernel: [504802.979873] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Terminating sender thread
Mar 13 20:40:11 linbit1 kernel: [504802.979937] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Starting sender thread (from drbd_r_pvc-2262 [2454600])
Mar 13 20:40:12 linbit1 kernel: [504803.055755] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Connection closed
Mar 13 20:40:12 linbit1 kernel: [504803.055771] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 20:40:12 linbit1 kernel: [504803.055782] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Restarting receiver thread
Mar 13 20:40:12 linbit1 kernel: [504803.055791] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( Unconnected -> Connecting )
Mar 13 20:40:12 linbit1 kernel: [504803.619780] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:40:12 linbit1 kernel: [504803.619787] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:40:12 linbit1 kernel: [504803.619958] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:40:12 linbit1 kernel: [504803.678469] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Preparing remote state change 2641424240
Mar 13 20:40:12 linbit1 kernel: [504803.711685] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:40:12 linbit1 kernel: [504803.711691] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit3: peer 3A856CD82155D872:D7CB34FBCF2CFEB5:0000000000000000:0000000000000000 bits:0 flags:1120
Mar 13 20:40:12 linbit1 kernel: [504803.718518] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Committing remote state change 2641424240 (primary_nodes=1)
Mar 13 20:40:12 linbit1 kernel: [504803.718540] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( Connecting -> Connected ) peer( Unknown -> Primary )
Mar 13 20:40:12 linbit1 kernel: [504803.718544] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:40:13 linbit1 kernel: [504804.766473] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: sock was shut down by peer
Mar 13 20:40:13 linbit1 kernel: [504804.766496] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:40:13 linbit1 kernel: [504804.766503] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: repl( Established -> Off ) resync-susp( peer -> no )
Mar 13 20:40:13 linbit1 kernel: [504804.766907] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Terminating sender thread
Mar 13 20:40:13 linbit1 kernel: [504804.766951] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Starting sender thread (from drbd_r_pvc-0f95 [2453907])
Mar 13 20:40:13 linbit1 kernel: [504804.851842] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Connection closed
Mar 13 20:40:13 linbit1 kernel: [504804.851859] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 20:40:13 linbit1 kernel: [504804.851869] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Restarting receiver thread
Mar 13 20:40:13 linbit1 kernel: [504804.851878] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Unconnected -> Connecting )
Mar 13 20:40:14 linbit1 kernel: [504805.379724] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:40:14 linbit1 kernel: [504805.379734] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:40:14 linbit1 kernel: [504805.379919] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:40:14 linbit1 kernel: [504805.473673] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: sock was shut down by peer
Mar 13 20:40:14 linbit1 kernel: [504805.473691] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:40:14 linbit1 kernel: [504805.473700] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( Connected -> BrokenPipe ) peer( Primary -> Unknown )
Mar 13 20:40:14 linbit1 kernel: [504805.473705] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: disk( UpToDate -> Consistent )
Mar 13 20:40:14 linbit1 kernel: [504805.473710] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:40:14 linbit1 kernel: [504805.473951] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Terminating sender thread
Mar 13 20:40:14 linbit1 kernel: [504805.473996] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Starting sender thread (from drbd_r_pvc-837c [2445412])
Mar 13 20:40:14 linbit1 kernel: [504805.474016] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Preparing cluster-wide state change 2372853105 (1->-1 0/0)
Mar 13 20:40:14 linbit1 kernel: [504805.474350] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: State change 2372853105: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFFA
Mar 13 20:40:14 linbit1 kernel: [504805.474356] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Committing cluster-wide state change 2372853105 (0ms)
Mar 13 20:40:14 linbit1 kernel: [504805.474365] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: disk( Consistent -> Outdated )
Mar 13 20:40:14 linbit1 kernel: [504805.486454] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:40:14 linbit1 kernel: [504805.486461] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: updated UUIDs 733662B8AC91036A:0000000000000000:0BBAA133EAF80B88:0000000000000000
Mar 13 20:40:14 linbit1 kernel: [504805.486470] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: disk( Inconsistent -> UpToDate )
Mar 13 20:40:14 linbit1 kernel: [504805.486472] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: resync-susp( connection dependency -> no )
Mar 13 20:40:14 linbit1 kernel: [504805.486474] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: repl( SyncTarget -> Established )
Mar 13 20:40:14 linbit1 kernel: [504805.493683] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: sock was shut down by peer
Mar 13 20:40:14 linbit1 kernel: [504805.493702] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Connected -> BrokenPipe ) peer( Primary -> Unknown )
Mar 13 20:40:14 linbit1 kernel: [504805.493709] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:40:14 linbit1 kernel: [504805.493863] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Terminating sender thread
Mar 13 20:40:14 linbit1 kernel: [504805.493902] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Starting sender thread (from drbd_r_pvc-b5e1 [2448489])
Mar 13 20:40:14 linbit1 kernel: [504805.559967] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Connection closed
Mar 13 20:40:14 linbit1 kernel: [504805.559985] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:40:14 linbit1 kernel: [504805.559998] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Restarting receiver thread
Mar 13 20:40:14 linbit1 kernel: [504805.560008] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( Unconnected -> Connecting )
Mar 13 20:40:14 linbit1 kernel: [504805.570531] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Preparing remote state change 1061149392
Mar 13 20:40:14 linbit1 kernel: [504805.575826] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Connection closed
Mar 13 20:40:14 linbit1 kernel: [504805.575840] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:40:14 linbit1 kernel: [504805.575851] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Restarting receiver thread
Mar 13 20:40:14 linbit1 kernel: [504805.575860] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Unconnected -> Connecting )
Mar 13 20:40:14 linbit1 kernel: [504805.591687] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: my exposed UUID: 0000000000000000
Mar 13 20:40:14 linbit1 kernel: [504805.591695] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: peer AE7361DB2B2FCAB8:0000000000000000:B228721D04295568:0000000000000000 bits:0 flags:1904
Mar 13 20:40:14 linbit1 kernel: [504805.618544] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Committing remote state change 1061149392 (primary_nodes=1)
Mar 13 20:40:14 linbit1 kernel: [504805.618564] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:40:14 linbit1 kernel: [504805.618570] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: repl( Off -> Established ) resync-susp( no -> peer )
Mar 13 20:40:15 linbit1 kernel: [504806.083758] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:40:15 linbit1 kernel: [504806.083766] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:40:15 linbit1 kernel: [504806.083948] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:40:15 linbit1 kernel: [504806.097724] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:40:15 linbit1 kernel: [504806.097733] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:40:15 linbit1 kernel: [504806.097889] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:40:15 linbit1 kernel: [504806.261659] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Preparing remote state change 798840467
Mar 13 20:40:15 linbit1 kernel: [504806.277673] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Preparing remote state change 499060828
Mar 13 20:40:15 linbit1 kernel: [504806.295688] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: my exposed UUID: 8CECECC5242E4A6E
Mar 13 20:40:15 linbit1 kernel: [504806.295696] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: peer 8CECECC5242E4A6E:2E5F4E1213C623B8:0000000000000000:0000000000000000 bits:0 flags:1120
Mar 13 20:40:15 linbit1 kernel: [504806.295873] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Committing remote state change 798840467 (primary_nodes=1)
Mar 13 20:40:15 linbit1 kernel: [504806.295890] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Connecting -> Connected ) peer( Unknown -> Primary )
Mar 13 20:40:15 linbit1 kernel: [504806.295896] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:40:15 linbit1 kernel: [504806.311692] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: drbd_sync_handshake:
Mar 13 20:40:15 linbit1 kernel: [504806.311699] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: self 67E6FE2E8F370A4C:0000000000000000:10FC5CC0C6FEC7C8:0000000000000000 bits:0 flags:120
Mar 13 20:40:15 linbit1 kernel: [504806.311707] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: peer 67E6FE2E8F370A4C:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:1120
Mar 13 20:40:15 linbit1 kernel: [504806.311715] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: uuid_compare()=no-sync by rule=reconnected
Mar 13 20:40:15 linbit1 kernel: [504806.313724] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Committing remote state change 499060828 (primary_nodes=1)
Mar 13 20:40:15 linbit1 kernel: [504806.313742] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( Connecting -> Connected ) peer( Unknown -> Primary )
Mar 13 20:40:15 linbit1 kernel: [504806.313747] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: disk( Outdated -> UpToDate )
Mar 13 20:40:15 linbit1 kernel: [504806.313752] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:40:15 linbit1 kernel: [504806.313860] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: cleared bm UUID and bitmap 67E6FE2E8F370A4C:0000000000000000:10FC5CC0C6FEC7C8:0000000000000000
Mar 13 20:40:17 linbit1 kernel: [504808.927353] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:40:17 linbit1 kernel: [504808.931160] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025: receiver updated UUIDs to exposed data uuid: B6CEB0CB01A622F8
Mar 13 20:40:18 linbit1 kernel: [504809.123616] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: sock_recvmsg returned -11
Mar 13 20:40:18 linbit1 kernel: [504809.123632] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:40:18 linbit1 kernel: [504809.123634] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: sock_recvmsg returned -11
Mar 13 20:40:18 linbit1 kernel: [504809.123640] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:40:18 linbit1 kernel: [504809.123646] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:40:18 linbit1 kernel: [504809.123651] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:40:18 linbit1 kernel: [504809.123660] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:40:18 linbit1 kernel: [504809.123666] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:40:18 linbit1 kernel: [504809.123806] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Terminating sender thread
Mar 13 20:40:18 linbit1 kernel: [504809.123844] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Starting sender thread (from drbd_r_pvc-986e [2445770])
Mar 13 20:40:18 linbit1 kernel: [504809.123944] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Terminating sender thread
Mar 13 20:40:18 linbit1 kernel: [504809.124001] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Starting sender thread (from drbd_r_pvc-01ec [2445633])
Mar 13 20:40:18 linbit1 kernel: [504809.160619] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Preparing remote state change 3583486255
Mar 13 20:40:18 linbit1 kernel: [504809.160898] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Committing remote state change 3583486255 (primary_nodes=1)
Mar 13 20:40:18 linbit1 kernel: [504809.160908] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: pdsk( DUnknown -> Outdated )
Mar 13 20:40:18 linbit1 kernel: [504809.168218] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Preparing remote state change 2649365801
Mar 13 20:40:18 linbit1 kernel: [504809.168574] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Committing remote state change 2649365801 (primary_nodes=1)
Mar 13 20:40:18 linbit1 kernel: [504809.168586] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( DUnknown -> Outdated )
Mar 13 20:40:18 linbit1 kernel: [504809.183898] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Connection closed
Mar 13 20:40:18 linbit1 kernel: [504809.183915] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:40:18 linbit1 kernel: [504809.183929] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Restarting receiver thread
Mar 13 20:40:18 linbit1 kernel: [504809.183939] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( Unconnected -> Connecting )
Mar 13 20:40:18 linbit1 kernel: [504809.203902] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Connection closed
Mar 13 20:40:18 linbit1 kernel: [504809.203919] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:40:18 linbit1 kernel: [504809.203930] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Restarting receiver thread
Mar 13 20:40:18 linbit1 kernel: [504809.203985] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Unconnected -> Connecting )
Mar 13 20:40:18 linbit1 kernel: [504809.582469] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Preparing remote state change 3064425139
Mar 13 20:40:18 linbit1 kernel: [504809.606528] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Committing remote state change 3064425139 (primary_nodes=1)
Mar 13 20:40:18 linbit1 kernel: [504809.763703] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:40:18 linbit1 kernel: [504809.763714] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:40:18 linbit1 kernel: [504809.763747] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:40:18 linbit1 kernel: [504809.763756] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:40:18 linbit1 kernel: [504809.763893] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:40:18 linbit1 kernel: [504809.763901] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:40:18 linbit1 kernel: [504809.911630] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 629517346 (0->1 499/145)
Mar 13 20:40:18 linbit1 kernel: [504809.923632] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 4102665490 (0->1 499/145)
Mar 13 20:40:18 linbit1 kernel: [504809.947712] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: drbd_sync_handshake:
Mar 13 20:40:18 linbit1 kernel: [504809.947719] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: self 1CE8657E0B4F4A9A:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:120
Mar 13 20:40:18 linbit1 kernel: [504809.947728] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: peer 1CE8657E0B4F4A9A:0000000000000000:7F08074A817B57EC:0000000000000000 bits:0 flags:1120
Mar 13 20:40:18 linbit1 kernel: [504809.947736] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: uuid_compare()=no-sync by rule=reconnected
Mar 13 20:40:18 linbit1 kernel: [504809.947756] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: State change 629517346: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:40:18 linbit1 kernel: [504809.947763] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 629517346 (36ms)
Mar 13 20:40:18 linbit1 kernel: [504809.947807] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:40:18 linbit1 kernel: [504809.947812] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: repl( Off -> Established )
Mar 13 20:40:18 linbit1 kernel: [504809.947887] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: cleared bm UUID and bitmap 1CE8657E0B4F4A9A:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:40:18 linbit1 kernel: [504809.949348] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: pdsk( Outdated -> UpToDate )
Mar 13 20:40:18 linbit1 kernel: [504809.967684] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: drbd_sync_handshake:
Mar 13 20:40:18 linbit1 kernel: [504809.967691] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: self 6E639916C4467B00:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:120
Mar 13 20:40:18 linbit1 kernel: [504809.967699] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: peer 6E639916C4467B00:0000000000000000:8E546FDABA594BCA:0000000000000000 bits:0 flags:1120
Mar 13 20:40:18 linbit1 kernel: [504809.967707] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: uuid_compare()=no-sync by rule=reconnected
Mar 13 20:40:18 linbit1 kernel: [504809.967796] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change 4102665490: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:40:18 linbit1 kernel: [504809.967805] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 4102665490 (44ms)
Mar 13 20:40:18 linbit1 kernel: [504809.967852] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:40:18 linbit1 kernel: [504809.967858] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: repl( Off -> Established )
Mar 13 20:40:18 linbit1 kernel: [504809.967937] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: cleared bm UUID and bitmap 6E639916C4467B00:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:40:18 linbit1 kernel: [504809.969344] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( Outdated -> UpToDate )
Mar 13 20:40:23 linbit1 kernel: [504814.069412] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit3: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:40:23 linbit1 kernel: [504814.073012] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030: receiver updated UUIDs to exposed data uuid: D57C92C0E41C71AC
Mar 13 20:40:23 linbit1 kernel: [504814.962279] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: updated UUIDs AE53567CC443078E:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:40:23 linbit1 kernel: [504814.962289] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: Clearing bitmap UUID for node 2
Mar 13 20:40:23 linbit1 kernel: [504814.962579] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:40:23 linbit1 kernel: [504814.962592] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:40:24 linbit1 kernel: [504815.271036] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit2: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:40:24 linbit1 kernel: [504815.274273] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026: receiver updated UUIDs to exposed data uuid: DD0C7B1BDAF9C81C
Mar 13 20:40:25 linbit1 kernel: [504816.410777] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:40:25 linbit1 kernel: [504816.410785] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: updated UUIDs 010864CFD47DAB04:0000000000000000:3C1CC23E444EC1D4:0000000000000000
Mar 13 20:40:25 linbit1 kernel: [504816.410794] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: disk( Inconsistent -> UpToDate )
Mar 13 20:40:25 linbit1 kernel: [504816.410796] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: repl( SyncTarget -> Established )
Mar 13 20:40:25 linbit1 kernel: [504816.410798] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit2: resync-susp( connection dependency -> no )
Mar 13 20:40:26 linbit1 kernel: [504817.355646] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: updated UUIDs 25005DEAB10165E0:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:40:26 linbit1 kernel: [504817.355661] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: Clearing bitmap UUID for node 2
Mar 13 20:40:26 linbit1 kernel: [504817.355997] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: Resync done (total 104 sec; paused 0 sec; 100824 K/sec)
Mar 13 20:40:26 linbit1 kernel: [504817.356018] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:40:27 linbit1 kernel: [504818.980504] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: updated UUIDs 65129730F932FD8A:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:40:27 linbit1 kernel: [504818.980515] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: Clearing bitmap UUID for node 2
Mar 13 20:40:27 linbit1 kernel: [504818.980822] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: Resync done (total 106 sec; paused 0 sec; 98920 K/sec)
Mar 13 20:40:27 linbit1 kernel: [504818.980837] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:40:28 linbit1 kernel: [504819.294086] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: updated UUIDs 49EB060E1205E2B4:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:40:28 linbit1 kernel: [504819.294429] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:40:28 linbit1 kernel: [504819.294447] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:40:30 linbit1 kernel: [504821.837748] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:40:30 linbit1 kernel: [504821.837762] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: updated UUIDs 2CB4450069C887E6:0000000000000000:DC5637792E5DB850:0000000000000000
Mar 13 20:40:30 linbit1 kernel: [504821.837776] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: disk( Inconsistent -> UpToDate )
Mar 13 20:40:30 linbit1 kernel: [504821.837780] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: repl( SyncTarget -> Established )
Mar 13 20:40:30 linbit1 kernel: [504821.837783] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit2: resync-susp( connection dependency -> no )
Mar 13 20:40:36 linbit1 kernel: [504827.555584] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: sock_recvmsg returned -11
Mar 13 20:40:36 linbit1 kernel: [504827.555604] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:40:36 linbit1 kernel: [504827.555613] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( Connected -> BrokenPipe ) peer( Primary -> Unknown )
Mar 13 20:40:36 linbit1 kernel: [504827.555618] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: disk( UpToDate -> Consistent )
Mar 13 20:40:36 linbit1 kernel: [504827.555623] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:40:36 linbit1 kernel: [504827.555869] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Terminating sender thread
Mar 13 20:40:36 linbit1 kernel: [504827.555913] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Starting sender thread (from drbd_r_pvc-654a [2446930])
Mar 13 20:40:36 linbit1 kernel: [504827.555933] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Preparing cluster-wide state change 986427049 (1->-1 0/0)
Mar 13 20:40:36 linbit1 kernel: [504827.556413] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: State change 986427049: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:40:36 linbit1 kernel: [504827.556420] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Committing cluster-wide state change 986427049 (0ms)
Mar 13 20:40:36 linbit1 kernel: [504827.556434] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: disk( Consistent -> Outdated )
Mar 13 20:40:36 linbit1 kernel: [504827.595825] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Connection closed
Mar 13 20:40:36 linbit1 kernel: [504827.595841] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 20:40:36 linbit1 kernel: [504827.595853] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Restarting receiver thread
Mar 13 20:40:36 linbit1 kernel: [504827.595863] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( Unconnected -> Connecting )
Mar 13 20:40:37 linbit1 kernel: [504828.163656] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:40:37 linbit1 kernel: [504828.163667] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:40:37 linbit1 kernel: [504828.163799] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:40:37 linbit1 kernel: [504828.318427] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Preparing remote state change 3693743227
Mar 13 20:40:37 linbit1 kernel: [504828.343625] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: drbd_sync_handshake:
Mar 13 20:40:37 linbit1 kernel: [504828.343633] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: self CACE1D3C53DDAC64:0000000000000000:C0DAB66176F1E84C:0000000000000000 bits:0 flags:120
Mar 13 20:40:37 linbit1 kernel: [504828.343642] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: peer CACE1D3C53DDAC64:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:1120
Mar 13 20:40:37 linbit1 kernel: [504828.343649] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: uuid_compare()=no-sync by rule=reconnected
Mar 13 20:40:37 linbit1 kernel: [504828.346570] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Committing remote state change 3693743227 (primary_nodes=1)
Mar 13 20:40:37 linbit1 kernel: [504828.346589] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( Connecting -> Connected ) peer( Unknown -> Primary )
Mar 13 20:40:37 linbit1 kernel: [504828.346594] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: disk( Outdated -> UpToDate )
Mar 13 20:40:37 linbit1 kernel: [504828.346599] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:40:37 linbit1 kernel: [504828.346715] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: cleared bm UUID and bitmap CACE1D3C53DDAC64:0000000000000000:C0DAB66176F1E84C:0000000000000000
Mar 13 20:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:40:40.347185    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:40:40.355932    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:40:42 linbit1 kernel: [504833.699580] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: sock_recvmsg returned -11
Mar 13 20:40:42 linbit1 kernel: [504833.699602] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:40:42 linbit1 kernel: [504833.699611] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( Connected -> BrokenPipe ) peer( Primary -> Unknown )
Mar 13 20:40:42 linbit1 kernel: [504833.699616] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: disk( UpToDate -> Consistent )
Mar 13 20:40:42 linbit1 kernel: [504833.699622] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:40:42 linbit1 kernel: [504833.699894] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Terminating sender thread
Mar 13 20:40:42 linbit1 kernel: [504833.699940] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Starting sender thread (from drbd_r_pvc-4ab0 [2448422])
Mar 13 20:40:42 linbit1 kernel: [504833.699963] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Preparing cluster-wide state change 4043920528 (1->-1 0/0)
Mar 13 20:40:42 linbit1 kernel: [504833.700365] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: State change 4043920528: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFF8
Mar 13 20:40:42 linbit1 kernel: [504833.700371] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Committing cluster-wide state change 4043920528 (0ms)
Mar 13 20:40:42 linbit1 kernel: [504833.700380] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: disk( Consistent -> Outdated )
Mar 13 20:40:42 linbit1 kernel: [504833.787808] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Connection closed
Mar 13 20:40:42 linbit1 kernel: [504833.787822] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:40:42 linbit1 kernel: [504833.787832] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Restarting receiver thread
Mar 13 20:40:42 linbit1 kernel: [504833.787840] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:40:43 linbit1 kernel: [504834.321766] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:40:43 linbit1 kernel: [504834.321777] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:40:43 linbit1 kernel: [504834.321924] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:40:43 linbit1 kernel: [504834.485668] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Preparing remote state change 832895807
Mar 13 20:40:43 linbit1 kernel: [504834.523613] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: drbd_sync_handshake:
Mar 13 20:40:43 linbit1 kernel: [504834.523619] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: self B08AE663A5D508EC:0000000000000000:9878D4B3AC0CDFE6:0000000000000000 bits:0 flags:120
Mar 13 20:40:43 linbit1 kernel: [504834.523628] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: peer B08AE663A5D508EC:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:1120
Mar 13 20:40:43 linbit1 kernel: [504834.523636] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: uuid_compare()=no-sync by rule=reconnected
Mar 13 20:40:43 linbit1 kernel: [504834.523823] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Committing remote state change 832895807 (primary_nodes=1)
Mar 13 20:40:43 linbit1 kernel: [504834.523843] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Primary )
Mar 13 20:40:43 linbit1 kernel: [504834.523848] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: disk( Outdated -> UpToDate )
Mar 13 20:40:43 linbit1 kernel: [504834.523853] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:40:43 linbit1 kernel: [504834.523966] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: cleared bm UUID and bitmap B08AE663A5D508EC:0000000000000000:9878D4B3AC0CDFE6:0000000000000000
Mar 13 20:40:46 linbit1 kernel: [504837.260766] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:40:46 linbit1 kernel: [504837.264520] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031: receiver updated UUIDs to exposed data uuid: AE7361DB2B2FCAB8
Mar 13 20:40:49 linbit1 kernel: [504840.547121] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: Resync done (total 105 sec; paused 0 sec; 99864 K/sec)
Mar 13 20:40:49 linbit1 kernel: [504840.547136] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: updated UUIDs C5D73FB57C5CBE18:0000000000000000:EB77D5EEDF720B64:0000000000000000
Mar 13 20:40:49 linbit1 kernel: [504840.547153] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( Inconsistent -> UpToDate )
Mar 13 20:40:49 linbit1 kernel: [504840.547158] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit3: resync-susp( connection dependency -> no )
Mar 13 20:40:49 linbit1 kernel: [504840.547162] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: repl( SyncTarget -> Established )
Mar 13 20:40:50 linbit1 kernel: [504841.694481] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: updated UUIDs 128417CE8BF51BA8:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:40:50 linbit1 kernel: [504841.694806] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: Resync done (total 106 sec; paused 0 sec; 98920 K/sec)
Mar 13 20:40:50 linbit1 kernel: [504841.694819] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: pdsk( Inconsistent -> UpToDate ) repl( SyncSource -> Established )
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.185779637Z" level=info msg="shim disconnected" id=a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.185879357Z" level=warning msg="cleaning up after shim disconnected" id=a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1 namespace=k8s.io
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.185906957Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:51 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1-rootfs.mount: Deactivated successfully.
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.203608251Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:51Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2462485 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:51 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba-rootfs.mount: Deactivated successfully.
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.479269006Z" level=info msg="shim disconnected" id=b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.479348966Z" level=warning msg="cleaning up after shim disconnected" id=b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba namespace=k8s.io
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.479371246Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.495369054Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:51Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2462524 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.667748816Z" level=info msg="shim disconnected" id=9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.667838977Z" level=warning msg="cleaning up after shim disconnected" id=9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc namespace=k8s.io
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.667865377Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:51 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc-rootfs.mount: Deactivated successfully.
Mar 13 20:40:51 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:51.685222469Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:51Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2462561 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.037817897Z" level=info msg="shim disconnected" id=4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.037899297Z" level=warning msg="cleaning up after shim disconnected" id=4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1 namespace=k8s.io
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.037925177Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:52 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1-rootfs.mount: Deactivated successfully.
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.045501080Z" level=info msg="CreateContainer within sandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.047188885Z" level=info msg="CreateContainer within sandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.048845330Z" level=info msg="CreateContainer within sandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.054047386Z" level=info msg="CreateContainer within sandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f\""
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.054766908Z" level=info msg="StartContainer for \"13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f\""
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.056514233Z" level=info msg="CreateContainer within sandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1\""
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.056772114Z" level=info msg="CreateContainer within sandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54\""
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.056935435Z" level=info msg="StartContainer for \"e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1\""
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.057134155Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:52Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2462613 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.057229876Z" level=info msg="StartContainer for \"b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54\""
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.155086172Z" level=info msg="StartContainer for \"e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1\" returns successfully"
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.155086652Z" level=info msg="StartContainer for \"13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f\" returns successfully"
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.155930294Z" level=info msg="StartContainer for \"b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54\" returns successfully"
Mar 13 20:40:52 linbit1 kernel: [504843.542645] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit2: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:40:52 linbit1 kernel: [504843.543342] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035: receiver updated UUIDs to exposed data uuid: 3A856CD82155D872
Mar 13 20:40:52 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1-rootfs.mount: Deactivated successfully.
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.591980695Z" level=info msg="shim disconnected" id=8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.592071055Z" level=warning msg="cleaning up after shim disconnected" id=8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1 namespace=k8s.io
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.592092215Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:52.603513650Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:52Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2462810 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.008689196Z" level=info msg="shim disconnected" id=914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.008773197Z" level=warning msg="cleaning up after shim disconnected" id=914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0 namespace=k8s.io
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.008800237Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.026798811Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:53Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2462884 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.053614372Z" level=info msg="CreateContainer within sandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.060131952Z" level=info msg="CreateContainer within sandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.060679634Z" level=info msg="CreateContainer within sandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99\""
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.060965035Z" level=info msg="StartContainer for \"4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99\""
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.062521799Z" level=info msg="CreateContainer within sandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.067505614Z" level=info msg="CreateContainer within sandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011\""
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.068042896Z" level=info msg="StartContainer for \"516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011\""
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.134065736Z" level=info msg="StartContainer for \"4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99\" returns successfully"
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.138397789Z" level=info msg="CreateContainer within sandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23\""
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.138947671Z" level=info msg="StartContainer for \"7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23\""
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.139448312Z" level=info msg="StartContainer for \"516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011\" returns successfully"
Mar 13 20:40:53 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0-rootfs.mount: Deactivated successfully.
Mar 13 20:40:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:53.252916176Z" level=info msg="StartContainer for \"7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23\" returns successfully"
Mar 13 20:40:53 linbit1 kernel: [504844.851339] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit2: pdsk( Inconsistent -> UpToDate ) resync-susp( peer -> no )
Mar 13 20:40:53 linbit1 kernel: [504844.854713] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032: receiver updated UUIDs to exposed data uuid: EF73B8240ED384C2
Mar 13 20:40:53 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:40:53.864335    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n3-0-zq6bl" podStartSLOduration=-9.223371828990528e+09 pod.CreationTimestamp="2023-03-13 20:37:26 +0000 UTC" firstStartedPulling="2023-03-13 20:39:09.271577284 +0000 UTC m=+504519.429042816" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:40:53.862661102 +0000 UTC m=+504624.020126674" watchObservedRunningTime="2023-03-13 20:40:53.864248267 +0000 UTC m=+504624.021713839"
Mar 13 20:40:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291-rootfs.mount: Deactivated successfully.
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.077623233Z" level=info msg="shim disconnected" id=8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.077717873Z" level=warning msg="cleaning up after shim disconnected" id=8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291 namespace=k8s.io
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.077736793Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.091306314Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:54Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463076 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe-rootfs.mount: Deactivated successfully.
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.259939905Z" level=info msg="shim disconnected" id=a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.261243189Z" level=warning msg="cleaning up after shim disconnected" id=a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe namespace=k8s.io
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.261278749Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.282986615Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:54Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463110 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c-rootfs.mount: Deactivated successfully.
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.416553179Z" level=info msg="shim disconnected" id=738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.416640379Z" level=warning msg="cleaning up after shim disconnected" id=738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c namespace=k8s.io
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.416668979Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.435359916Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:54Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463154 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe-rootfs.mount: Deactivated successfully.
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.516773922Z" level=info msg="shim disconnected" id=e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.516842163Z" level=warning msg="cleaning up after shim disconnected" id=e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe namespace=k8s.io
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.516858363Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.534170095Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:54Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463175 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:40:54.660665    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n7-4-ckbx6" podStartSLOduration=-9.223371830194155e+09 pod.CreationTimestamp="2023-03-13 20:37:28 +0000 UTC" firstStartedPulling="2023-03-13 20:38:37.237655586 +0000 UTC m=+504487.395121118" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:40:54.658922113 +0000 UTC m=+504624.816387645" watchObservedRunningTime="2023-03-13 20:40:54.660621598 +0000 UTC m=+504624.818087090"
Mar 13 20:40:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd-rootfs.mount: Deactivated successfully.
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.759484737Z" level=info msg="shim disconnected" id=2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.759553578Z" level=warning msg="cleaning up after shim disconnected" id=2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd namespace=k8s.io
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.759568058Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:54.777766873Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:54Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463207 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.079855147Z" level=info msg="CreateContainer within sandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.081845593Z" level=info msg="CreateContainer within sandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.084051800Z" level=info msg="CreateContainer within sandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.085889686Z" level=info msg="CreateContainer within sandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.088294533Z" level=info msg="CreateContainer within sandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.088626054Z" level=info msg="CreateContainer within sandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.088687894Z" level=info msg="StartContainer for \"08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.091540823Z" level=info msg="CreateContainer within sandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.092070664Z" level=info msg="StartContainer for \"9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.093722949Z" level=info msg="CreateContainer within sandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.094208791Z" level=info msg="StartContainer for \"04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.095703315Z" level=info msg="CreateContainer within sandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.096232517Z" level=info msg="StartContainer for \"5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.097808882Z" level=info msg="CreateContainer within sandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.098276603Z" level=info msg="StartContainer for \"11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec\""
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.191902127Z" level=info msg="StartContainer for \"11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec\" returns successfully"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.193165250Z" level=info msg="StartContainer for \"08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a\" returns successfully"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.193706252Z" level=info msg="StartContainer for \"9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf\" returns successfully"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.197242183Z" level=info msg="StartContainer for \"5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7\" returns successfully"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.205865009Z" level=info msg="StartContainer for \"04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086\" returns successfully"
Mar 13 20:40:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928-rootfs.mount: Deactivated successfully.
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.281584558Z" level=info msg="shim disconnected" id=716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.281645318Z" level=warning msg="cleaning up after shim disconnected" id=716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928 namespace=k8s.io
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.281659198Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.298935131Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463395 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65-rootfs.mount: Deactivated successfully.
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.408610463Z" level=info msg="shim disconnected" id=dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.408681703Z" level=warning msg="cleaning up after shim disconnected" id=dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65 namespace=k8s.io
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.408702623Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.427291439Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463441 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29-rootfs.mount: Deactivated successfully.
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.706140324Z" level=info msg="shim disconnected" id=a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.706219724Z" level=warning msg="cleaning up after shim disconnected" id=a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29 namespace=k8s.io
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.706243684Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.724218218Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463474 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c-rootfs.mount: Deactivated successfully.
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.896118659Z" level=info msg="shim disconnected" id=1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.896202579Z" level=warning msg="cleaning up after shim disconnected" id=1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c namespace=k8s.io
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.896225299Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.896239459Z" level=info msg="shim disconnected" id=93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.896306299Z" level=warning msg="cleaning up after shim disconnected" id=93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e namespace=k8s.io
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.896325139Z" level=info msg="cleaning up dead shim"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.914330434Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463524 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:55.915320397Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:40:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2463525 runtime=io.containerd.runc.v2\n"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.099900396Z" level=info msg="CreateContainer within sandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.101519961Z" level=info msg="CreateContainer within sandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.111242150Z" level=info msg="CreateContainer within sandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.111989952Z" level=info msg="CreateContainer within sandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.112436154Z" level=info msg="StartContainer for \"6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.113343877Z" level=info msg="CreateContainer within sandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.114698641Z" level=info msg="CreateContainer within sandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.115111322Z" level=info msg="StartContainer for \"d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.115229842Z" level=info msg="CreateContainer within sandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" for container &ContainerMetadata{Name:fio-bench,Attempt:0,}"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.119550735Z" level=info msg="CreateContainer within sandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.120122417Z" level=info msg="StartContainer for \"dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.121840222Z" level=info msg="CreateContainer within sandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.122815665Z" level=info msg="StartContainer for \"795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.126118275Z" level=info msg="CreateContainer within sandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" for &ContainerMetadata{Name:fio-bench,Attempt:0,} returns container id \"d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.126600397Z" level=info msg="StartContainer for \"d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c\""
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.230429151Z" level=info msg="StartContainer for \"dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a\" returns successfully"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.230431871Z" level=info msg="StartContainer for \"6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7\" returns successfully"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.230449951Z" level=info msg="StartContainer for \"795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b\" returns successfully"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.230978833Z" level=info msg="StartContainer for \"d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c\" returns successfully"
Mar 13 20:40:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:40:56.231370954Z" level=info msg="StartContainer for \"d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e\" returns successfully"
Mar 13 20:40:56 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e-rootfs.mount: Deactivated successfully.
Mar 13 20:40:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:40:57.078093    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n1-1-vjkzb" podStartSLOduration=-9.223371825776762e+09 pod.CreationTimestamp="2023-03-13 20:37:26 +0000 UTC" firstStartedPulling="2023-03-13 20:38:06.684723726 +0000 UTC m=+504456.842189218" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:40:57.076240592 +0000 UTC m=+504627.233706124" watchObservedRunningTime="2023-03-13 20:40:57.078013677 +0000 UTC m=+504627.235479209"
Mar 13 20:40:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:40:57.875583    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n4-4-mmrrc" podStartSLOduration=-9.22337182597926e+09 pod.CreationTimestamp="2023-03-13 20:37:27 +0000 UTC" firstStartedPulling="2023-03-13 20:38:25.479179204 +0000 UTC m=+504475.636644736" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:40:57.873689129 +0000 UTC m=+504628.031154701" watchObservedRunningTime="2023-03-13 20:40:57.875516415 +0000 UTC m=+504628.032981947"
Mar 13 20:40:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:40:58.670542    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n0-0-pml7f" podStartSLOduration=-9.223371823184301e+09 pod.CreationTimestamp="2023-03-13 20:37:25 +0000 UTC" firstStartedPulling="2023-03-13 20:38:00.687016923 +0000 UTC m=+504450.844482455" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:40:58.669843631 +0000 UTC m=+504628.827309203" watchObservedRunningTime="2023-03-13 20:40:58.670474274 +0000 UTC m=+504628.827939766"
Mar 13 20:40:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:40:59.466188    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n0-3-bh94n" podStartSLOduration=-9.223371822388649e+09 pod.CreationTimestamp="2023-03-13 20:37:25 +0000 UTC" firstStartedPulling="2023-03-13 20:38:06.187057582 +0000 UTC m=+504456.344523074" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:40:59.465261761 +0000 UTC m=+504629.622727333" watchObservedRunningTime="2023-03-13 20:40:59.466127525 +0000 UTC m=+504629.623593017"
Mar 13 20:41:02 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:02.665507    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n1-3-hntjb" podStartSLOduration=-9.223371820189339e+09 pod.CreationTimestamp="2023-03-13 20:37:26 +0000 UTC" firstStartedPulling="2023-03-13 20:38:00.415453983 +0000 UTC m=+504450.572919515" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:02.662702175 +0000 UTC m=+504632.820167707" watchObservedRunningTime="2023-03-13 20:41:02.665437627 +0000 UTC m=+504632.822903159"
Mar 13 20:41:03 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:03.466047    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n0-1-559g2" podStartSLOduration=-9.223371818388796e+09 pod.CreationTimestamp="2023-03-13 20:37:25 +0000 UTC" firstStartedPulling="2023-03-13 20:38:05.811510447 +0000 UTC m=+504455.968975939" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:03.464258285 +0000 UTC m=+504633.621723777" watchObservedRunningTime="2023-03-13 20:41:03.465979572 +0000 UTC m=+504633.623445104"
Mar 13 20:41:04 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:04.265231    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n8-4-vfll8" podStartSLOduration=-9.22337182058961e+09 pod.CreationTimestamp="2023-03-13 20:37:28 +0000 UTC" firstStartedPulling="2023-03-13 20:39:07.152947037 +0000 UTC m=+504517.310412569" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:04.262233257 +0000 UTC m=+504634.419698789" watchObservedRunningTime="2023-03-13 20:41:04.26516743 +0000 UTC m=+504634.422632962"
Mar 13 20:41:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:05.073741    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n0-2-hmskv" podStartSLOduration=-9.223371816781244e+09 pod.CreationTimestamp="2023-03-13 20:37:25 +0000 UTC" firstStartedPulling="2023-03-13 20:38:04.241414543 +0000 UTC m=+504454.398880115" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:05.07209628 +0000 UTC m=+504635.229561812" watchObservedRunningTime="2023-03-13 20:41:05.073530926 +0000 UTC m=+504635.230996418"
Mar 13 20:41:05 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:05.863759    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n7-3-mfqd4" podStartSLOduration=-9.223371818991081e+09 pod.CreationTimestamp="2023-03-13 20:37:28 +0000 UTC" firstStartedPulling="2023-03-13 20:38:35.874266425 +0000 UTC m=+504486.031731957" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:05.862387536 +0000 UTC m=+504636.019853108" watchObservedRunningTime="2023-03-13 20:41:05.863695062 +0000 UTC m=+504636.021160634"
Mar 13 20:41:06 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:06.471203    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n8-1-5nnlm" podStartSLOduration=-9.223371818383635e+09 pod.CreationTimestamp="2023-03-13 20:37:28 +0000 UTC" firstStartedPulling="2023-03-13 20:39:07.087757759 +0000 UTC m=+504517.245223291" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:06.468305434 +0000 UTC m=+504636.625770966" watchObservedRunningTime="2023-03-13 20:41:06.471141127 +0000 UTC m=+504636.628606659"
Mar 13 20:41:06 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:06.866468    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n7-2-bnfd5" podStartSLOduration=-9.223371817988373e+09 pod.CreationTimestamp="2023-03-13 20:37:28 +0000 UTC" firstStartedPulling="2023-03-13 20:38:33.325608401 +0000 UTC m=+504483.483073973" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:06.865180749 +0000 UTC m=+504637.022646281" watchObservedRunningTime="2023-03-13 20:41:06.866402754 +0000 UTC m=+504637.023868286"
Mar 13 20:41:07 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:07.266445    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n8-3-8hwhw" podStartSLOduration=-9.223371817588387e+09 pod.CreationTimestamp="2023-03-13 20:37:28 +0000 UTC" firstStartedPulling="2023-03-13 20:39:06.776068017 +0000 UTC m=+504516.933533549" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:07.265343558 +0000 UTC m=+504637.422809130" watchObservedRunningTime="2023-03-13 20:41:07.266389762 +0000 UTC m=+504637.423855254"
Mar 13 20:41:07 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:07.667043    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n5-3-5zdjt" podStartSLOduration=-9.223371816187798e+09 pod.CreationTimestamp="2023-03-13 20:37:27 +0000 UTC" firstStartedPulling="2023-03-13 20:39:22.668158881 +0000 UTC m=+504532.825624413" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:07.664948843 +0000 UTC m=+504637.822414375" watchObservedRunningTime="2023-03-13 20:41:07.666978332 +0000 UTC m=+504637.824443864"
Mar 13 20:41:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:41:08.068991    2448 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/fio-bench-r2-n0-4-9lr25" podStartSLOduration=-9.223371814785917e+09 pod.CreationTimestamp="2023-03-13 20:37:26 +0000 UTC" firstStartedPulling="2023-03-13 20:38:01.879758327 +0000 UTC m=+504452.037223899" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-03-13 20:41:08.066653538 +0000 UTC m=+504638.224119110" watchObservedRunningTime="2023-03-13 20:41:08.068857987 +0000 UTC m=+504638.226323599"
Mar 13 20:41:39 linbit1 kernel: [504890.653442] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: sock was shut down by peer
Mar 13 20:41:39 linbit1 kernel: [504890.653458] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:41:39 linbit1 kernel: [504890.653466] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( Connected -> BrokenPipe ) peer( Primary -> Unknown )
Mar 13 20:41:39 linbit1 kernel: [504890.653470] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( UpToDate -> Consistent )
Mar 13 20:41:39 linbit1 kernel: [504890.653474] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:41:39 linbit1 kernel: [504890.653785] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Preparing cluster-wide state change 1422540288 (1->-1 0/0)
Mar 13 20:41:39 linbit1 kernel: [504890.653893] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Terminating sender thread
Mar 13 20:41:39 linbit1 kernel: [504890.653929] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Starting sender thread (from drbd_r_pvc-329d [2454393])
Mar 13 20:41:39 linbit1 kernel: [504890.654358] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: State change 1422540288: primary_nodes=1, weak_nodes=FFFFFFFFFFFFFFFA
Mar 13 20:41:39 linbit1 kernel: [504890.654362] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Committing cluster-wide state change 1422540288 (0ms)
Mar 13 20:41:39 linbit1 kernel: [504890.654373] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( Consistent -> Outdated )
Mar 13 20:41:39 linbit1 kernel: [504890.735919] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Connection closed
Mar 13 20:41:39 linbit1 kernel: [504890.735934] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:41:39 linbit1 kernel: [504890.735944] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Restarting receiver thread
Mar 13 20:41:39 linbit1 kernel: [504890.736229] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( Unconnected -> Connecting )
Mar 13 20:41:40 linbit1 kernel: [504891.267439] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:41:40 linbit1 kernel: [504891.267448] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:41:40 linbit1 kernel: [504891.267628] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:41:40 linbit1 kernel: [504891.429492] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Preparing remote state change 1893524803
Mar 13 20:41:40 linbit1 kernel: [504891.471637] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: drbd_sync_handshake:
Mar 13 20:41:40 linbit1 kernel: [504891.471650] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: self C5D73FB57C5CBE18:0000000000000000:EB77D5EEDF720B64:0000000000000000 bits:0 flags:120
Mar 13 20:41:40 linbit1 kernel: [504891.471659] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: peer 149D0026E5FEAA63:C5D73FB57C5CBE18:0000000000000000:0000000000000000 bits:31072 flags:1120
Mar 13 20:41:40 linbit1 kernel: [504891.471667] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: uuid_compare()=target-use-bitmap by rule=bitmap-peer
Mar 13 20:41:40 linbit1 kernel: [504891.481665] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Committing remote state change 1893524803 (primary_nodes=1)
Mar 13 20:41:40 linbit1 kernel: [504891.481683] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( Connecting -> Connected ) peer( Unknown -> Primary )
Mar 13 20:41:40 linbit1 kernel: [504891.481687] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> WFBitMapT )
Mar 13 20:41:40 linbit1 kernel: [504891.550592] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: receive bitmap stats [Bytes(packets)]: plain 0(0), RLE 30(1), total 30; compression: 100.0%
Mar 13 20:41:40 linbit1 kernel: [504891.552318] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: send bitmap stats [Bytes(packets)]: plain 0(0), RLE 30(1), total 30; compression: 100.0%
Mar 13 20:41:40 linbit1 kernel: [504891.552346] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( Outdated -> Inconsistent )
Mar 13 20:41:40 linbit1 kernel: [504891.552351] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit3: resync-susp( no -> connection dependency )
Mar 13 20:41:40 linbit1 kernel: [504891.552355] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: repl( WFBitMapT -> SyncTarget )
Mar 13 20:41:40 linbit1 kernel: [504891.552829] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: Began resync as SyncTarget (will sync 147072 KB [36768 bits set]).
Mar 13 20:42:49 linbit1 kernel: [504960.209116] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: Resync done (total 68 sec; paused 0 sec; 2164 K/sec)
Mar 13 20:42:49 linbit1 kernel: [504960.209130] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: updated UUIDs 149D0026E5FEAA62:0000000000000000:EB77D5EEDF720B64:0000000000000000
Mar 13 20:42:49 linbit1 kernel: [504960.209144] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( Inconsistent -> UpToDate )
Mar 13 20:42:49 linbit1 kernel: [504960.209148] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit3: resync-susp( connection dependency -> no )
Mar 13 20:42:49 linbit1 kernel: [504960.209151] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: repl( SyncTarget -> Established )
Mar 13 20:43:13 linbit1 kernel: [504984.187550] {1}[Hardware Error]: Hardware error from APEI Generic Hardware Error Source: 2
Mar 13 20:43:13 linbit1 kernel: [504984.187558] {1}[Hardware Error]: It has been corrected by h/w and requires no further action
Mar 13 20:43:13 linbit1 kernel: [504984.187561] {1}[Hardware Error]: event severity: corrected
Mar 13 20:43:13 linbit1 kernel: [504984.187563] {1}[Hardware Error]:  Error 0, type: corrected
Mar 13 20:43:13 linbit1 kernel: [504984.187566] {1}[Hardware Error]:   section_type: memory error
Mar 13 20:43:13 linbit1 kernel: [504984.187568] {1}[Hardware Error]:   error_status: 0x0000000000000400
Mar 13 20:43:13 linbit1 kernel: [504984.187571] {1}[Hardware Error]:   physical_address: 0x0000083ee7e78e40
Mar 13 20:43:13 linbit1 kernel: [504984.187576] {1}[Hardware Error]:   node: 0 card: 2 module: 0 rank: 0 bank: 3 row: 128831 column: 568 
Mar 13 20:43:13 linbit1 kernel: [504984.187578] {1}[Hardware Error]:   error_type: 2, single-bit ECC
Mar 13 20:43:13 linbit1 kernel: [504984.187581] {1}[Hardware Error]:  Error 1, type: corrected
Mar 13 20:43:13 linbit1 kernel: [504984.187584] {1}[Hardware Error]:   section type: unknown, e8ed898d-df16-43cc-8ecc-54f060ef157f
Mar 13 20:43:13 linbit1 kernel: [504984.187587] {1}[Hardware Error]:   section length: 0x30
Mar 13 20:43:13 linbit1 kernel: [504984.187591] {1}[Hardware Error]:   00000000: 00020101 c500000c e7e78e40 0000083e  ........@...>...
Mar 13 20:43:13 linbit1 kernel: [504984.187595] {1}[Hardware Error]:   00000010: 87dcfc47 810000b3 00000001 00000000  G...............
Mar 13 20:43:13 linbit1 kernel: [504984.187599] {1}[Hardware Error]:   00000020: 00000000 00000000 00000000 00000000  ................
Mar 13 20:43:13 linbit1 kernel: [504984.187609] EDAC MC0: 1 CE Single-bit ECC on unknown memory (node:0 card:2 module:0 rank:0 bank:3 row:128831 col:568 page:0x83ee7e78 offset:0xe40 grain:1 syndrome:0x0 - APEI location: node:0 card:2 module:0 rank:0 bank:3 row:128831 col:568 status(0x0000000000000400): Storage error in DRAM memory)
Mar 13 20:43:13 linbit1 kernel: [504984.195127] Memory failure: 0x83ee7e78: unhandlable page.
Mar 13 20:44:37 linbit1 snapd[2460]: storehelpers.go:769: cannot refresh: snap has no updates available: "core18", "core20", "lxd", "microk8s", "snapd"
Mar 13 20:44:37 linbit1 snapd[2460]: autorefresh.go:551: auto-refresh: all snaps are up-to-date
Mar 13 20:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:45:40.345299    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:45:40.354729    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:45:52 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54-rootfs.mount: Deactivated successfully.
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.526980274Z" level=info msg="shim disconnected" id=b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.527077834Z" level=warning msg="cleaning up after shim disconnected" id=b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54 namespace=k8s.io
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.527097474Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.545956907Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:52Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467068 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:52 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1-rootfs.mount: Deactivated successfully.
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.680526743Z" level=info msg="shim disconnected" id=e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.680710183Z" level=warning msg="cleaning up after shim disconnected" id=e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1 namespace=k8s.io
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.680729303Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.694618397Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:52Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467131 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.792506732Z" level=info msg="shim disconnected" id=13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.792602372Z" level=warning msg="cleaning up after shim disconnected" id=13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f namespace=k8s.io
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.792619293Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:52 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:52.808154832Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:52Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467162 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:53 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f-rootfs.mount: Deactivated successfully.
Mar 13 20:45:53 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99-rootfs.mount: Deactivated successfully.
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.546167782Z" level=info msg="shim disconnected" id=4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.548484711Z" level=warning msg="cleaning up after shim disconnected" id=4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99 namespace=k8s.io
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.548719512Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.567567344Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:53Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467200 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:53 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011-rootfs.mount: Deactivated successfully.
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.584623489Z" level=info msg="shim disconnected" id=516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.585061331Z" level=warning msg="cleaning up after shim disconnected" id=516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011 namespace=k8s.io
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.585337412Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.603565122Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:53Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467218 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.706910758Z" level=info msg="shim disconnected" id=7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.707013239Z" level=warning msg="cleaning up after shim disconnected" id=7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23 namespace=k8s.io
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.707034039Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:53 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23-rootfs.mount: Deactivated successfully.
Mar 13 20:45:53 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:53.724172224Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:53Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467244 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.074769569Z" level=info msg="StopPodSandbox for \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.075177570Z" level=info msg="Container to stop \"e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.075367371Z" level=info msg="Container to stop \"b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.074800289Z" level=info msg="StopPodSandbox for \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.074908849Z" level=info msg="StopPodSandbox for \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.075765852Z" level=info msg="Container to stop \"13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.075792532Z" level=info msg="Container to stop \"a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.075817133Z" level=info msg="Container to stop \"9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.075844573Z" level=info msg="Container to stop \"b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6-shm.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557-shm.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2-shm.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.136316444Z" level=info msg="shim disconnected" id=71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.136386245Z" level=warning msg="cleaning up after shim disconnected" id=71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2 namespace=k8s.io
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.136403765Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.151844504Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:54Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467310 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6-rootfs.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557-rootfs.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2-rootfs.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.232613454Z" level=info msg="shim disconnected" id=a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.232676694Z" level=warning msg="cleaning up after shim disconnected" id=a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6 namespace=k8s.io
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.232693254Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.250728243Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:54Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467369 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:54 linbit1 systemd-networkd[2297221]: calid3c598300ec: Link DOWN
Mar 13 20:45:54 linbit1 systemd-networkd[2297221]: calid3c598300ec: Lost carrier
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.273308930Z" level=info msg="shim disconnected" id=f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.273362730Z" level=warning msg="cleaning up after shim disconnected" id=f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557 namespace=k8s.io
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.273375330Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.291235758Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:54Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467403 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:54 linbit1 systemd-networkd[2297221]: cali97811183456: Link DOWN
Mar 13 20:45:54 linbit1 systemd-networkd[2297221]: cali97811183456: Lost carrier
Mar 13 20:45:54 linbit1 systemd-networkd[2297221]: cali03e68263c68: Link DOWN
Mar 13 20:45:54 linbit1 systemd-networkd[2297221]: cali03e68263c68: Lost carrier
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.249 [INFO][2467344] k8s.go 576: Cleaning up netns ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.249 [INFO][2467344] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" iface="eth0" netns="/var/run/netns/cni-8dc04d64-86e4-a324-ac84-e5b77a4df571"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.249 [INFO][2467344] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" iface="eth0" netns="/var/run/netns/cni-8dc04d64-86e4-a324-ac84-e5b77a4df571"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.383 [INFO][2467344] dataplane_linux.go 549: Deleted device in netns. ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" after=134.228474ms iface="eth0" netns="/var/run/netns/cni-8dc04d64-86e4-a324-ac84-e5b77a4df571"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.383 [INFO][2467344] k8s.go 583: Releasing IP address(es) ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.384 [INFO][2467344] utils.go 196: Calico CNI releasing IP address ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.416 [INFO][2467479] ipam_plugin.go 411: Releasing address using handleID ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.539 [INFO][2467479] ipam_plugin.go 430: Released address using handleID ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.539 [INFO][2467479] ipam_plugin.go 439: Releasing address using workloadID ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.549 [INFO][2467344] k8s.go 589: Teardown processing complete. ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.552625600Z" level=info msg="TearDown network for sandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" successfully"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.552681000Z" level=info msg="StopPodSandbox for \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" returns successfully"
Mar 13 20:45:54 linbit1 systemd[1]: run-netns-cni\x2d8dc04d64\x2d86e4\x2da324\x2dac84\x2de5b77a4df571.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 kernel: [505145.633500] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Preparing remote state change 1071917505
Mar 13 20:45:54 linbit1 kernel: [505145.634424] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Committing remote state change 1071917505 (primary_nodes=0)
Mar 13 20:45:54 linbit1 kernel: [505145.634453] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: peer( Primary -> Secondary )
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.660225    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\") pod \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\" (UID: \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\") "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.660293    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-5v499\" (UniqueName: \"kubernetes.io/projected/f15c9220-13bf-47a1-93d8-bf5be8a9a7a5-kube-api-access-5v499\") pod \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\" (UID: \"f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\") "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.664607    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f15c9220-13bf-47a1-93d8-bf5be8a9a7a5-kube-api-access-5v499" (OuterVolumeSpecName: "kube-api-access-5v499") pod "f15c9220-13bf-47a1-93d8-bf5be8a9a7a5" (UID: "f15c9220-13bf-47a1-93d8-bf5be8a9a7a5"). InnerVolumeSpecName "kube-api-access-5v499". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:54 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-f15c9220\x2d13bf\x2d47a1\x2d93d8\x2dbf5be8a9a7a5-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2d5v499.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 kernel: [505145.710565] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 2543767119 (0->-1 3/2)
Mar 13 20:45:54 linbit1 kernel: [505145.710876] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: State change 2543767119: primary_nodes=0, weak_nodes=0
Mar 13 20:45:54 linbit1 kernel: [505145.710882] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 2543767119 (0ms)
Mar 13 20:45:54 linbit1 kernel: [505145.710912] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: role( Primary -> Secondary )
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.382 [INFO][2467449] k8s.go 576: Cleaning up netns ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.383 [INFO][2467449] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" iface="eth0" netns="/var/run/netns/cni-f6f236d0-395a-0a5a-55aa-b90d93cf87eb"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.383 [INFO][2467449] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" iface="eth0" netns="/var/run/netns/cni-f6f236d0-395a-0a5a-55aa-b90d93cf87eb"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.488 [INFO][2467449] dataplane_linux.go 549: Deleted device in netns. ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" after=104.996202ms iface="eth0" netns="/var/run/netns/cni-f6f236d0-395a-0a5a-55aa-b90d93cf87eb"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.488 [INFO][2467449] k8s.go 583: Releasing IP address(es) ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.488 [INFO][2467449] utils.go 196: Calico CNI releasing IP address ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.519 [INFO][2467507] ipam_plugin.go 411: Releasing address using handleID ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.686 [INFO][2467507] ipam_plugin.go 430: Released address using handleID ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.686 [INFO][2467507] ipam_plugin.go 439: Releasing address using workloadID ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.696 [INFO][2467449] k8s.go 589: Teardown processing complete. ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.700718248Z" level=info msg="TearDown network for sandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" successfully"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.700769848Z" level=info msg="StopPodSandbox for \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" returns successfully"
Mar 13 20:45:54 linbit1 systemd[1]: run-netns-cni\x2df6f236d0\x2d395a\x2d0a5a\x2d55aa\x2db90d93cf87eb.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d20f0b4c5\x2d4f16\x2d4b65\x2da52c\x2dfb6c96b56cbc-dev-f15c9220\x2d13bf\x2d47a1\x2d93d8\x2dbf5be8a9a7a5.mount: Deactivated successfully.
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.712997    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc" (OuterVolumeSpecName: "linstor-vol") pod "f15c9220-13bf-47a1-93d8-bf5be8a9a7a5" (UID: "f15c9220-13bf-47a1-93d8-bf5be8a9a7a5"). InnerVolumeSpecName "pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:54 linbit1 kernel: [505145.737572] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Preparing remote state change 3350668810
Mar 13 20:45:54 linbit1 kernel: [505145.737841] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Committing remote state change 3350668810 (primary_nodes=0)
Mar 13 20:45:54 linbit1 kernel: [505145.737869] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: peer( Primary -> Secondary )
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.761637    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\") pod \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\" (UID: \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\") "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.761773    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-6mgjs\" (UniqueName: \"kubernetes.io/projected/05dd1efc-8486-47dd-89df-7ee739a2d7e8-kube-api-access-6mgjs\") pod \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\" (UID: \"05dd1efc-8486-47dd-89df-7ee739a2d7e8\") "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.761935    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-5v499\" (UniqueName: \"kubernetes.io/projected/f15c9220-13bf-47a1-93d8-bf5be8a9a7a5-kube-api-access-5v499\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.762077    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\") on node \"linbit1\" "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.762677    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc") on node "linbit1"
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.766293    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/05dd1efc-8486-47dd-89df-7ee739a2d7e8-kube-api-access-6mgjs" (OuterVolumeSpecName: "kube-api-access-6mgjs") pod "05dd1efc-8486-47dd-89df-7ee739a2d7e8" (UID: "05dd1efc-8486-47dd-89df-7ee739a2d7e8"). InnerVolumeSpecName "kube-api-access-6mgjs". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:54 linbit1 kernel: [505145.818548] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 2924971178 (0->-1 3/2)
Mar 13 20:45:54 linbit1 kernel: [505145.818866] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: State change 2924971178: primary_nodes=0, weak_nodes=0
Mar 13 20:45:54 linbit1 kernel: [505145.818872] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 2924971178 (0ms)
Mar 13 20:45:54 linbit1 kernel: [505145.818902] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: role( Primary -> Secondary )
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.820014    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa" (OuterVolumeSpecName: "linstor-vol") pod "05dd1efc-8486-47dd-89df-7ee739a2d7e8" (UID: "05dd1efc-8486-47dd-89df-7ee739a2d7e8"). InnerVolumeSpecName "pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.363 [INFO][2467402] k8s.go 576: Cleaning up netns ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.363 [INFO][2467402] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" iface="eth0" netns="/var/run/netns/cni-3826e5ef-1983-1ac5-a390-ccf3736b6eef"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.363 [INFO][2467402] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" iface="eth0" netns="/var/run/netns/cni-3826e5ef-1983-1ac5-a390-ccf3736b6eef"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.527 [INFO][2467402] dataplane_linux.go 549: Deleted device in netns. ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" after=164.210589ms iface="eth0" netns="/var/run/netns/cni-3826e5ef-1983-1ac5-a390-ccf3736b6eef"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.527 [INFO][2467402] k8s.go 583: Releasing IP address(es) ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.528 [INFO][2467402] utils.go 196: Calico CNI releasing IP address ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.560 [INFO][2467525] ipam_plugin.go 411: Releasing address using handleID ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.808 [INFO][2467525] ipam_plugin.go 430: Released address using handleID ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.808 [INFO][2467525] ipam_plugin.go 439: Releasing address using workloadID ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:54.818 [INFO][2467402] k8s.go 589: Teardown processing complete. ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.822500154Z" level=info msg="TearDown network for sandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" successfully"
Mar 13 20:45:54 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:54.822551195Z" level=info msg="StopPodSandbox for \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" returns successfully"
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.863080    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\") pod \"23c28465-eb55-4375-9bd1-5fd757745f89\" (UID: \"23c28465-eb55-4375-9bd1-5fd757745f89\") "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.863163    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-cltvg\" (UniqueName: \"kubernetes.io/projected/23c28465-eb55-4375-9bd1-5fd757745f89-kube-api-access-cltvg\") pod \"23c28465-eb55-4375-9bd1-5fd757745f89\" (UID: \"23c28465-eb55-4375-9bd1-5fd757745f89\") "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.863296    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/f15c9220-13bf-47a1-93d8-bf5be8a9a7a5\""
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.863413    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\") on node \"linbit1\" "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.863442    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-6mgjs\" (UniqueName: \"kubernetes.io/projected/05dd1efc-8486-47dd-89df-7ee739a2d7e8-kube-api-access-6mgjs\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.864019    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa") on node "linbit1"
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.867375    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/23c28465-eb55-4375-9bd1-5fd757745f89-kube-api-access-cltvg" (OuterVolumeSpecName: "kube-api-access-cltvg") pod "23c28465-eb55-4375-9bd1-5fd757745f89" (UID: "23c28465-eb55-4375-9bd1-5fd757745f89"). InnerVolumeSpecName "kube-api-access-cltvg". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:54 linbit1 kernel: [505145.910501] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 53120589 (0->-1 3/2)
Mar 13 20:45:54 linbit1 kernel: [505145.910847] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: State change 53120589: primary_nodes=0, weak_nodes=0
Mar 13 20:45:54 linbit1 kernel: [505145.910852] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 53120589 (0ms)
Mar 13 20:45:54 linbit1 kernel: [505145.910882] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: role( Primary -> Secondary )
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.909592    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338" (OuterVolumeSpecName: "linstor-vol") pod "23c28465-eb55-4375-9bd1-5fd757745f89" (UID: "23c28465-eb55-4375-9bd1-5fd757745f89"). InnerVolumeSpecName "pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.963820    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-cltvg\" (UniqueName: \"kubernetes.io/projected/23c28465-eb55-4375-9bd1-5fd757745f89-kube-api-access-cltvg\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.963872    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/05dd1efc-8486-47dd-89df-7ee739a2d7e8\""
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.964006    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\") on node \"linbit1\" "
Mar 13 20:45:54 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:54.964429    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338") on node "linbit1"
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.065215    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/23c28465-eb55-4375-9bd1-5fd757745f89\""
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.079701    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.082522    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 20:45:55 linbit1 kernel: [505146.097067] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Preparing remote state change 1503032566
Mar 13 20:45:55 linbit1 kernel: [505146.097318] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Committing remote state change 1503032566 (primary_nodes=0)
Mar 13 20:45:55 linbit1 kernel: [505146.097343] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: peer( Primary -> Secondary )
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.085021    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085543643Z" level=info msg="StopPodSandbox for \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085648283Z" level=info msg="Container to stop \"4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085675123Z" level=info msg="Container to stop \"4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085708603Z" level=info msg="StopPodSandbox for \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085808284Z" level=info msg="Container to stop \"914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085833204Z" level=info msg="Container to stop \"7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085838204Z" level=info msg="StopPodSandbox for \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085959844Z" level=info msg="Container to stop \"516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.085994084Z" level=info msg="Container to stop \"8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.149241287Z" level=info msg="shim disconnected" id=470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.149313527Z" level=warning msg="cleaning up after shim disconnected" id=470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc namespace=k8s.io
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.149333127Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.166923034Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467633 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:55 linbit1 kernel: [505146.197073] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Preparing remote state change 787814057
Mar 13 20:45:55 linbit1 kernel: [505146.197287] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Committing remote state change 787814057 (primary_nodes=0)
Mar 13 20:45:55 linbit1 kernel: [505146.197296] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: peer( Primary -> Secondary )
Mar 13 20:45:55 linbit1 systemd[1]: run-netns-cni\x2d3826e5ef\x2d1983\x2d1ac5\x2da390\x2dccf3736b6eef.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d8f12a6ad\x2db16c\x2d4206\x2d91d3\x2dc5ec07b55338-dev-23c28465\x2deb55\x2d4375\x2d9bd1\x2d5fd757745f89.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d8f12a6ad\x2db16c\x2d4206\x2d91d3\x2dc5ec07b55338-23c28465\x2deb55\x2d4375\x2d9bd1\x2d5fd757745f89.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-23c28465\x2deb55\x2d4375\x2d9bd1\x2d5fd757745f89-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2dcltvg.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2db7bece75\x2d44cf\x2d4036\x2db1c6\x2dbfcafce9c1aa-dev-05dd1efc\x2d8486\x2d47dd\x2d89df\x2d7ee739a2d7e8.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2db7bece75\x2d44cf\x2d4036\x2db1c6\x2dbfcafce9c1aa-05dd1efc\x2d8486\x2d47dd\x2d89df\x2d7ee739a2d7e8.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa-rootfs.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa-shm.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-05dd1efc\x2d8486\x2d47dd\x2d89df\x2d7ee739a2d7e8-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2d6mgjs.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba-rootfs.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba-shm.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d20f0b4c5\x2d4f16\x2d4b65\x2da52c\x2dfb6c96b56cbc-f15c9220\x2d13bf\x2d47a1\x2d93d8\x2dbf5be8a9a7a5.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc-rootfs.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc-shm.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd-networkd[2297221]: caliccb68bb9c36: Link DOWN
Mar 13 20:45:55 linbit1 systemd-networkd[2297221]: caliccb68bb9c36: Lost carrier
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.288645181Z" level=info msg="shim disconnected" id=1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.288725061Z" level=warning msg="cleaning up after shim disconnected" id=1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa namespace=k8s.io
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.288748181Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.288561500Z" level=info msg="shim disconnected" id=41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.288859422Z" level=warning msg="cleaning up after shim disconnected" id=41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba namespace=k8s.io
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.288883422Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.302948556Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467707 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.306267488Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467706 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:55 linbit1 systemd-networkd[2297221]: cali7bcd57bedf4: Link DOWN
Mar 13 20:45:55 linbit1 systemd-networkd[2297221]: cali7bcd57bedf4: Lost carrier
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086-rootfs.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.562497110Z" level=info msg="shim disconnected" id=04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.562572030Z" level=warning msg="cleaning up after shim disconnected" id=04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086 namespace=k8s.io
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.562593630Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.579161854Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467874 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:55 linbit1 systemd-networkd[2297221]: cali23222f1245f: Link DOWN
Mar 13 20:45:55 linbit1 systemd-networkd[2297221]: cali23222f1245f: Lost carrier
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.279 [INFO][2467670] k8s.go 576: Cleaning up netns ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.280 [INFO][2467670] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" iface="eth0" netns="/var/run/netns/cni-05f860b2-1b4c-eea1-14f6-e9c36c766f9d"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.280 [INFO][2467670] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" iface="eth0" netns="/var/run/netns/cni-05f860b2-1b4c-eea1-14f6-e9c36c766f9d"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.395 [INFO][2467670] dataplane_linux.go 549: Deleted device in netns. ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" after=115.797604ms iface="eth0" netns="/var/run/netns/cni-05f860b2-1b4c-eea1-14f6-e9c36c766f9d"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.395 [INFO][2467670] k8s.go 583: Releasing IP address(es) ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.396 [INFO][2467670] utils.go 196: Calico CNI releasing IP address ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.430 [INFO][2467815] ipam_plugin.go 411: Releasing address using handleID ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.584 [INFO][2467815] ipam_plugin.go 430: Released address using handleID ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.584 [INFO][2467815] ipam_plugin.go 439: Releasing address using workloadID ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.594 [INFO][2467670] k8s.go 589: Teardown processing complete. ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.599127530Z" level=info msg="TearDown network for sandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" successfully"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.599178131Z" level=info msg="StopPodSandbox for \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" returns successfully"
Mar 13 20:45:55 linbit1 systemd[1]: run-netns-cni\x2d05f860b2\x2d1b4c\x2deea1\x2d14f6\x2de9c36c766f9d.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec-rootfs.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7-rootfs.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a-rootfs.mount: Deactivated successfully.
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.414 [INFO][2467768] k8s.go 576: Cleaning up netns ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.414 [INFO][2467768] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" iface="eth0" netns="/var/run/netns/cni-1c298fc2-b3f6-6a34-28d7-a5bc1734841c"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.414 [INFO][2467768] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" iface="eth0" netns="/var/run/netns/cni-1c298fc2-b3f6-6a34-28d7-a5bc1734841c"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.519 [INFO][2467768] dataplane_linux.go 549: Deleted device in netns. ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" after=105.395124ms iface="eth0" netns="/var/run/netns/cni-1c298fc2-b3f6-6a34-28d7-a5bc1734841c"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.520 [INFO][2467768] k8s.go 583: Releasing IP address(es) ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.520 [INFO][2467768] utils.go 196: Calico CNI releasing IP address ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.549 [INFO][2467841] ipam_plugin.go 411: Releasing address using handleID ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.692 [INFO][2467841] ipam_plugin.go 430: Released address using handleID ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.692 [INFO][2467841] ipam_plugin.go 439: Releasing address using workloadID ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.702 [INFO][2467768] k8s.go 589: Teardown processing complete. ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.706489782Z" level=info msg="TearDown network for sandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" successfully"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.706538422Z" level=info msg="StopPodSandbox for \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" returns successfully"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744479407Z" level=info msg="shim disconnected" id=08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744546248Z" level=info msg="shim disconnected" id=5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744615888Z" level=warning msg="cleaning up after shim disconnected" id=5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7 namespace=k8s.io
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744637688Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744547448Z" level=warning msg="cleaning up after shim disconnected" id=08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a namespace=k8s.io
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744701848Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744566888Z" level=info msg="shim disconnected" id=11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744747688Z" level=warning msg="cleaning up after shim disconnected" id=11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec namespace=k8s.io
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.744775568Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:55 linbit1 kernel: [505146.765481] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Preparing remote state change 4128722732
Mar 13 20:45:55 linbit1 kernel: [505146.765688] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Committing remote state change 4128722732 (primary_nodes=0)
Mar 13 20:45:55 linbit1 kernel: [505146.765699] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: peer( Primary -> Secondary )
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.760555069Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467927 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.761020591Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467928 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.762039075Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2467929 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.770265    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\") pod \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\" (UID: \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\") "
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.770344    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-5qsvc\" (UniqueName: \"kubernetes.io/projected/1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f-kube-api-access-5qsvc\") pod \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\" (UID: \"1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\") "
Mar 13 20:45:55 linbit1 kernel: [505146.818593] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 906497456 (0->-1 3/2)
Mar 13 20:45:55 linbit1 kernel: [505146.818921] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change 906497456: primary_nodes=0, weak_nodes=0
Mar 13 20:45:55 linbit1 kernel: [505146.818928] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 906497456 (0ms)
Mar 13 20:45:55 linbit1 kernel: [505146.818970] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: role( Primary -> Secondary )
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.817517    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0" (OuterVolumeSpecName: "linstor-vol") pod "1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f" (UID: "1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f"). InnerVolumeSpecName "pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.848428    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f-kube-api-access-5qsvc" (OuterVolumeSpecName: "kube-api-access-5qsvc") pod "1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f" (UID: "1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f"). InnerVolumeSpecName "kube-api-access-5qsvc". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.848434846Z" level=info msg="shim disconnected" id=9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.848504526Z" level=warning msg="cleaning up after shim disconnected" id=9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf namespace=k8s.io
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.848524406Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.869157965Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:55Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468009 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.871026    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b\") pod \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\" (UID: \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\") "
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.871148    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-dgb5b\" (UniqueName: \"kubernetes.io/projected/163fb3b6-cd3e-4b72-8f21-6a84e97abdc3-kube-api-access-dgb5b\") pod \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\" (UID: \"163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\") "
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.871370    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\") on node \"linbit1\" "
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.871406    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-5qsvc\" (UniqueName: \"kubernetes.io/projected/1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f-kube-api-access-5qsvc\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.872070    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0") on node "linbit1"
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.875298    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/163fb3b6-cd3e-4b72-8f21-6a84e97abdc3-kube-api-access-dgb5b" (OuterVolumeSpecName: "kube-api-access-dgb5b") pod "163fb3b6-cd3e-4b72-8f21-6a84e97abdc3" (UID: "163fb3b6-cd3e-4b72-8f21-6a84e97abdc3"). InnerVolumeSpecName "kube-api-access-dgb5b". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.585 [INFO][2467773] k8s.go 576: Cleaning up netns ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.585 [INFO][2467773] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" iface="eth0" netns="/var/run/netns/cni-552afb8a-1606-90ad-2cb0-0ee4c1c53919"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.586 [INFO][2467773] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" iface="eth0" netns="/var/run/netns/cni-552afb8a-1606-90ad-2cb0-0ee4c1c53919"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.743 [INFO][2467773] dataplane_linux.go 549: Deleted device in netns. ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" after=157.957805ms iface="eth0" netns="/var/run/netns/cni-552afb8a-1606-90ad-2cb0-0ee4c1c53919"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.744 [INFO][2467773] k8s.go 583: Releasing IP address(es) ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.744 [INFO][2467773] utils.go 196: Calico CNI releasing IP address ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.776 [INFO][2467926] ipam_plugin.go 411: Releasing address using handleID ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.882 [INFO][2467926] ipam_plugin.go 430: Released address using handleID ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.882 [INFO][2467926] ipam_plugin.go 439: Releasing address using workloadID ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:55.892 [INFO][2467773] k8s.go 589: Teardown processing complete. ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.896804031Z" level=info msg="TearDown network for sandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" successfully"
Mar 13 20:45:55 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:55.896854031Z" level=info msg="StopPodSandbox for \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" returns successfully"
Mar 13 20:45:55 linbit1 kernel: [505146.926537] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 2360897096 (0->-1 3/2)
Mar 13 20:45:55 linbit1 kernel: [505146.926842] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: State change 2360897096: primary_nodes=0, weak_nodes=0
Mar 13 20:45:55 linbit1 kernel: [505146.926849] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 2360897096 (0ms)
Mar 13 20:45:55 linbit1 kernel: [505146.926884] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: role( Primary -> Secondary )
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.928510    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b" (OuterVolumeSpecName: "linstor-vol") pod "163fb3b6-cd3e-4b72-8f21-6a84e97abdc3" (UID: "163fb3b6-cd3e-4b72-8f21-6a84e97abdc3"). InnerVolumeSpecName "pvc-63619ac2-d122-438b-adb1-8d387095e32b". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.971882    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f\""
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.972052    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-63619ac2-d122-438b-adb1-8d387095e32b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b\") on node \"linbit1\" "
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.972084    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-dgb5b\" (UniqueName: \"kubernetes.io/projected/163fb3b6-cd3e-4b72-8f21-6a84e97abdc3-kube-api-access-dgb5b\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:55 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:55.973726    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-63619ac2-d122-438b-adb1-8d387095e32b" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b") on node "linbit1"
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.072937    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3\") pod \"fa760197-68d3-48e7-abee-bc22cb31e820\" (UID: \"fa760197-68d3-48e7-abee-bc22cb31e820\") "
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.073022    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sgnhk\" (UniqueName: \"kubernetes.io/projected/fa760197-68d3-48e7-abee-bc22cb31e820-kube-api-access-sgnhk\") pod \"fa760197-68d3-48e7-abee-bc22cb31e820\" (UID: \"fa760197-68d3-48e7-abee-bc22cb31e820\") "
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.073130    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-63619ac2-d122-438b-adb1-8d387095e32b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-63619ac2-d122-438b-adb1-8d387095e32b\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-63619ac2-d122-438b-adb1-8d387095e32b/163fb3b6-cd3e-4b72-8f21-6a84e97abdc3\""
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.077348    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/fa760197-68d3-48e7-abee-bc22cb31e820-kube-api-access-sgnhk" (OuterVolumeSpecName: "kube-api-access-sgnhk") pod "fa760197-68d3-48e7-abee-bc22cb31e820" (UID: "fa760197-68d3-48e7-abee-bc22cb31e820"). InnerVolumeSpecName "kube-api-access-sgnhk". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.094943    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.096747    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 20:45:56 linbit1 kernel: [505147.112947] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Preparing remote state change 1082498113
Mar 13 20:45:56 linbit1 kernel: [505147.113129] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Committing remote state change 1082498113 (primary_nodes=0)
Mar 13 20:45:56 linbit1 kernel: [505147.113144] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: peer( Primary -> Secondary )
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.100155    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 20:45:56 linbit1 kernel: [505147.138471] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 2603980111 (0->-1 3/2)
Mar 13 20:45:56 linbit1 kernel: [505147.138824] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: State change 2603980111: primary_nodes=0, weak_nodes=0
Mar 13 20:45:56 linbit1 kernel: [505147.138830] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 2603980111 (0ms)
Mar 13 20:45:56 linbit1 kernel: [505147.138859] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: role( Primary -> Secondary )
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.137003    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3" (OuterVolumeSpecName: "linstor-vol") pod "fa760197-68d3-48e7-abee-bc22cb31e820" (UID: "fa760197-68d3-48e7-abee-bc22cb31e820"). InnerVolumeSpecName "pvc-f35dc493-2853-439d-84a0-6d25a62830f3". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.174137    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-sgnhk\" (UniqueName: \"kubernetes.io/projected/fa760197-68d3-48e7-abee-bc22cb31e820-kube-api-access-sgnhk\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.174326    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-f35dc493-2853-439d-84a0-6d25a62830f3\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3\") on node \"linbit1\" "
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.174887    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-f35dc493-2853-439d-84a0-6d25a62830f3" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3") on node "linbit1"
Mar 13 20:45:56 linbit1 kernel: [505147.197104] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Preparing remote state change 2931764909
Mar 13 20:45:56 linbit1 kernel: [505147.197309] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Committing remote state change 2931764909 (primary_nodes=0)
Mar 13 20:45:56 linbit1 kernel: [505147.197340] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: peer( Primary -> Secondary )
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf-rootfs.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: run-netns-cni\x2d552afb8a\x2d1606\x2d90ad\x2d2cb0\x2d0ee4c1c53919.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2df35dc493\x2d2853\x2d439d\x2d84a0\x2d6d25a62830f3-dev-fa760197\x2d68d3\x2d48e7\x2dabee\x2dbc22cb31e820.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2df35dc493\x2d2853\x2d439d\x2d84a0\x2d6d25a62830f3-fa760197\x2d68d3\x2d48e7\x2dabee\x2dbc22cb31e820.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-fa760197\x2d68d3\x2d48e7\x2dabee\x2dbc22cb31e820-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2dsgnhk.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: run-netns-cni\x2d1c298fc2\x2db3f6\x2d6a34\x2d28d7\x2da5bc1734841c.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d63619ac2\x2dd122\x2d438b\x2dadb1\x2d8d387095e32b-dev-163fb3b6\x2dcd3e\x2d4b72\x2d8f21\x2d6a84e97abdc3.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d63619ac2\x2dd122\x2d438b\x2dadb1\x2d8d387095e32b-163fb3b6\x2dcd3e\x2d4b72\x2d8f21\x2d6a84e97abdc3.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d1f7111f9\x2dabe0\x2d4526\x2dabf4\x2d8bb246304fd0-dev-1e2dd0e4\x2d53f1\x2d4f9c\x2db6e7\x2d94bbf8fdd70f.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d1f7111f9\x2dabe0\x2d4526\x2dabf4\x2d8bb246304fd0-1e2dd0e4\x2d53f1\x2d4f9c\x2db6e7\x2d94bbf8fdd70f.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-163fb3b6\x2dcd3e\x2d4b72\x2d8f21\x2d6a84e97abdc3-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2ddgb5b.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-1e2dd0e4\x2d53f1\x2d4f9c\x2db6e7\x2d94bbf8fdd70f-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2d5qsvc.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:56.275218    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-f35dc493-2853-439d-84a0-6d25a62830f3\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-f35dc493-2853-439d-84a0-6d25a62830f3\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-f35dc493-2853-439d-84a0-6d25a62830f3/fa760197-68d3-48e7-abee-bc22cb31e820\""
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a-rootfs.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.605707026Z" level=info msg="shim disconnected" id=dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.605771107Z" level=warning msg="cleaning up after shim disconnected" id=dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a namespace=k8s.io
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.605792627Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e-rootfs.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.618676516Z" level=info msg="shim disconnected" id=d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.618750916Z" level=warning msg="cleaning up after shim disconnected" id=d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e namespace=k8s.io
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.618770636Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.622474251Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:56Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468078 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7-rootfs.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.629363317Z" level=info msg="shim disconnected" id=6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.629420317Z" level=warning msg="cleaning up after shim disconnected" id=6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7 namespace=k8s.io
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.629437997Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.635369780Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:56Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468104 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:56 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c-rootfs.mount: Deactivated successfully.
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.638796193Z" level=info msg="shim disconnected" id=d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.638860393Z" level=warning msg="cleaning up after shim disconnected" id=d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c namespace=k8s.io
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.638926994Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.645565139Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:56Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468122 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:56 linbit1 kernel: [505147.661491] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Preparing remote state change 2606459042
Mar 13 20:45:56 linbit1 kernel: [505147.661642] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Committing remote state change 2606459042 (primary_nodes=0)
Mar 13 20:45:56 linbit1 kernel: [505147.661654] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: peer( Primary -> Secondary )
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.654678334Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:56Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468134 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.736356967Z" level=info msg="shim disconnected" id=795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.736426407Z" level=warning msg="cleaning up after shim disconnected" id=795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b namespace=k8s.io
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.736446967Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:56 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:56.752811670Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:56Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468165 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.117836068Z" level=info msg="StopPodSandbox for \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.117920348Z" level=info msg="Container to stop \"11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.117943148Z" level=info msg="Container to stop \"8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118037189Z" level=info msg="StopPodSandbox for \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118107109Z" level=info msg="Container to stop \"04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118128189Z" level=info msg="Container to stop \"738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118240389Z" level=info msg="StopPodSandbox for \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118324590Z" level=info msg="Container to stop \"e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118363670Z" level=info msg="Container to stop \"08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118484750Z" level=info msg="StopPodSandbox for \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118582071Z" level=info msg="Container to stop \"5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118615111Z" level=info msg="Container to stop \"a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118639791Z" level=info msg="StopPodSandbox for \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118824272Z" level=info msg="Container to stop \"2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.118877512Z" level=info msg="Container to stop \"9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.192242753Z" level=info msg="shim disconnected" id=d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.192306553Z" level=warning msg="cleaning up after shim disconnected" id=d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf namespace=k8s.io
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.192325033Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.193101516Z" level=info msg="shim disconnected" id=e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.193163916Z" level=warning msg="cleaning up after shim disconnected" id=e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61 namespace=k8s.io
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.193184516Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.193061436Z" level=info msg="shim disconnected" id=a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.193238357Z" level=warning msg="cleaning up after shim disconnected" id=a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078 namespace=k8s.io
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.193258197Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.196959371Z" level=info msg="shim disconnected" id=585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.197024091Z" level=warning msg="cleaning up after shim disconnected" id=585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba namespace=k8s.io
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.197045091Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.196700290Z" level=info msg="shim disconnected" id=3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.197105411Z" level=warning msg="cleaning up after shim disconnected" id=3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61 namespace=k8s.io
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.197127291Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:57 linbit1 kernel: [505148.217048] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Preparing remote state change 3790225193
Mar 13 20:45:57 linbit1 kernel: [505148.217298] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Committing remote state change 3790225193 (primary_nodes=0)
Mar 13 20:45:57 linbit1 kernel: [505148.217309] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: peer( Primary -> Secondary )
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b-rootfs.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61-rootfs.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61-shm.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba-rootfs.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba-shm.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61-rootfs.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61-shm.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078-rootfs.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078-shm.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf-rootfs.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf-shm.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.207478771Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:57Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468301 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.208080093Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:57Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468302 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.209122497Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:57Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468300 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.213035512Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:57Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468322 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.213537034Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:57Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468316 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: califebb37077c9: Link DOWN
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: califebb37077c9: Lost carrier
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: cali9bae66a6c22: Link DOWN
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: cali9bae66a6c22: Lost carrier
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: caliead4891e62b: Link DOWN
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: caliead4891e62b: Lost carrier
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: calide24a04cc7c: Link DOWN
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: calide24a04cc7c: Lost carrier
Mar 13 20:45:57 linbit1 kernel: [505148.393108] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Preparing remote state change 800479752
Mar 13 20:45:57 linbit1 kernel: [505148.393269] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Committing remote state change 800479752 (primary_nodes=0)
Mar 13 20:45:57 linbit1 kernel: [505148.393282] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: peer( Primary -> Secondary )
Mar 13 20:45:57 linbit1 kernel: [505148.513050] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Preparing remote state change 2921451480
Mar 13 20:45:57 linbit1 kernel: [505148.513290] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Committing remote state change 2921451480 (primary_nodes=0)
Mar 13 20:45:57 linbit1 kernel: [505148.513305] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: peer( Primary -> Secondary )
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: cali7414e773585: Link DOWN
Mar 13 20:45:57 linbit1 systemd-networkd[2297221]: cali7414e773585: Lost carrier
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.327 [INFO][2468436] k8s.go 576: Cleaning up netns ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.327 [INFO][2468436] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" iface="eth0" netns="/var/run/netns/cni-6e4f3211-1bb6-a44d-5edb-f1e1cf9fc64e"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.328 [INFO][2468436] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" iface="eth0" netns="/var/run/netns/cni-6e4f3211-1bb6-a44d-5edb-f1e1cf9fc64e"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.467 [INFO][2468436] dataplane_linux.go 549: Deleted device in netns. ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" after=139.905975ms iface="eth0" netns="/var/run/netns/cni-6e4f3211-1bb6-a44d-5edb-f1e1cf9fc64e"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.468 [INFO][2468436] k8s.go 583: Releasing IP address(es) ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.468 [INFO][2468436] utils.go 196: Calico CNI releasing IP address ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.496 [INFO][2468568] ipam_plugin.go 411: Releasing address using handleID ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.618 [INFO][2468568] ipam_plugin.go 430: Released address using handleID ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.619 [INFO][2468568] ipam_plugin.go 439: Releasing address using workloadID ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.629 [INFO][2468436] k8s.go 589: Teardown processing complete. ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.633884564Z" level=info msg="TearDown network for sandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" successfully"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.633945164Z" level=info msg="StopPodSandbox for \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" returns successfully"
Mar 13 20:45:57 linbit1 systemd[1]: run-netns-cni\x2d6e4f3211\x2d1bb6\x2da44d\x2d5edb\x2df1e1cf9fc64e.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 kernel: [505148.765497] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Preparing remote state change 3521497267
Mar 13 20:45:57 linbit1 kernel: [505148.765700] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Committing remote state change 3521497267 (primary_nodes=0)
Mar 13 20:45:57 linbit1 kernel: [505148.765714] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: peer( Primary -> Secondary )
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.358 [INFO][2468458] k8s.go 576: Cleaning up netns ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.361 [INFO][2468458] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" iface="eth0" netns="/var/run/netns/cni-55858e15-3dbf-766c-f9eb-20c0142116d5"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.361 [INFO][2468458] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" iface="eth0" netns="/var/run/netns/cni-55858e15-3dbf-766c-f9eb-20c0142116d5"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.467 [INFO][2468458] dataplane_linux.go 549: Deleted device in netns. ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" after=106.165207ms iface="eth0" netns="/var/run/netns/cni-55858e15-3dbf-766c-f9eb-20c0142116d5"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.467 [INFO][2468458] k8s.go 583: Releasing IP address(es) ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.468 [INFO][2468458] utils.go 196: Calico CNI releasing IP address ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.496 [INFO][2468567] ipam_plugin.go 411: Releasing address using handleID ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.750 [INFO][2468567] ipam_plugin.go 430: Released address using handleID ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.750 [INFO][2468567] ipam_plugin.go 439: Releasing address using workloadID ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.758 [INFO][2468458] k8s.go 589: Teardown processing complete. ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.762893138Z" level=info msg="TearDown network for sandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" successfully"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.762951418Z" level=info msg="StopPodSandbox for \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" returns successfully"
Mar 13 20:45:57 linbit1 systemd[1]: run-netns-cni\x2d55858e15\x2d3dbf\x2d766c\x2df9eb\x2d20c0142116d5.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.786440    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-rpjrh\" (UniqueName: \"kubernetes.io/projected/ad7770d4-2d2d-4215-a027-2d52b0e30ae0-kube-api-access-rpjrh\") pod \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\" (UID: \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\") "
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.786632    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\") pod \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\" (UID: \"ad7770d4-2d2d-4215-a027-2d52b0e30ae0\") "
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.790007    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/ad7770d4-2d2d-4215-a027-2d52b0e30ae0-kube-api-access-rpjrh" (OuterVolumeSpecName: "kube-api-access-rpjrh") pod "ad7770d4-2d2d-4215-a027-2d52b0e30ae0" (UID: "ad7770d4-2d2d-4215-a027-2d52b0e30ae0"). InnerVolumeSpecName "kube-api-access-rpjrh". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-ad7770d4\x2d2d2d\x2d4215\x2da027\x2d2d52b0e30ae0-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2drpjrh.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 kernel: [505148.830496] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 1941123448 (0->-1 3/2)
Mar 13 20:45:57 linbit1 kernel: [505148.830859] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change 1941123448: primary_nodes=0, weak_nodes=0
Mar 13 20:45:57 linbit1 kernel: [505148.830865] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 1941123448 (0ms)
Mar 13 20:45:57 linbit1 kernel: [505148.830897] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: role( Primary -> Secondary )
Mar 13 20:45:57 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d01ec89c0\x2dc4ad\x2d45be\x2d9350\x2dcf49e39860ac-dev-ad7770d4\x2d2d2d\x2d4215\x2da027\x2d2d52b0e30ae0.mount: Deactivated successfully.
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.832315    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac" (OuterVolumeSpecName: "linstor-vol") pod "ad7770d4-2d2d-4215-a027-2d52b0e30ae0" (UID: "ad7770d4-2d2d-4215-a027-2d52b0e30ae0"). InnerVolumeSpecName "pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.361 [INFO][2468453] k8s.go 576: Cleaning up netns ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.362 [INFO][2468453] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" iface="eth0" netns="/var/run/netns/cni-e2c1d143-25ea-daf2-f17c-0d88fc4b829a"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.362 [INFO][2468453] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" iface="eth0" netns="/var/run/netns/cni-e2c1d143-25ea-daf2-f17c-0d88fc4b829a"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.491 [INFO][2468453] dataplane_linux.go 549: Deleted device in netns. ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" after=129.755377ms iface="eth0" netns="/var/run/netns/cni-e2c1d143-25ea-daf2-f17c-0d88fc4b829a"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.491 [INFO][2468453] k8s.go 583: Releasing IP address(es) ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.492 [INFO][2468453] utils.go 196: Calico CNI releasing IP address ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.522 [INFO][2468600] ipam_plugin.go 411: Releasing address using handleID ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.873 [INFO][2468600] ipam_plugin.go 430: Released address using handleID ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.873 [INFO][2468600] ipam_plugin.go 439: Releasing address using workloadID ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.883 [INFO][2468453] k8s.go 589: Teardown processing complete. ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.887687816Z" level=info msg="TearDown network for sandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" successfully"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.887740896Z" level=info msg="StopPodSandbox for \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" returns successfully"
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.887886    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\") pod \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\" (UID: \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\") "
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.887949    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-4bvsj\" (UniqueName: \"kubernetes.io/projected/cc6de1a1-21c4-4733-838a-eb8fcbbe552d-kube-api-access-4bvsj\") pod \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\" (UID: \"cc6de1a1-21c4-4733-838a-eb8fcbbe552d\") "
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.888097    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\") on node \"linbit1\" "
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.888120    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-rpjrh\" (UniqueName: \"kubernetes.io/projected/ad7770d4-2d2d-4215-a027-2d52b0e30ae0-kube-api-access-rpjrh\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.888552    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac") on node "linbit1"
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.892075    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/cc6de1a1-21c4-4733-838a-eb8fcbbe552d-kube-api-access-4bvsj" (OuterVolumeSpecName: "kube-api-access-4bvsj") pod "cc6de1a1-21c4-4733-838a-eb8fcbbe552d" (UID: "cc6de1a1-21c4-4733-838a-eb8fcbbe552d"). InnerVolumeSpecName "kube-api-access-4bvsj". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:57 linbit1 kernel: [505148.930515] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 980687080 (0->-1 3/2)
Mar 13 20:45:57 linbit1 kernel: [505148.930837] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: State change 980687080: primary_nodes=0, weak_nodes=0
Mar 13 20:45:57 linbit1 kernel: [505148.930843] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 980687080 (0ms)
Mar 13 20:45:57 linbit1 kernel: [505148.930882] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: role( Primary -> Secondary )
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.930202    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22" (OuterVolumeSpecName: "linstor-vol") pod "cc6de1a1-21c4-4733-838a-eb8fcbbe552d" (UID: "cc6de1a1-21c4-4733-838a-eb8fcbbe552d"). InnerVolumeSpecName "pvc-c917761f-d3bb-4a7c-a219-5e270d819a22". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:57 linbit1 kernel: [505148.985463] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Preparing remote state change 2196344710
Mar 13 20:45:57 linbit1 kernel: [505148.985674] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Committing remote state change 2196344710 (primary_nodes=0)
Mar 13 20:45:57 linbit1 kernel: [505148.985706] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: peer( Primary -> Secondary )
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.989233    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\") on node \"linbit1\" "
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.989277    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-4bvsj\" (UniqueName: \"kubernetes.io/projected/cc6de1a1-21c4-4733-838a-eb8fcbbe552d-kube-api-access-4bvsj\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.989314    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/ad7770d4-2d2d-4215-a027-2d52b0e30ae0\""
Mar 13 20:45:57 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:57.989836    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-c917761f-d3bb-4a7c-a219-5e270d819a22" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22") on node "linbit1"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.346 [INFO][2468434] k8s.go 576: Cleaning up netns ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.347 [INFO][2468434] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" iface="eth0" netns="/var/run/netns/cni-e3d6e7ab-6858-ed9b-5906-f694b3e382e3"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.347 [INFO][2468434] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" iface="eth0" netns="/var/run/netns/cni-e3d6e7ab-6858-ed9b-5906-f694b3e382e3"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.491 [INFO][2468434] dataplane_linux.go 549: Deleted device in netns. ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" after=144.595754ms iface="eth0" netns="/var/run/netns/cni-e3d6e7ab-6858-ed9b-5906-f694b3e382e3"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.491 [INFO][2468434] k8s.go 583: Releasing IP address(es) ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.491 [INFO][2468434] utils.go 196: Calico CNI releasing IP address ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.523 [INFO][2468599] ipam_plugin.go 411: Releasing address using handleID ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.977 [INFO][2468599] ipam_plugin.go 430: Released address using handleID ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.978 [INFO][2468599] ipam_plugin.go 439: Releasing address using workloadID ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.987 [INFO][2468434] k8s.go 589: Teardown processing complete. ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.996344992Z" level=info msg="TearDown network for sandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" successfully"
Mar 13 20:45:57 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57.996417792Z" level=info msg="StopPodSandbox for \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" returns successfully"
Mar 13 20:45:58 linbit1 kernel: [505149.073479] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Preparing remote state change 3498674160
Mar 13 20:45:58 linbit1 kernel: [505149.073668] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Committing remote state change 3498674160 (primary_nodes=0)
Mar 13 20:45:58 linbit1 kernel: [505149.073701] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: peer( Primary -> Secondary )
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.090485    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588\") pod \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\" (UID: \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.090637    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sp4lb\" (UniqueName: \"kubernetes.io/projected/d9db77e8-50b0-4260-89f3-ddb5a0de385a-kube-api-access-sp4lb\") pod \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\" (UID: \"d9db77e8-50b0-4260-89f3-ddb5a0de385a\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.090715    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-5kb72\" (UniqueName: \"kubernetes.io/projected/5d081096-b326-416f-9081-50608c26d3d1-kube-api-access-5kb72\") pod \"5d081096-b326-416f-9081-50608c26d3d1\" (UID: \"5d081096-b326-416f-9081-50608c26d3d1\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.090838    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c917761f-d3bb-4a7c-a219-5e270d819a22\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/cc6de1a1-21c4-4733-838a-eb8fcbbe552d\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.095157    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/d9db77e8-50b0-4260-89f3-ddb5a0de385a-kube-api-access-sp4lb" (OuterVolumeSpecName: "kube-api-access-sp4lb") pod "d9db77e8-50b0-4260-89f3-ddb5a0de385a" (UID: "d9db77e8-50b0-4260-89f3-ddb5a0de385a"). InnerVolumeSpecName "kube-api-access-sp4lb". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.095650    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/5d081096-b326-416f-9081-50608c26d3d1-kube-api-access-5kb72" (OuterVolumeSpecName: "kube-api-access-5kb72") pod "5d081096-b326-416f-9081-50608c26d3d1" (UID: "5d081096-b326-416f-9081-50608c26d3d1"). InnerVolumeSpecName "kube-api-access-5kb72". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.501 [INFO][2468433] k8s.go 576: Cleaning up netns ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.501 [INFO][2468433] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" iface="eth0" netns="/var/run/netns/cni-6130d9ab-adc9-01b0-ede1-ae6664d6479c"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.501 [INFO][2468433] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" iface="eth0" netns="/var/run/netns/cni-6130d9ab-adc9-01b0-ede1-ae6664d6479c"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.587 [INFO][2468433] dataplane_linux.go 549: Deleted device in netns. ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" after=86.04885ms iface="eth0" netns="/var/run/netns/cni-6130d9ab-adc9-01b0-ede1-ae6664d6479c"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.588 [INFO][2468433] k8s.go 583: Releasing IP address(es) ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.588 [INFO][2468433] utils.go 196: Calico CNI releasing IP address ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:57.618 [INFO][2468652] ipam_plugin.go 411: Releasing address using handleID ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:57Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.090 [INFO][2468652] ipam_plugin.go 430: Released address using handleID ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.091 [INFO][2468652] ipam_plugin.go 439: Releasing address using workloadID ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.099 [INFO][2468433] k8s.go 589: Teardown processing complete. ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.104540966Z" level=info msg="TearDown network for sandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" successfully"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.104597726Z" level=info msg="StopPodSandbox for \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" returns successfully"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.122764    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.129652    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 20:45:58 linbit1 kernel: [505149.142458] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 3716969281 (0->-1 3/2)
Mar 13 20:45:58 linbit1 kernel: [505149.142742] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change 3716969281: primary_nodes=0, weak_nodes=0
Mar 13 20:45:58 linbit1 kernel: [505149.142747] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 3716969281 (0ms)
Mar 13 20:45:58 linbit1 kernel: [505149.142776] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: role( Primary -> Secondary )
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.131770    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.133436    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.135307    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.135701245Z" level=info msg="StopPodSandbox for \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.135773085Z" level=info msg="Container to stop \"dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.135791245Z" level=info msg="Container to stop \"1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.135924446Z" level=info msg="StopPodSandbox for \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136032206Z" level=info msg="StopPodSandbox for \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136056646Z" level=info msg="Container to stop \"dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136096607Z" level=info msg="Container to stop \"6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136121367Z" level=info msg="Container to stop \"93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136144367Z" level=info msg="Container to stop \"d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136226567Z" level=info msg="StopPodSandbox for \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136234367Z" level=info msg="StopPodSandbox for \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136299207Z" level=info msg="Container to stop \"716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136315327Z" level=info msg="Container to stop \"795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136320327Z" level=info msg="Container to stop \"a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.136344128Z" level=info msg="Container to stop \"d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c\" must be in running or unknown state, current state \"CONTAINER_EXITED\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.142836    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588" (OuterVolumeSpecName: "linstor-vol") pod "d9db77e8-50b0-4260-89f3-ddb5a0de385a" (UID: "d9db77e8-50b0-4260-89f3-ddb5a0de385a"). InnerVolumeSpecName "pvc-b01e907a-4035-4022-9701-ff7a07ac1588". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.191926    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b\") pod \"5d081096-b326-416f-9081-50608c26d3d1\" (UID: \"5d081096-b326-416f-9081-50608c26d3d1\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.192050    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\") pod \"6352c9ef-1397-436e-b40b-a40ae673de66\" (UID: \"6352c9ef-1397-436e-b40b-a40ae673de66\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.192106    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sk5cf\" (UniqueName: \"kubernetes.io/projected/6352c9ef-1397-436e-b40b-a40ae673de66-kube-api-access-sk5cf\") pod \"6352c9ef-1397-436e-b40b-a40ae673de66\" (UID: \"6352c9ef-1397-436e-b40b-a40ae673de66\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.192318    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-sp4lb\" (UniqueName: \"kubernetes.io/projected/d9db77e8-50b0-4260-89f3-ddb5a0de385a-kube-api-access-sp4lb\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.192373    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-5kb72\" (UniqueName: \"kubernetes.io/projected/5d081096-b326-416f-9081-50608c26d3d1-kube-api-access-5kb72\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.192523    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-b01e907a-4035-4022-9701-ff7a07ac1588\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588\") on node \"linbit1\" "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.193012    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-b01e907a-4035-4022-9701-ff7a07ac1588" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588") on node "linbit1"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.195878    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/6352c9ef-1397-436e-b40b-a40ae673de66-kube-api-access-sk5cf" (OuterVolumeSpecName: "kube-api-access-sk5cf") pod "6352c9ef-1397-436e-b40b-a40ae673de66" (UID: "6352c9ef-1397-436e-b40b-a40ae673de66"). InnerVolumeSpecName "kube-api-access-sk5cf". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:58 linbit1 systemd[1]: run-netns-cni\x2d6130d9ab\x2dadc9\x2d01b0\x2dede1\x2dae6664d6479c.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-6352c9ef\x2d1397\x2d436e\x2db40b\x2da40ae673de66-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2dsk5cf.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc-rootfs.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc-shm.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7-rootfs.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7-shm.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2dc917761f\x2dd3bb\x2d4a7c\x2da219\x2d5e270d819a22-dev-cc6de1a1\x2d21c4\x2d4733\x2d838a\x2deb8fcbbe552d.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2dc917761f\x2dd3bb\x2d4a7c\x2da219\x2d5e270d819a22-cc6de1a1\x2d21c4\x2d4733\x2d838a\x2deb8fcbbe552d.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-cc6de1a1\x2d21c4\x2d4733\x2d838a\x2deb8fcbbe552d-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2d4bvsj.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: run-netns-cni\x2de2c1d143\x2d25ea\x2ddaf2\x2df17c\x2d0d88fc4b829a.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2db01e907a\x2d4035\x2d4022\x2d9701\x2dff7a07ac1588-dev-d9db77e8\x2d50b0\x2d4260\x2d89f3\x2dddb5a0de385a.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2db01e907a\x2d4035\x2d4022\x2d9701\x2dff7a07ac1588-d9db77e8\x2d50b0\x2d4260\x2d89f3\x2dddb5a0de385a.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5-shm.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-d9db77e8\x2d50b0\x2d4260\x2d89f3\x2dddb5a0de385a-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2dsp4lb.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: run-netns-cni\x2de3d6e7ab\x2d6858\x2ded9b\x2d5906\x2df694b3e382e3.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0-rootfs.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0-shm.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.grpc.v1.cri-sandboxes-4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672-shm.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d01ec89c0\x2dc4ad\x2d45be\x2d9350\x2dcf49e39860ac-ad7770d4\x2d2d2d\x2d4215\x2da027\x2d2d52b0e30ae0.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-5d081096\x2db326\x2d416f\x2d9081\x2d50608c26d3d1-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2d5kb72.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672-rootfs.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-run-containerd-io.containerd.runtime.v2.task-k8s.io-c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5-rootfs.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 kernel: [505149.238508] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 2237330995 (0->-1 3/2)
Mar 13 20:45:58 linbit1 kernel: [505149.238521] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 2179659970 (0->-1 3/2)
Mar 13 20:45:58 linbit1 kernel: [505149.238825] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: State change 2179659970: primary_nodes=0, weak_nodes=0
Mar 13 20:45:58 linbit1 kernel: [505149.238832] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 2179659970 (0ms)
Mar 13 20:45:58 linbit1 kernel: [505149.238832] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: State change 2237330995: primary_nodes=0, weak_nodes=0
Mar 13 20:45:58 linbit1 kernel: [505149.238838] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 2237330995 (0ms)
Mar 13 20:45:58 linbit1 kernel: [505149.238868] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: role( Primary -> Secondary )
Mar 13 20:45:58 linbit1 kernel: [505149.238869] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: role( Primary -> Secondary )
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2dd1746b46\x2dd2be\x2d4657\x2daaf4\x2d05198959329b-dev-5d081096\x2db326\x2d416f\x2d9081\x2d50608c26d3d1.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.237464    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7" (OuterVolumeSpecName: "linstor-vol") pod "6352c9ef-1397-436e-b40b-a40ae673de66" (UID: "6352c9ef-1397-436e-b40b-a40ae673de66"). InnerVolumeSpecName "pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.237523    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b" (OuterVolumeSpecName: "linstor-vol") pod "5d081096-b326-416f-9081-50608c26d3d1" (UID: "5d081096-b326-416f-9081-50608c26d3d1"). InnerVolumeSpecName "pvc-d1746b46-d2be-4657-aaf4-05198959329b". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:58 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d0845260b\x2dd8f2\x2d4d2d\x2d9ee6\x2d67932e95f1f7-dev-6352c9ef\x2d1397\x2d436e\x2db40b\x2da40ae673de66.mount: Deactivated successfully.
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.293618    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-b01e907a-4035-4022-9701-ff7a07ac1588\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-b01e907a-4035-4022-9701-ff7a07ac1588\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-b01e907a-4035-4022-9701-ff7a07ac1588/d9db77e8-50b0-4260-89f3-ddb5a0de385a\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.293822    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-d1746b46-d2be-4657-aaf4-05198959329b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b\") on node \"linbit1\" "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.293944    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\") on node \"linbit1\" "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.293974    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-sk5cf\" (UniqueName: \"kubernetes.io/projected/6352c9ef-1397-436e-b40b-a40ae673de66-kube-api-access-sk5cf\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.294518    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-d1746b46-d2be-4657-aaf4-05198959329b" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b") on node "linbit1"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.294564    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7") on node "linbit1"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301203279Z" level=info msg="shim disconnected" id=e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301280639Z" level=warning msg="cleaning up after shim disconnected" id=e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc namespace=k8s.io
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301308079Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301489440Z" level=info msg="shim disconnected" id=4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301895161Z" level=info msg="shim disconnected" id=173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301954641Z" level=warning msg="cleaning up after shim disconnected" id=173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7 namespace=k8s.io
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301973722Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301899161Z" level=info msg="shim disconnected" id=c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.302017162Z" level=warning msg="cleaning up after shim disconnected" id=c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5 namespace=k8s.io
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.301981962Z" level=info msg="shim disconnected" id=5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.302052282Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.302095602Z" level=warning msg="cleaning up after shim disconnected" id=4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672 namespace=k8s.io
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.302142402Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.302078722Z" level=warning msg="cleaning up after shim disconnected" id=5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0 namespace=k8s.io
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.302190242Z" level=info msg="cleaning up dead shim"
Mar 13 20:45:58 linbit1 kernel: [505149.329034] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Preparing remote state change 1911966178
Mar 13 20:45:58 linbit1 kernel: [505149.329237] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Committing remote state change 1911966178 (primary_nodes=0)
Mar 13 20:45:58 linbit1 kernel: [505149.329270] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: peer( Primary -> Secondary )
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.317689742Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:58Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468822 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.318490945Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:58Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468823 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.318659465Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:58Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468824 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.318744146Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:58Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468825 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.319296028Z" level=warning msg="cleanup warnings time=\"2023-03-13T20:45:58Z\" level=info msg=\"starting signal loop\" namespace=k8s.io pid=2468828 runtime=io.containerd.runc.v2\n"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.394302    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-d1746b46-d2be-4657-aaf4-05198959329b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-d1746b46-d2be-4657-aaf4-05198959329b\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-d1746b46-d2be-4657-aaf4-05198959329b/5d081096-b326-416f-9081-50608c26d3d1\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.394383    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/6352c9ef-1397-436e-b40b-a40ae673de66\""
Mar 13 20:45:58 linbit1 kernel: [505149.428986] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Preparing remote state change 1762080544
Mar 13 20:45:58 linbit1 kernel: [505149.429218] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Committing remote state change 1762080544 (primary_nodes=0)
Mar 13 20:45:58 linbit1 kernel: [505149.429250] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: peer( Primary -> Secondary )
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: cali375d3b08313: Link DOWN
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: cali375d3b08313: Lost carrier
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: cali05d59ceb4c6: Link DOWN
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: cali05d59ceb4c6: Lost carrier
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: calib820d340f7b: Link DOWN
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: calib820d340f7b: Lost carrier
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: cali898463cd9b2: Link DOWN
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: cali898463cd9b2: Lost carrier
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: cali929a2f4f008: Link DOWN
Mar 13 20:45:58 linbit1 systemd-networkd[2297221]: cali929a2f4f008: Lost carrier
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.439 [INFO][2468957] k8s.go 576: Cleaning up netns ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.439 [INFO][2468957] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" iface="eth0" netns="/var/run/netns/cni-964e6b9a-decc-3d7d-fea1-6fb57ef8da0f"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.439 [INFO][2468957] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" iface="eth0" netns="/var/run/netns/cni-964e6b9a-decc-3d7d-fea1-6fb57ef8da0f"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.539 [INFO][2468957] dataplane_linux.go 549: Deleted device in netns. ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" after=100.321584ms iface="eth0" netns="/var/run/netns/cni-964e6b9a-decc-3d7d-fea1-6fb57ef8da0f"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.540 [INFO][2468957] k8s.go 583: Releasing IP address(es) ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.540 [INFO][2468957] utils.go 196: Calico CNI releasing IP address ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.569 [INFO][2469095] ipam_plugin.go 411: Releasing address using handleID ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.683 [INFO][2469095] ipam_plugin.go 430: Released address using handleID ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.683 [INFO][2469095] ipam_plugin.go 439: Releasing address using workloadID ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.691 [INFO][2468957] k8s.go 589: Teardown processing complete. ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.695450828Z" level=info msg="TearDown network for sandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" successfully"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.695516908Z" level=info msg="StopPodSandbox for \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" returns successfully"
Mar 13 20:45:58 linbit1 kernel: [505149.757459] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Preparing remote state change 467499404
Mar 13 20:45:58 linbit1 kernel: [505149.757698] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Committing remote state change 467499404 (primary_nodes=0)
Mar 13 20:45:58 linbit1 kernel: [505149.757708] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: peer( Primary -> Secondary )
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.796848    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\") pod \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\" (UID: \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.796936    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-6xtsx\" (UniqueName: \"kubernetes.io/projected/f49ecb96-a6f0-48a0-b80c-53dd0806643d-kube-api-access-6xtsx\") pod \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\" (UID: \"f49ecb96-a6f0-48a0-b80c-53dd0806643d\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.477 [INFO][2468966] k8s.go 576: Cleaning up netns ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.477 [INFO][2468966] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" iface="eth0" netns="/var/run/netns/cni-04c5cd1b-7f40-077a-7cda-208dad60cf23"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.478 [INFO][2468966] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" iface="eth0" netns="/var/run/netns/cni-04c5cd1b-7f40-077a-7cda-208dad60cf23"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.595 [INFO][2468966] dataplane_linux.go 549: Deleted device in netns. ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" after=117.736491ms iface="eth0" netns="/var/run/netns/cni-04c5cd1b-7f40-077a-7cda-208dad60cf23"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.596 [INFO][2468966] k8s.go 583: Releasing IP address(es) ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.596 [INFO][2468966] utils.go 196: Calico CNI releasing IP address ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.625 [INFO][2469119] ipam_plugin.go 411: Releasing address using handleID ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.786 [INFO][2469119] ipam_plugin.go 430: Released address using handleID ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.786 [INFO][2469119] ipam_plugin.go 439: Releasing address using workloadID ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.795 [INFO][2468966] k8s.go 589: Teardown processing complete. ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.799655707Z" level=info msg="TearDown network for sandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" successfully"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.799707467Z" level=info msg="StopPodSandbox for \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" returns successfully"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.800720    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f49ecb96-a6f0-48a0-b80c-53dd0806643d-kube-api-access-6xtsx" (OuterVolumeSpecName: "kube-api-access-6xtsx") pod "f49ecb96-a6f0-48a0-b80c-53dd0806643d" (UID: "f49ecb96-a6f0-48a0-b80c-53dd0806643d"). InnerVolumeSpecName "kube-api-access-6xtsx". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:58 linbit1 kernel: [505149.846453] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 3069349758 (0->-1 3/2)
Mar 13 20:45:58 linbit1 kernel: [505149.846785] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: State change 3069349758: primary_nodes=0, weak_nodes=0
Mar 13 20:45:58 linbit1 kernel: [505149.846792] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 3069349758 (0ms)
Mar 13 20:45:58 linbit1 kernel: [505149.846825] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: role( Primary -> Secondary )
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.845454    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8" (OuterVolumeSpecName: "linstor-vol") pod "f49ecb96-a6f0-48a0-b80c-53dd0806643d" (UID: "f49ecb96-a6f0-48a0-b80c-53dd0806643d"). InnerVolumeSpecName "pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:58 linbit1 kernel: [505149.869446] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Preparing remote state change 1084738973
Mar 13 20:45:58 linbit1 kernel: [505149.869725] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Committing remote state change 1084738973 (primary_nodes=0)
Mar 13 20:45:58 linbit1 kernel: [505149.869741] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: peer( Primary -> Secondary )
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.897623    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-hczfq\" (UniqueName: \"kubernetes.io/projected/f3b42727-fd59-4af5-a4e2-dc97ffd00b2d-kube-api-access-hczfq\") pod \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\" (UID: \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.897848    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b\") pod \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\" (UID: \"f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.897994    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\") on node \"linbit1\" "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.898022    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-6xtsx\" (UniqueName: \"kubernetes.io/projected/f49ecb96-a6f0-48a0-b80c-53dd0806643d-kube-api-access-6xtsx\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.898509    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8") on node "linbit1"
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.901168    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/f3b42727-fd59-4af5-a4e2-dc97ffd00b2d-kube-api-access-hczfq" (OuterVolumeSpecName: "kube-api-access-hczfq") pod "f3b42727-fd59-4af5-a4e2-dc97ffd00b2d" (UID: "f3b42727-fd59-4af5-a4e2-dc97ffd00b2d"). InnerVolumeSpecName "kube-api-access-hczfq". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.471 [INFO][2468968] k8s.go 576: Cleaning up netns ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.471 [INFO][2468968] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" iface="eth0" netns="/var/run/netns/cni-b1b4faaf-57c7-6683-d226-381fc56226b0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.471 [INFO][2468968] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" iface="eth0" netns="/var/run/netns/cni-b1b4faaf-57c7-6683-d226-381fc56226b0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.595 [INFO][2468968] dataplane_linux.go 549: Deleted device in netns. ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" after=124.530957ms iface="eth0" netns="/var/run/netns/cni-b1b4faaf-57c7-6683-d226-381fc56226b0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.596 [INFO][2468968] k8s.go 583: Releasing IP address(es) ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.596 [INFO][2468968] utils.go 196: Calico CNI releasing IP address ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.627 [INFO][2469118] ipam_plugin.go 411: Releasing address using handleID ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.893 [INFO][2469118] ipam_plugin.go 430: Released address using handleID ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.893 [INFO][2469118] ipam_plugin.go 439: Releasing address using workloadID ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.909 [INFO][2468968] k8s.go 589: Teardown processing complete. ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.913414662Z" level=info msg="TearDown network for sandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" successfully"
Mar 13 20:45:58 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58.913476302Z" level=info msg="StopPodSandbox for \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" returns successfully"
Mar 13 20:45:58 linbit1 kernel: [505149.946735] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 4292485995 (0->-1 3/2)
Mar 13 20:45:58 linbit1 kernel: [505149.947036] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change 4292485995: primary_nodes=0, weak_nodes=0
Mar 13 20:45:58 linbit1 kernel: [505149.947041] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 4292485995 (0ms)
Mar 13 20:45:58 linbit1 kernel: [505149.947072] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: role( Primary -> Secondary )
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.945733    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b" (OuterVolumeSpecName: "linstor-vol") pod "f3b42727-fd59-4af5-a4e2-dc97ffd00b2d" (UID: "f3b42727-fd59-4af5-a4e2-dc97ffd00b2d"). InnerVolumeSpecName "pvc-942882e3-6d43-415f-8f08-18cb1792333b". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.998961    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b\") pod \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\" (UID: \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.999061    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-49t98\" (UniqueName: \"kubernetes.io/projected/1a7e1b25-e333-4558-9a33-7f7b07d5d61c-kube-api-access-49t98\") pod \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\" (UID: \"1a7e1b25-e333-4558-9a33-7f7b07d5d61c\") "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.999156    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-hczfq\" (UniqueName: \"kubernetes.io/projected/f3b42727-fd59-4af5-a4e2-dc97ffd00b2d-kube-api-access-hczfq\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.999289    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-942882e3-6d43-415f-8f08-18cb1792333b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b\") on node \"linbit1\" "
Mar 13 20:45:58 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.999335    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/f49ecb96-a6f0-48a0-b80c-53dd0806643d\""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:58.999955    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-942882e3-6d43-415f-8f08-18cb1792333b" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b") on node "linbit1"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.002693    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/1a7e1b25-e333-4558-9a33-7f7b07d5d61c-kube-api-access-49t98" (OuterVolumeSpecName: "kube-api-access-49t98") pod "1a7e1b25-e333-4558-9a33-7f7b07d5d61c" (UID: "1a7e1b25-e333-4558-9a33-7f7b07d5d61c"). InnerVolumeSpecName "kube-api-access-49t98". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:59 linbit1 kernel: [505150.034441] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 2184436774 (0->-1 3/2)
Mar 13 20:45:59 linbit1 kernel: [505150.034778] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: State change 2184436774: primary_nodes=0, weak_nodes=0
Mar 13 20:45:59 linbit1 kernel: [505150.034783] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 2184436774 (0ms)
Mar 13 20:45:59 linbit1 kernel: [505150.034815] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: role( Primary -> Secondary )
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.456 [INFO][2468967] k8s.go 576: Cleaning up netns ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.456 [INFO][2468967] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" iface="eth0" netns="/var/run/netns/cni-89f66928-335e-7532-2ba6-23c9a229b58d"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.457 [INFO][2468967] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" iface="eth0" netns="/var/run/netns/cni-89f66928-335e-7532-2ba6-23c9a229b58d"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.596 [INFO][2468967] dataplane_linux.go 549: Deleted device in netns. ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" after=139.093372ms iface="eth0" netns="/var/run/netns/cni-89f66928-335e-7532-2ba6-23c9a229b58d"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.596 [INFO][2468967] k8s.go 583: Releasing IP address(es) ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.596 [INFO][2468967] utils.go 196: Calico CNI releasing IP address ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.633 [INFO][2469120] ipam_plugin.go 411: Releasing address using handleID ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:59.018 [INFO][2469120] ipam_plugin.go 430: Released address using handleID ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:59.018 [INFO][2469120] ipam_plugin.go 439: Releasing address using workloadID ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:59Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:59.027 [INFO][2468967] k8s.go 589: Teardown processing complete. ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:59.031384474Z" level=info msg="TearDown network for sandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" successfully"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:59.031435794Z" level=info msg="StopPodSandbox for \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" returns successfully"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.034983    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b" (OuterVolumeSpecName: "linstor-vol") pod "1a7e1b25-e333-4558-9a33-7f7b07d5d61c" (UID: "1a7e1b25-e333-4558-9a33-7f7b07d5d61c"). InnerVolumeSpecName "pvc-400778d2-ad89-410f-90fd-625de046658b". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.099805    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-zws8f\" (UniqueName: \"kubernetes.io/projected/75ce48a0-322f-463e-9859-01950deee06d-kube-api-access-zws8f\") pod \"75ce48a0-322f-463e-9859-01950deee06d\" (UID: \"75ce48a0-322f-463e-9859-01950deee06d\") "
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.100047    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\") pod \"75ce48a0-322f-463e-9859-01950deee06d\" (UID: \"75ce48a0-322f-463e-9859-01950deee06d\") "
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.100155    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-49t98\" (UniqueName: \"kubernetes.io/projected/1a7e1b25-e333-4558-9a33-7f7b07d5d61c-kube-api-access-49t98\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.100217    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-942882e3-6d43-415f-8f08-18cb1792333b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-942882e3-6d43-415f-8f08-18cb1792333b\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-942882e3-6d43-415f-8f08-18cb1792333b/f3b42727-fd59-4af5-a4e2-dc97ffd00b2d\""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.100355    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-400778d2-ad89-410f-90fd-625de046658b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b\") on node \"linbit1\" "
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.100923    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-400778d2-ad89-410f-90fd-625de046658b" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b") on node "linbit1"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.103854    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/75ce48a0-322f-463e-9859-01950deee06d-kube-api-access-zws8f" (OuterVolumeSpecName: "kube-api-access-zws8f") pod "75ce48a0-322f-463e-9859-01950deee06d" (UID: "75ce48a0-322f-463e-9859-01950deee06d"). InnerVolumeSpecName "kube-api-access-zws8f". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.462 [INFO][2468969] k8s.go 576: Cleaning up netns ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.463 [INFO][2468969] dataplane_linux.go 504: Deleting workload's device in netns. ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" iface="eth0" netns="/var/run/netns/cni-e5e99c5b-ebb0-9eba-e45e-3de96d5b46df"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.463 [INFO][2468969] dataplane_linux.go 515: Entered netns, deleting veth. ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" iface="eth0" netns="/var/run/netns/cni-e5e99c5b-ebb0-9eba-e45e-3de96d5b46df"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.647 [INFO][2468969] dataplane_linux.go 549: Deleted device in netns. ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" after=184.748547ms iface="eth0" netns="/var/run/netns/cni-e5e99c5b-ebb0-9eba-e45e-3de96d5b46df"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.648 [INFO][2468969] k8s.go 583: Releasing IP address(es) ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.648 [INFO][2468969] utils.go 196: Calico CNI releasing IP address ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:58.682 [INFO][2469178] ipam_plugin.go 411: Releasing address using handleID ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:58Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:59Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:59.120 [INFO][2469178] ipam_plugin.go 430: Released address using handleID ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:59.120 [INFO][2469178] ipam_plugin.go 439: Releasing address using workloadID ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:59Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 20:45:59.128 [INFO][2468969] k8s.go 589: Teardown processing complete. ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:59.133806745Z" level=info msg="TearDown network for sandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" successfully"
Mar 13 20:45:59 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T20:45:59.133868706Z" level=info msg="StopPodSandbox for \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" returns successfully"
Mar 13 20:45:59 linbit1 kernel: [505150.146506] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 1133168202 (0->-1 3/2)
Mar 13 20:45:59 linbit1 kernel: [505150.146847] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: State change 1133168202: primary_nodes=0, weak_nodes=0
Mar 13 20:45:59 linbit1 kernel: [505150.146854] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 1133168202 (0ms)
Mar 13 20:45:59 linbit1 kernel: [505150.146890] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: role( Primary -> Secondary )
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.139830    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.141886    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.143803    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.145585    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.145725    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694" (OuterVolumeSpecName: "linstor-vol") pod "75ce48a0-322f-463e-9859-01950deee06d" (UID: "75ce48a0-322f-463e-9859-01950deee06d"). InnerVolumeSpecName "pvc-c41969dd-54f4-413c-a3c5-a272b9c80694". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.147523    2448 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.201343    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"kube-api-access-b88x9\" (UniqueName: \"kubernetes.io/projected/e5888ce9-60e2-4ded-94a7-5ac6c28650b6-kube-api-access-b88x9\") pod \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\" (UID: \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\") "
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.201511    2448 reconciler_common.go:169] "operationExecutor.UnmountVolume started for volume \"linstor-vol\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\") pod \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\" (UID: \"e5888ce9-60e2-4ded-94a7-5ac6c28650b6\") "
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.201657    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\") on node \"linbit1\" "
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.201689    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-zws8f\" (UniqueName: \"kubernetes.io/projected/75ce48a0-322f-463e-9859-01950deee06d-kube-api-access-zws8f\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.201726    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-400778d2-ad89-410f-90fd-625de046658b\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-400778d2-ad89-410f-90fd-625de046658b\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-400778d2-ad89-410f-90fd-625de046658b/1a7e1b25-e333-4558-9a33-7f7b07d5d61c\""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.202332    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-c41969dd-54f4-413c-a3c5-a272b9c80694" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694") on node "linbit1"
Mar 13 20:45:59 linbit1 kernel: [505150.225023] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Preparing remote state change 2619424734
Mar 13 20:45:59 linbit1 kernel: [505150.225275] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Committing remote state change 2619424734 (primary_nodes=0)
Mar 13 20:45:59 linbit1 kernel: [505150.225289] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: peer( Primary -> Secondary )
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d0845260b\x2dd8f2\x2d4d2d\x2d9ee6\x2d67932e95f1f7-6352c9ef\x2d1397\x2d436e\x2db40b\x2da40ae673de66.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: run-netns-cni\x2d964e6b9a\x2ddecc\x2d3d7d\x2dfea1\x2d6fb57ef8da0f.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.204986    2448 operation_generator.go:890] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e5888ce9-60e2-4ded-94a7-5ac6c28650b6-kube-api-access-b88x9" (OuterVolumeSpecName: "kube-api-access-b88x9") pod "e5888ce9-60e2-4ded-94a7-5ac6c28650b6" (UID: "e5888ce9-60e2-4ded-94a7-5ac6c28650b6"). InnerVolumeSpecName "kube-api-access-b88x9". PluginName "kubernetes.io/projected", VolumeGidValue ""
Mar 13 20:45:59 linbit1 systemd[1]: run-netns-cni\x2d04c5cd1b\x2d7f40\x2d077a\x2d7cda\x2d208dad60cf23.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d8385374b\x2dabfd\x2d40fc\x2dba54\x2d1047ad8dd1a8-dev-f49ecb96\x2da6f0\x2d48a0\x2db80c\x2d53dd0806643d.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d8385374b\x2dabfd\x2d40fc\x2dba54\x2d1047ad8dd1a8-f49ecb96\x2da6f0\x2d48a0\x2db80c\x2d53dd0806643d.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d942882e3\x2d6d43\x2d415f\x2d8f08\x2d18cb1792333b-dev-f3b42727\x2dfd59\x2d4af5\x2da4e2\x2ddc97ffd00b2d.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d942882e3\x2d6d43\x2d415f\x2d8f08\x2d18cb1792333b-f3b42727\x2dfd59\x2d4af5\x2da4e2\x2ddc97ffd00b2d.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-f3b42727\x2dfd59\x2d4af5\x2da4e2\x2ddc97ffd00b2d-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2dhczfq.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-f49ecb96\x2da6f0\x2d48a0\x2db80c\x2d53dd0806643d-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2d6xtsx.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: run-netns-cni\x2db1b4faaf\x2d57c7\x2d6683\x2dd226\x2d381fc56226b0.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d400778d2\x2dad89\x2d410f\x2d90fd\x2d625de046658b-dev-1a7e1b25\x2de333\x2d4558\x2d9a33\x2d7f7b07d5d61c.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d400778d2\x2dad89\x2d410f\x2d90fd\x2d625de046658b-1a7e1b25\x2de333\x2d4558\x2d9a33\x2d7f7b07d5d61c.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-1a7e1b25\x2de333\x2d4558\x2d9a33\x2d7f7b07d5d61c-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2d49t98.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2dd1746b46\x2dd2be\x2d4657\x2daaf4\x2d05198959329b-5d081096\x2db326\x2d416f\x2d9081\x2d50608c26d3d1.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: run-netns-cni\x2d89f66928\x2d335e\x2d7532\x2d2ba6\x2d23c9a229b58d.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2dc41969dd\x2d54f4\x2d413c\x2da3c5\x2da272b9c80694-dev-75ce48a0\x2d322f\x2d463e\x2d9859\x2d01950deee06d.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2dc41969dd\x2d54f4\x2d413c\x2da3c5\x2da272b9c80694-75ce48a0\x2d322f\x2d463e\x2d9859\x2d01950deee06d.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: run-netns-cni\x2de5e99c5b\x2debb0\x2d9eba\x2de45e\x2d3de96d5b46df.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-75ce48a0\x2d322f\x2d463e\x2d9859\x2d01950deee06d-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2dzws8f.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-pods-e5888ce9\x2d60e2\x2d4ded\x2d94a7\x2d5ac6c28650b6-volumes-kubernetes.io\x7eprojected-kube\x2dapi\x2daccess\x2db88x9.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 kernel: [505150.266451] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 4011006949 (0->-1 3/2)
Mar 13 20:45:59 linbit1 kernel: [505150.266794] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: State change 4011006949: primary_nodes=0, weak_nodes=0
Mar 13 20:45:59 linbit1 kernel: [505150.266801] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 4011006949 (0ms)
Mar 13 20:45:59 linbit1 kernel: [505150.266836] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: role( Primary -> Secondary )
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-pvc\x2d986e7e35\x2dae6d\x2d4e62\x2db90d\x2d191e128bd24a-dev-e5888ce9\x2d60e2\x2d4ded\x2d94a7\x2d5ac6c28650b6.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.265214    2448 operation_generator.go:1342] UnmapVolume succeeded for volume "kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a" (OuterVolumeSpecName: "linstor-vol") pod "e5888ce9-60e2-4ded-94a7-5ac6c28650b6" (UID: "e5888ce9-60e2-4ded-94a7-5ac6c28650b6"). InnerVolumeSpecName "pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a". PluginName "kubernetes.io/csi", VolumeGidValue ""
Mar 13 20:45:59 linbit1 systemd[1]: var-snap-microk8s-common-var-lib-kubelet-plugins-kubernetes.io-csi-volumeDevices-publish-pvc\x2d986e7e35\x2dae6d\x2d4e62\x2db90d\x2d191e128bd24a-e5888ce9\x2d60e2\x2d4ded\x2d94a7\x2d5ac6c28650b6.mount: Deactivated successfully.
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.302276    2448 reconciler_common.go:295] "Volume detached for volume \"kube-api-access-b88x9\" (UniqueName: \"kubernetes.io/projected/e5888ce9-60e2-4ded-94a7-5ac6c28650b6-kube-api-access-b88x9\") on node \"linbit1\" DevicePath \"\""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.302561    2448 reconciler_common.go:288] "operationExecutor.UnmountDevice started for volume \"pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\") on node \"linbit1\" "
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.302657    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-c41969dd-54f4-413c-a3c5-a272b9c80694\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/75ce48a0-322f-463e-9859-01950deee06d\""
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.303103    2448 operation_generator.go:1471] UnmapDevice succeeded for volume "pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a" (UniqueName: "kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a") on node "linbit1"
Mar 13 20:45:59 linbit1 microk8s.daemon-kubelite[2448]: I0313 20:45:59.403263    2448 reconciler_common.go:295] "Volume detached for volume \"pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\" (UniqueName: \"kubernetes.io/csi/linstor.csi.linbit.com^pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a\") on node \"linbit1\" DevicePath \"/var/snap/microk8s/common/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/e5888ce9-60e2-4ded-94a7-5ac6c28650b6\""
Mar 13 20:46:08 linbit1 kernel: [505159.548719] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: sock was shut down by peer
Mar 13 20:46:08 linbit1 kernel: [505159.548739] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: Would lose quorum, but using tiebreaker logic to keep
Mar 13 20:46:08 linbit1 kernel: [505159.548749] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:46:08 linbit1 kernel: [505159.548755] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:46:08 linbit1 kernel: [505159.548893] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Terminating sender thread
Mar 13 20:46:08 linbit1 kernel: [505159.548941] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Starting sender thread (from drbd_r_pvc-01ec [2445633])
Mar 13 20:46:08 linbit1 kernel: [505159.602550] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Connection closed
Mar 13 20:46:08 linbit1 kernel: [505159.602566] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:46:08 linbit1 kernel: [505159.602578] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Restarting receiver thread
Mar 13 20:46:08 linbit1 kernel: [505159.602588] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Unconnected -> Connecting )
Mar 13 20:46:09 linbit1 kernel: [505160.130426] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:46:09 linbit1 kernel: [505160.130435] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:46:09 linbit1 kernel: [505160.130595] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:46:09 linbit1 kernel: [505160.318253] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 3007287420 (0->1 499/146)
Mar 13 20:46:09 linbit1 kernel: [505160.334404] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: drbd_sync_handshake:
Mar 13 20:46:09 linbit1 kernel: [505160.334413] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: self 6E639916C4467B00:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:120
Mar 13 20:46:09 linbit1 kernel: [505160.334422] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: peer 6E639916C4467B00:0000000000000000:8E546FDABA594BCA:0000000000000000 bits:0 flags:1120
Mar 13 20:46:09 linbit1 kernel: [505160.334430] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: uuid_compare()=no-sync by rule=reconnected
Mar 13 20:46:09 linbit1 kernel: [505160.334447] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change 3007287420: primary_nodes=0, weak_nodes=0
Mar 13 20:46:09 linbit1 kernel: [505160.334453] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 3007287420 (16ms)
Mar 13 20:46:09 linbit1 kernel: [505160.334504] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:46:09 linbit1 kernel: [505160.334510] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:46:09 linbit1 kernel: [505160.334562] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: cleared bm UUID and bitmap 6E639916C4467B00:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:46:28 linbit1 kernel: [505179.236709] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: sock was shut down by peer
Mar 13 20:46:28 linbit1 kernel: [505179.236729] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:46:28 linbit1 kernel: [505179.236736] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:46:28 linbit1 kernel: [505179.236825] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Terminating sender thread
Mar 13 20:46:28 linbit1 kernel: [505179.236864] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Starting sender thread (from drbd_r_pvc-7a5e [2450809])
Mar 13 20:46:28 linbit1 kernel: [505179.266242] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Connection closed
Mar 13 20:46:28 linbit1 kernel: [505179.266255] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:46:28 linbit1 kernel: [505179.266265] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Restarting receiver thread
Mar 13 20:46:28 linbit1 kernel: [505179.266273] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( Unconnected -> Connecting )
Mar 13 20:46:28 linbit1 kernel: [505179.810336] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 20:46:28 linbit1 kernel: [505179.810345] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:46:28 linbit1 kernel: [505179.810538] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:46:28 linbit1 kernel: [505180.004678] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Preparing remote state change 971032460
Mar 13 20:46:29 linbit1 kernel: [505180.022266] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: my exposed UUID: B6CEB0CB01A622F8
Mar 13 20:46:29 linbit1 kernel: [505180.022274] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: peer B6CEB0CB01A622F8:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:1120
Mar 13 20:46:29 linbit1 kernel: [505180.024681] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Committing remote state change 971032460 (primary_nodes=0)
Mar 13 20:46:29 linbit1 kernel: [505180.024696] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:46:29 linbit1 kernel: [505180.024702] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:46:47 linbit1 kernel: [505198.588669] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Preparing remote state change 2417808908
Mar 13 20:46:47 linbit1 kernel: [505198.629272] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Committing remote state change 2417808908 (primary_nodes=0)
Mar 13 20:48:45 linbit1 systemd[2625]: Started snap.microk8s.kubectl.e6ef1964-d705-418e-8497-e4f0446c9bb1.scope.
Mar 13 20:50:18 linbit1 systemd[2625]: Started snap.microk8s.kubectl.de39c6ab-cd44-4b33-8c77-7c8a2c173cad.scope.
Mar 13 20:50:27 linbit1 systemd[2625]: Started snap.microk8s.kubectl.65cf1d2f-00fe-4756-860f-a045856ac749.scope.
Mar 13 20:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:50:40.343340    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:50:40.351210    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:50:41 linbit1 systemd[2625]: Started snap.microk8s.kubectl.d113a7ea-8d29-4be3-a3c3-1684a214da9b.scope.
Mar 13 20:51:05 linbit1 systemd[2625]: Started snap.microk8s.kubectl.ce840e3c-2c3d-45eb-8670-ac5ff1ff73f5.scope.
Mar 13 20:51:08 linbit1 systemd[2625]: Started snap.microk8s.kubectl.bbebd69b-d45d-4472-9fec-804138aecea3.scope.
Mar 13 20:51:15 linbit1 microk8s.daemon-apiserver-kicker[2473335]: chmod: cannot access '/var/snap/microk8s/4567/var/kubernetes/backend/0000000056240661-0000000056241083': No such file or directory
Mar 13 20:51:23 linbit1 systemd[2625]: Started snap.microk8s.kubectl.730cac52-3d8c-4689-974e-599c355666b2.scope.
Mar 13 20:51:34 linbit1 systemd[2625]: Started snap.microk8s.kubectl.fc13ca77-fc1f-49e7-a705-caebc343651d.scope.
Mar 13 20:51:35 linbit1 systemd[2625]: Started snap.microk8s.kubectl.b7446a7b-ac01-4dea-8c9c-10d2e9225644.scope.
Mar 13 20:51:35 linbit1 systemd[2625]: Started snap.microk8s.kubectl.971b1bae-71df-43e6-b71e-11fd91709f61.scope.
Mar 13 20:51:35 linbit1 systemd[2625]: Started snap.microk8s.kubectl.bdf76345-20d9-4f4e-a6c0-aae156f4017f.scope.
Mar 13 20:51:35 linbit1 systemd[2625]: Started snap.microk8s.kubectl.72e022a4-6cf5-416f-8c4e-cfe94ecf673d.scope.
Mar 13 20:51:35 linbit1 systemd[2625]: Started snap.microk8s.kubectl.073964a5-7f7c-456c-909d-1a4c94a04c07.scope.
Mar 13 20:51:38 linbit1 systemd[2625]: Started snap.microk8s.kubectl.e3bcff01-b05e-412c-a120-6e4813a437ea.scope.
Mar 13 20:51:49 linbit1 systemd[2625]: Started snap.microk8s.kubectl.1c3a1987-5a81-48aa-b0cf-1eac1a98b6ba.scope.
Mar 13 20:55:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 20:55:40.343892    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 20:55:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 20:55:40.352926    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 20:55:45 linbit1 kernel: [505736.847058] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: sock was shut down by peer
Mar 13 20:55:45 linbit1 kernel: [505736.847079] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:55:45 linbit1 kernel: [505736.847085] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 20:55:45 linbit1 kernel: [505736.847177] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Terminating sender thread
Mar 13 20:55:45 linbit1 kernel: [505736.847217] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Starting sender thread (from drbd_r_pvc-0f95 [2453907])
Mar 13 20:55:45 linbit1 kernel: [505736.900450] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Connection closed
Mar 13 20:55:45 linbit1 kernel: [505736.900466] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 20:55:45 linbit1 kernel: [505736.900478] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Restarting receiver thread
Mar 13 20:55:45 linbit1 kernel: [505736.900487] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Unconnected -> Connecting )
Mar 13 20:55:46 linbit1 kernel: [505737.431218] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 20:55:46 linbit1 kernel: [505737.431230] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:55:46 linbit1 kernel: [505737.431365] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:55:46 linbit1 kernel: [505737.527023] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Preparing remote state change 2508110010
Mar 13 20:55:46 linbit1 kernel: [505737.544278] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: my exposed UUID: AE7361DB2B2FCAB8
Mar 13 20:55:46 linbit1 kernel: [505737.544287] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: peer AE7361DB2B2FCAB8:0000000000000000:B228721D04295568:0000000000000000 bits:0 flags:1120
Mar 13 20:55:46 linbit1 kernel: [505737.544467] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Committing remote state change 2508110010 (primary_nodes=0)
Mar 13 20:55:46 linbit1 kernel: [505737.544483] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:55:46 linbit1 kernel: [505737.544488] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 20:56:34 linbit1 kernel: [505785.139000] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Preparing remote state change 2849688486
Mar 13 20:56:34 linbit1 kernel: [505785.183126] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Committing remote state change 2849688486 (primary_nodes=0)
Mar 13 20:58:13 linbit1 kernel: [505884.570571] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: sock was shut down by peer
Mar 13 20:58:13 linbit1 kernel: [505884.570595] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:58:13 linbit1 kernel: [505884.570601] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 20:58:13 linbit1 kernel: [505884.570706] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Terminating sender thread
Mar 13 20:58:13 linbit1 kernel: [505884.570758] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Starting sender thread (from drbd_r_pvc-8f12 [2452407])
Mar 13 20:58:13 linbit1 kernel: [505884.619971] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Connection closed
Mar 13 20:58:13 linbit1 kernel: [505884.619987] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 20:58:13 linbit1 kernel: [505884.619998] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Restarting receiver thread
Mar 13 20:58:13 linbit1 kernel: [505884.620008] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Unconnected -> Connecting )
Mar 13 20:58:14 linbit1 kernel: [505885.151858] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:58:14 linbit1 kernel: [505885.151866] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:58:14 linbit1 kernel: [505885.152086] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:58:14 linbit1 kernel: [505885.179697] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 727771154 (0->2 499/146)
Mar 13 20:58:14 linbit1 kernel: [505885.195707] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: self 49EB060E1205E2B4:FE91709162BFE132:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:58:14 linbit1 kernel: [505885.195718] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: peer's exposed UUID: 49EB060E1205E2B4
Mar 13 20:58:14 linbit1 kernel: [505885.198615] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: State change 727771154: primary_nodes=0, weak_nodes=0
Mar 13 20:58:14 linbit1 kernel: [505885.198622] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 727771154 (16ms)
Mar 13 20:58:14 linbit1 kernel: [505885.198660] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:58:14 linbit1 kernel: [505885.198666] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:58:14 linbit1 kernel: [505885.198720] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: cleared bm UUID and bitmap 49EB060E1205E2B4:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:59:27 linbit1 kernel: [505958.686446] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Preparing remote state change 3744857211
Mar 13 20:59:27 linbit1 kernel: [505958.730383] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Committing remote state change 3744857211 (primary_nodes=0)
Mar 13 20:59:58 linbit1 kernel: [505989.734244] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: sock was shut down by peer
Mar 13 20:59:58 linbit1 kernel: [505989.734267] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:59:58 linbit1 kernel: [505989.734274] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 20:59:58 linbit1 kernel: [505989.734370] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Terminating sender thread
Mar 13 20:59:58 linbit1 kernel: [505989.734407] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Starting sender thread (from drbd_r_pvc-9428 [2451976])
Mar 13 20:59:58 linbit1 kernel: [505989.763523] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Connection closed
Mar 13 20:59:58 linbit1 kernel: [505989.763537] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 20:59:58 linbit1 kernel: [505989.763549] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Restarting receiver thread
Mar 13 20:59:58 linbit1 kernel: [505989.763558] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Unconnected -> Connecting )
Mar 13 20:59:59 linbit1 kernel: [505990.046168] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: sock was shut down by peer
Mar 13 20:59:59 linbit1 kernel: [505990.046189] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 20:59:59 linbit1 kernel: [505990.046195] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 20:59:59 linbit1 kernel: [505990.046283] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Terminating sender thread
Mar 13 20:59:59 linbit1 kernel: [505990.046323] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Starting sender thread (from drbd_r_pvc-01ec [2445634])
Mar 13 20:59:59 linbit1 kernel: [505990.054267] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Preparing remote state change 1936881006
Mar 13 20:59:59 linbit1 kernel: [505990.083511] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Connection closed
Mar 13 20:59:59 linbit1 kernel: [505990.083525] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 20:59:59 linbit1 kernel: [505990.083536] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Restarting receiver thread
Mar 13 20:59:59 linbit1 kernel: [505990.083546] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Unconnected -> Connecting )
Mar 13 20:59:59 linbit1 kernel: [505990.110362] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Committing remote state change 1936881006 (primary_nodes=0)
Mar 13 20:59:59 linbit1 kernel: [505990.286421] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:59:59 linbit1 kernel: [505990.286431] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:59:59 linbit1 kernel: [505990.286599] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 20:59:59 linbit1 kernel: [505990.343379] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 3019923745 (0->2 499/146)
Mar 13 20:59:59 linbit1 kernel: [505990.371344] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: self 65129730F932FD8A:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:59:59 linbit1 kernel: [505990.371354] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: peer's exposed UUID: 65129730F932FD8A
Mar 13 20:59:59 linbit1 kernel: [505990.371372] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change 3019923745: primary_nodes=0, weak_nodes=0
Mar 13 20:59:59 linbit1 kernel: [505990.371379] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 3019923745 (28ms)
Mar 13 20:59:59 linbit1 kernel: [505990.371422] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:59:59 linbit1 kernel: [505990.371428] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:59:59 linbit1 kernel: [505990.371476] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: cleared bm UUID and bitmap 65129730F932FD8A:0000000000000000:0000000000000000:0000000000000000
Mar 13 20:59:59 linbit1 kernel: [505990.614249] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 20:59:59 linbit1 kernel: [505990.614258] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 20:59:59 linbit1 kernel: [505990.614386] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 20:59:59 linbit1 kernel: [505990.675344] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 1972004473 (0->2 499/146)
Mar 13 20:59:59 linbit1 kernel: [505990.703346] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: self 6E639916C4467B00:0000000000000000:0000000000000000:0000000000000000 bits:0 flags:0
Mar 13 20:59:59 linbit1 kernel: [505990.703356] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: peer's exposed UUID: 6E639916C4467B00
Mar 13 20:59:59 linbit1 kernel: [505990.703369] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change 1972004473: primary_nodes=0, weak_nodes=0
Mar 13 20:59:59 linbit1 kernel: [505990.703375] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 1972004473 (28ms)
Mar 13 20:59:59 linbit1 kernel: [505990.703426] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 20:59:59 linbit1 kernel: [505990.703432] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: pdsk( DUnknown -> Diskless ) repl( Off -> Established )
Mar 13 20:59:59 linbit1 kernel: [505990.703484] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: cleared bm UUID and bitmap 6E639916C4467B00:0000000000000000:0000000000000000:0000000000000000
Mar 13 21:00:22 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Gog8BN.mount: Deactivated successfully.
Mar 13 21:00:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:00:40.342173    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:00:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:00:40.350782    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:00:42 linbit1 kernel: [506033.710170] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Preparing remote state change 1611087027
Mar 13 21:00:42 linbit1 kernel: [506033.742170] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Committing remote state change 1611087027 (primary_nodes=0)
Mar 13 21:01:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.j54Iwe.mount: Deactivated successfully.
Mar 13 21:02:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.97H7rI.mount: Deactivated successfully.
Mar 13 21:02:37 linbit1 kernel: [506148.765679] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: sock was shut down by peer
Mar 13 21:02:37 linbit1 kernel: [506148.765703] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:02:37 linbit1 kernel: [506148.765711] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:02:37 linbit1 kernel: [506148.765814] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Terminating sender thread
Mar 13 21:02:37 linbit1 kernel: [506148.765868] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Starting sender thread (from drbd_r_pvc-7a5e [2450811])
Mar 13 21:02:37 linbit1 kernel: [506148.810929] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Connection closed
Mar 13 21:02:37 linbit1 kernel: [506148.810944] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:02:37 linbit1 kernel: [506148.810956] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Restarting receiver thread
Mar 13 21:02:37 linbit1 kernel: [506148.810966] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:02:38 linbit1 kernel: [506149.343013] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 21:02:38 linbit1 kernel: [506149.343024] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:02:38 linbit1 kernel: [506149.343214] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 21:02:38 linbit1 kernel: [506149.389668] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Preparing remote state change 1989259783
Mar 13 21:02:38 linbit1 kernel: [506149.402913] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: my exposed UUID: B6CEB0CB01A622F8
Mar 13 21:02:38 linbit1 kernel: [506149.402923] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: peer B6CEB0CB01A622F8:0000000000000000:4B43951C0E42492A:0000000000000000 bits:0 flags:1120
Mar 13 21:02:38 linbit1 kernel: [506149.437699] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Committing remote state change 1989259783 (primary_nodes=0)
Mar 13 21:02:38 linbit1 kernel: [506149.437718] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:02:38 linbit1 kernel: [506149.437724] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: pdsk( DUnknown -> UpToDate ) repl( Off -> Established )
Mar 13 21:04:31 linbit1 kernel: [506262.657486] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Preparing remote state change 2547107281
Mar 13 21:04:31 linbit1 kernel: [506262.689464] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Committing remote state change 2547107281 (primary_nodes=0)
Mar 13 21:05:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:05:40.343611    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:05:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:05:40.352055    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:05:52 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.Xy6yk1.mount: Deactivated successfully.
Mar 13 21:07:12 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.QVb2Gp.mount: Deactivated successfully.
Mar 13 21:08:06 linbit1 systemd[2625]: Started snap.microk8s.kubectl.fc215d3f-1dfa-4b72-bfba-e25ddf822609.scope.
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.530078    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=05dd1efc-8486-47dd-89df-7ee739a2d7e8 path="/var/snap/microk8s/common/var/lib/kubelet/pods/05dd1efc-8486-47dd-89df-7ee739a2d7e8/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.531004    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=163fb3b6-cd3e-4b72-8f21-6a84e97abdc3 path="/var/snap/microk8s/common/var/lib/kubelet/pods/163fb3b6-cd3e-4b72-8f21-6a84e97abdc3/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.531894    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=1a7e1b25-e333-4558-9a33-7f7b07d5d61c path="/var/snap/microk8s/common/var/lib/kubelet/pods/1a7e1b25-e333-4558-9a33-7f7b07d5d61c/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.532748    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f path="/var/snap/microk8s/common/var/lib/kubelet/pods/1e2dd0e4-53f1-4f9c-b6e7-94bbf8fdd70f/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.533594    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=23c28465-eb55-4375-9bd1-5fd757745f89 path="/var/snap/microk8s/common/var/lib/kubelet/pods/23c28465-eb55-4375-9bd1-5fd757745f89/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.534435    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=5d081096-b326-416f-9081-50608c26d3d1 path="/var/snap/microk8s/common/var/lib/kubelet/pods/5d081096-b326-416f-9081-50608c26d3d1/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.535576    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=6352c9ef-1397-436e-b40b-a40ae673de66 path="/var/snap/microk8s/common/var/lib/kubelet/pods/6352c9ef-1397-436e-b40b-a40ae673de66/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.536076    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=75ce48a0-322f-463e-9859-01950deee06d path="/var/snap/microk8s/common/var/lib/kubelet/pods/75ce48a0-322f-463e-9859-01950deee06d/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.536568    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=ad7770d4-2d2d-4215-a027-2d52b0e30ae0 path="/var/snap/microk8s/common/var/lib/kubelet/pods/ad7770d4-2d2d-4215-a027-2d52b0e30ae0/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.537046    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=cc6de1a1-21c4-4733-838a-eb8fcbbe552d path="/var/snap/microk8s/common/var/lib/kubelet/pods/cc6de1a1-21c4-4733-838a-eb8fcbbe552d/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.537525    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=d9db77e8-50b0-4260-89f3-ddb5a0de385a path="/var/snap/microk8s/common/var/lib/kubelet/pods/d9db77e8-50b0-4260-89f3-ddb5a0de385a/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.537993    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=e5888ce9-60e2-4ded-94a7-5ac6c28650b6 path="/var/snap/microk8s/common/var/lib/kubelet/pods/e5888ce9-60e2-4ded-94a7-5ac6c28650b6/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.538485    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=f15c9220-13bf-47a1-93d8-bf5be8a9a7a5 path="/var/snap/microk8s/common/var/lib/kubelet/pods/f15c9220-13bf-47a1-93d8-bf5be8a9a7a5/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.538958    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=f3b42727-fd59-4af5-a4e2-dc97ffd00b2d path="/var/snap/microk8s/common/var/lib/kubelet/pods/f3b42727-fd59-4af5-a4e2-dc97ffd00b2d/volumes"
Mar 13 21:08:08 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:08.539432    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=fa760197-68d3-48e7-abee-bc22cb31e820 path="/var/snap/microk8s/common/var/lib/kubelet/pods/fa760197-68d3-48e7-abee-bc22cb31e820/volumes"
Mar 13 21:08:09 linbit1 kernel: [506480.358044] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:09 linbit1 kernel: [506480.370212] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74: State change failed: State change was refused by peer node
Mar 13 21:08:09 linbit1 kernel: [506480.380899] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:09 linbit1 kernel: [506480.380945] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:09 linbit1 kernel: [506480.393105] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74: State change failed: State change was refused by peer node
Mar 13 21:08:09 linbit1 kernel: [506480.403788] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74/0 drbd1007 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:09 linbit1 kernel: [506480.501474] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Preparing remote state change 1033385350
Mar 13 21:08:09 linbit1 kernel: [506480.501655] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Committing remote state change 1033385350 (primary_nodes=0)
Mar 13 21:08:09 linbit1 kernel: [506480.501771] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:09 linbit1 kernel: [506480.501862] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:09 linbit1 kernel: [506480.517671] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Preparing remote state change 2805034489
Mar 13 21:08:09 linbit1 kernel: [506480.517897] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Committing remote state change 2805034489 (primary_nodes=0)
Mar 13 21:08:09 linbit1 kernel: [506480.518298] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:09 linbit1 kernel: [506480.518711] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:09 linbit1 kernel: [506480.698911] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:09 linbit1 kernel: [506480.711078] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a: State change failed: State change was refused by peer node
Mar 13 21:08:09 linbit1 kernel: [506480.721764] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:09 linbit1 kernel: [506480.721803] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:09 linbit1 kernel: [506480.733962] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a: State change failed: State change was refused by peer node
Mar 13 21:08:09 linbit1 kernel: [506480.744646] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a/0 drbd1031 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:09 linbit1 kernel: [506480.761074] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 1486606417 (0->-1 7680/1024)
Mar 13 21:08:09 linbit1 kernel: [506480.761319] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change 1486606417: primary_nodes=0, weak_nodes=0
Mar 13 21:08:09 linbit1 kernel: [506480.761325] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 1486606417 (0ms)
Mar 13 21:08:09 linbit1 kernel: [506480.761358] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: disk( UpToDate -> Detaching )
Mar 13 21:08:09 linbit1 kernel: [506480.761479] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: disk( Detaching -> Diskless )
Mar 13 21:08:09 linbit1 kernel: [506480.762516] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: drbd_bm_resize called with capacity == 0
Mar 13 21:08:10 linbit1 kernel: [506481.039567] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:10 linbit1 kernel: [506481.051734] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change failed: State change was refused by peer node
Mar 13 21:08:10 linbit1 kernel: [506481.062421] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:10 linbit1 kernel: [506481.062465] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:10 linbit1 kernel: [506481.074625] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change failed: State change was refused by peer node
Mar 13 21:08:10 linbit1 kernel: [506481.085308] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000: Failed: quorum( yes -> no )
Mar 13 21:08:10 linbit1 kernel: [506481.085313] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0/0 drbd1000 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:10 linbit1 kernel: [506481.129205] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Preparing cluster-wide state change 1404644433 (0->-1 7680/1024)
Mar 13 21:08:10 linbit1 kernel: [506481.129457] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: State change 1404644433: primary_nodes=0, weak_nodes=0
Mar 13 21:08:10 linbit1 kernel: [506481.129462] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Committing cluster-wide state change 1404644433 (0ms)
Mar 13 21:08:10 linbit1 kernel: [506481.129494] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: disk( UpToDate -> Detaching )
Mar 13 21:08:10 linbit1 kernel: [506481.129655] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: disk( Detaching -> Diskless )
Mar 13 21:08:10 linbit1 kernel: [506481.130597] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: drbd_bm_resize called with capacity == 0
Mar 13 21:08:10 linbit1 kernel: [506481.431666] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:10 linbit1 kernel: [506481.443833] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: State change failed: State change was refused by peer node
Mar 13 21:08:10 linbit1 kernel: [506481.454518] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:10 linbit1 kernel: [506481.454557] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:10 linbit1 kernel: [506481.466717] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: State change failed: State change was refused by peer node
Mar 13 21:08:10 linbit1 kernel: [506481.477400] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006: Failed: quorum( yes -> no )
Mar 13 21:08:10 linbit1 kernel: [506481.477404] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc/0 drbd1006 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:10 linbit1 kernel: [506481.512592] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:10 linbit1 kernel: [506481.524757] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c: State change failed: State change was refused by peer node
Mar 13 21:08:10 linbit1 kernel: [506481.535443] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:10 linbit1 kernel: [506481.535483] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:10 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:08:10.528973    2448 kubelet_volumes.go:160] "Cleaned up orphaned pod volumes dir" podUID=f49ecb96-a6f0-48a0-b80c-53dd0806643d path="/var/snap/microk8s/common/var/lib/kubelet/pods/f49ecb96-a6f0-48a0-b80c-53dd0806643d/volumes"
Mar 13 21:08:10 linbit1 kernel: [506481.547643] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c: State change failed: State change was refused by peer node
Mar 13 21:08:10 linbit1 kernel: [506481.558327] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c/0 drbd1035 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:10 linbit1 kernel: [506481.581550] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Preparing cluster-wide state change 415334599 (1->-1 7680/1024)
Mar 13 21:08:10 linbit1 kernel: [506481.581910] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: State change 415334599: primary_nodes=0, weak_nodes=0
Mar 13 21:08:10 linbit1 kernel: [506481.581916] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Committing cluster-wide state change 415334599 (4ms)
Mar 13 21:08:10 linbit1 kernel: [506481.581950] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: disk( UpToDate -> Detaching )
Mar 13 21:08:10 linbit1 kernel: [506481.582122] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: disk( Detaching -> Diskless )
Mar 13 21:08:10 linbit1 kernel: [506481.583150] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: drbd_bm_resize called with capacity == 0
Mar 13 21:08:10 linbit1 kernel: [506481.875481] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: State change failed: State change was refused by peer node
Mar 13 21:08:10 linbit1 kernel: [506481.886173] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:10 linbit1 kernel: [506481.886211] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: State change failed: State change was refused by peer node
Mar 13 21:08:10 linbit1 kernel: [506481.896896] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009: Failed: quorum( yes -> no )
Mar 13 21:08:10 linbit1 kernel: [506481.896900] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae/0 drbd1009 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:10 linbit1 kernel: [506481.916971] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Preparing cluster-wide state change 262047917 (0->-1 7680/1024)
Mar 13 21:08:10 linbit1 kernel: [506481.917241] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: State change 262047917: primary_nodes=0, weak_nodes=0
Mar 13 21:08:10 linbit1 kernel: [506481.917245] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Committing cluster-wide state change 262047917 (0ms)
Mar 13 21:08:10 linbit1 kernel: [506481.917269] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: disk( UpToDate -> Detaching )
Mar 13 21:08:10 linbit1 kernel: [506481.917396] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: disk( Detaching -> Diskless )
Mar 13 21:08:10 linbit1 kernel: [506481.918314] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: drbd_bm_resize called with capacity == 0
Mar 13 21:08:11 linbit1 kernel: [506482.034076] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Preparing remote state change 771120321
Mar 13 21:08:11 linbit1 kernel: [506482.034412] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Committing remote state change 771120321 (primary_nodes=0)
Mar 13 21:08:11 linbit1 kernel: [506482.034526] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:11 linbit1 kernel: [506482.034632] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:11 linbit1 kernel: [506482.240343] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: State change failed: State change was refused by peer node
Mar 13 21:08:11 linbit1 kernel: [506482.251035] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:11 linbit1 kernel: [506482.251081] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: State change failed: State change was refused by peer node
Mar 13 21:08:11 linbit1 kernel: [506482.261766] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003: Failed: quorum( yes -> no )
Mar 13 21:08:11 linbit1 kernel: [506482.261770] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a/0 drbd1003 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:11 linbit1 kernel: [506482.289031] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Preparing cluster-wide state change 1605528591 (0->-1 7680/1024)
Mar 13 21:08:11 linbit1 kernel: [506482.289323] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: State change 1605528591: primary_nodes=0, weak_nodes=0
Mar 13 21:08:11 linbit1 kernel: [506482.289328] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Committing cluster-wide state change 1605528591 (0ms)
Mar 13 21:08:11 linbit1 kernel: [506482.289357] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: disk( UpToDate -> Detaching )
Mar 13 21:08:11 linbit1 kernel: [506482.289483] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: disk( Detaching -> Diskless )
Mar 13 21:08:11 linbit1 kernel: [506482.290366] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: drbd_bm_resize called with capacity == 0
Mar 13 21:08:11 linbit1 kernel: [506482.589573] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: State change failed: State change was refused by peer node
Mar 13 21:08:11 linbit1 kernel: [506482.600265] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:11 linbit1 kernel: [506482.600312] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: State change failed: State change was refused by peer node
Mar 13 21:08:11 linbit1 kernel: [506482.610997] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002: Failed: quorum( yes -> no )
Mar 13 21:08:11 linbit1 kernel: [506482.611001] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b/0 drbd1002 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:11 linbit1 kernel: [506482.656312] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee: State change failed: State change was refused by peer node
Mar 13 21:08:11 linbit1 kernel: [506482.667001] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:11 linbit1 kernel: [506482.667072] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee: State change failed: State change was refused by peer node
Mar 13 21:08:11 linbit1 kernel: [506482.677756] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee/0 drbd1030 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:11 linbit1 kernel: [506482.688341] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Preparing remote state change 3863447949
Mar 13 21:08:11 linbit1 kernel: [506482.688582] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Committing remote state change 3863447949 (primary_nodes=0)
Mar 13 21:08:11 linbit1 kernel: [506482.688686] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:11 linbit1 kernel: [506482.688783] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:11 linbit1 kernel: [506482.713086] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 4232542866 (0->-1 7680/1024)
Mar 13 21:08:11 linbit1 kernel: [506482.713456] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: State change 4232542866: primary_nodes=0, weak_nodes=0
Mar 13 21:08:11 linbit1 kernel: [506482.713462] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 4232542866 (0ms)
Mar 13 21:08:11 linbit1 kernel: [506482.713495] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: disk( UpToDate -> Detaching )
Mar 13 21:08:11 linbit1 kernel: [506482.713624] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: disk( Detaching -> Diskless )
Mar 13 21:08:11 linbit1 kernel: [506482.714481] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: drbd_bm_resize called with capacity == 0
Mar 13 21:08:11 linbit1 kernel: [506482.978100] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Preparing remote state change 640896911
Mar 13 21:08:11 linbit1 kernel: [506482.978276] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Committing remote state change 640896911 (primary_nodes=0)
Mar 13 21:08:11 linbit1 kernel: [506482.978365] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:11 linbit1 kernel: [506482.978457] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:12 linbit1 kernel: [506483.032654] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: State change failed: State change was refused by peer node
Mar 13 21:08:12 linbit1 kernel: [506483.043345] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012: Failed: quorum( yes -> no )
Mar 13 21:08:12 linbit1 kernel: [506483.043350] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:12 linbit1 kernel: [506483.043402] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: State change failed: State change was refused by peer node
Mar 13 21:08:12 linbit1 kernel: [506483.054087] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3/0 drbd1012 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:12 linbit1 kernel: [506483.336372] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Preparing remote state change 3968106246
Mar 13 21:08:12 linbit1 kernel: [506483.336619] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Committing remote state change 3968106246 (primary_nodes=0)
Mar 13 21:08:12 linbit1 kernel: [506483.336719] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:12 linbit1 kernel: [506483.336793] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:12 linbit1 kernel: [506483.368288] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Preparing remote state change 4082592844
Mar 13 21:08:12 linbit1 kernel: [506483.368530] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Committing remote state change 4082592844 (primary_nodes=0)
Mar 13 21:08:12 linbit1 kernel: [506483.368620] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:12 linbit1 kernel: [506483.368740] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:12 linbit1 kernel: [506483.752026] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Preparing remote state change 353651553
Mar 13 21:08:12 linbit1 kernel: [506483.752220] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Committing remote state change 353651553 (primary_nodes=0)
Mar 13 21:08:12 linbit1 kernel: [506483.752328] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:12 linbit1 kernel: [506483.752425] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:12 linbit1 kernel: [506483.789346] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Preparing remote state change 1162522661
Mar 13 21:08:12 linbit1 kernel: [506483.789478] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Committing remote state change 1162522661 (primary_nodes=0)
Mar 13 21:08:12 linbit1 kernel: [506483.789596] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:12 linbit1 kernel: [506483.789689] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:13 linbit1 kernel: [506484.224738] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Preparing remote state change 53409075
Mar 13 21:08:13 linbit1 kernel: [506484.224964] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Committing remote state change 53409075 (primary_nodes=0)
Mar 13 21:08:13 linbit1 kernel: [506484.225041] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:13 linbit1 kernel: [506484.225100] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:13 linbit1 kernel: [506484.656585] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Preparing remote state change 1437852000
Mar 13 21:08:13 linbit1 kernel: [506484.656815] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Committing remote state change 1437852000 (primary_nodes=0)
Mar 13 21:08:13 linbit1 kernel: [506484.656890] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:13 linbit1 kernel: [506484.656986] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:13 linbit1 kernel: [506484.698214] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:13 linbit1 kernel: [506484.698338] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Terminating sender thread
Mar 13 21:08:13 linbit1 kernel: [506484.698391] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Starting sender thread (from drbd_r_pvc-409c [2445337])
Mar 13 21:08:13 linbit1 kernel: [506484.757788] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Connection closed
Mar 13 21:08:13 linbit1 kernel: [506484.757805] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:13 linbit1 kernel: [506484.757811] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Terminating receiver thread
Mar 13 21:08:13 linbit1 kernel: [506484.757873] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit2: Terminating sender thread
Mar 13 21:08:13 linbit1 kernel: [506484.758049] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:13 linbit1 kernel: [506484.758128] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Terminating sender thread
Mar 13 21:08:13 linbit1 kernel: [506484.758154] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Starting sender thread (from drbd_r_pvc-409c [2445339])
Mar 13 21:08:13 linbit1 kernel: [506484.837783] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Connection closed
Mar 13 21:08:13 linbit1 kernel: [506484.837797] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:13 linbit1 kernel: [506484.837802] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Terminating receiver thread
Mar 13 21:08:13 linbit1 kernel: [506484.837860] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74 linbit3: Terminating sender thread
Mar 13 21:08:13 linbit1 kernel: [506484.877736] drbd pvc-409c1926-1f83-4e34-9689-1d30973c3c74: Terminating worker thread
Mar 13 21:08:14 linbit1 kernel: [506485.076288] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Preparing remote state change 3276733988
Mar 13 21:08:14 linbit1 kernel: [506485.076527] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Committing remote state change 3276733988 (primary_nodes=0)
Mar 13 21:08:14 linbit1 kernel: [506485.076632] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:14 linbit1 kernel: [506485.076738] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:14 linbit1 kernel: [506485.476056] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Preparing remote state change 1525686554
Mar 13 21:08:14 linbit1 kernel: [506485.476303] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Committing remote state change 1525686554 (primary_nodes=0)
Mar 13 21:08:14 linbit1 kernel: [506485.476394] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:14 linbit1 kernel: [506485.476484] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:14 linbit1 kernel: [506485.880374] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Preparing remote state change 1412319344
Mar 13 21:08:14 linbit1 kernel: [506485.880615] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Committing remote state change 1412319344 (primary_nodes=0)
Mar 13 21:08:14 linbit1 kernel: [506485.880710] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:14 linbit1 kernel: [506485.880806] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:18 linbit1 kernel: [506489.901356] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:18 linbit1 kernel: [506489.901419] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Terminating sender thread
Mar 13 21:08:18 linbit1 kernel: [506489.901438] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Starting sender thread (from drbd_r_pvc-1f71 [2445287])
Mar 13 21:08:18 linbit1 kernel: [506489.957757] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Connection closed
Mar 13 21:08:18 linbit1 kernel: [506489.957774] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:18 linbit1 kernel: [506489.957780] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Terminating receiver thread
Mar 13 21:08:18 linbit1 kernel: [506489.957842] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Terminating sender thread
Mar 13 21:08:18 linbit1 kernel: [506489.971827] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Starting sender thread (from drbdsetup [2485604])
Mar 13 21:08:18 linbit1 kernel: [506489.975001] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:18 linbit1 kernel: [506489.975033] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Starting receiver thread (from drbd_w_pvc-1f71 [2444383])
Mar 13 21:08:18 linbit1 kernel: [506489.975168] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:21 linbit1 kernel: [506492.109261] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.109343] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.109383] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Starting sender thread (from drbd_r_pvc-0f95 [2453904])
Mar 13 21:08:21 linbit1 kernel: [506492.144555] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: sock was shut down by peer
Mar 13 21:08:21 linbit1 kernel: [506492.144566] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.144634] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.144669] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Starting sender thread (from drbd_r_pvc-0f95 [2453907])
Mar 13 21:08:21 linbit1 kernel: [506492.165869] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.165886] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.165891] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Terminating receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.165997] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit2: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.166246] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( BrokenPipe -> Disconnecting )
Mar 13 21:08:21 linbit1 kernel: [506492.205750] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.205764] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.205769] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Terminating receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.205834] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.225637] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.225724] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.225755] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Starting sender thread (from drbd_r_pvc-20f0 [2445707])
Mar 13 21:08:21 linbit1 kernel: [506492.237711] drbd pvc-0f95b9c8-104d-4a2d-973a-2e200bbeaf2a: Terminating worker thread
Mar 13 21:08:21 linbit1 kernel: [506492.297759] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.297775] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.297781] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Terminating receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.297947] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.312590] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Starting sender thread (from drbdsetup [2485646])
Mar 13 21:08:21 linbit1 kernel: [506492.314890] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:21 linbit1 kernel: [506492.314916] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Starting receiver thread (from drbd_w_pvc-20f0 [2444772])
Mar 13 21:08:21 linbit1 kernel: [506492.315005] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:21 linbit1 kernel: [506492.317715] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.317783] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.317815] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Starting sender thread (from drbd_r_pvc-2262 [2454598])
Mar 13 21:08:21 linbit1 kernel: [506492.365751] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.365768] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.365774] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Terminating receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.365837] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit2: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.366120] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.366175] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.366199] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Starting sender thread (from drbd_r_pvc-2262 [2454600])
Mar 13 21:08:21 linbit1 kernel: [506492.425753] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.425766] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.425771] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Terminating receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.425837] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.439347] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.439436] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.439465] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Starting sender thread (from drbd_r_pvc-837c [2445415])
Mar 13 21:08:21 linbit1 kernel: [506492.457710] drbd pvc-2262d084-a733-49a7-9aaa-2f6a35a6a31c: Terminating worker thread
Mar 13 21:08:21 linbit1 kernel: [506492.468788] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: sock was shut down by peer
Mar 13 21:08:21 linbit1 kernel: [506492.468801] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.468864] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.468905] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Starting sender thread (from drbd_r_pvc-dfca [2451430])
Mar 13 21:08:21 linbit1 kernel: [506492.493748] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.493764] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.493770] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Terminating receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.493833] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.507555] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Starting sender thread (from drbdsetup [2485673])
Mar 13 21:08:21 linbit1 kernel: [506492.510310] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:21 linbit1 kernel: [506492.510338] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Starting receiver thread (from drbd_w_pvc-837c [2445393])
Mar 13 21:08:21 linbit1 kernel: [506492.510433] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:21 linbit1 kernel: [506492.513751] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.513759] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:21 linbit1 kernel: [506492.513768] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Restarting receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.513772] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:21 linbit1 kernel: [506492.522331] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.522413] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.522447] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Starting sender thread (from drbd_r_pvc-986e [2445771])
Mar 13 21:08:21 linbit1 kernel: [506492.589754] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.589772] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.589778] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Terminating receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.589847] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.604773] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Starting sender thread (from drbdsetup [2485692])
Mar 13 21:08:21 linbit1 kernel: [506492.607704] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:21 linbit1 kernel: [506492.607733] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Starting receiver thread (from drbd_w_pvc-986e [2444858])
Mar 13 21:08:21 linbit1 kernel: [506492.607836] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:21 linbit1 kernel: [506492.620522] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.620603] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.620634] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Starting sender thread (from drbd_r_pvc-d174 [2445824])
Mar 13 21:08:21 linbit1 kernel: [506492.685761] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.685778] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.685784] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Terminating receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.685854] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.699180] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Starting sender thread (from drbdsetup [2485711])
Mar 13 21:08:21 linbit1 kernel: [506492.702111] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:21 linbit1 kernel: [506492.702144] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Starting receiver thread (from drbd_w_pvc-d174 [2445011])
Mar 13 21:08:21 linbit1 kernel: [506492.702253] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:21 linbit1 kernel: [506492.705832] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:21 linbit1 kernel: [506492.705911] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.705921] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Starting sender thread (from drbd_r_pvc-dfca [2451430])
Mar 13 21:08:21 linbit1 kernel: [506492.753753] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.753768] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:21 linbit1 kernel: [506492.852530] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: sock was shut down by peer
Mar 13 21:08:21 linbit1 kernel: [506492.852537] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.852596] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.852628] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Starting sender thread (from drbd_r_pvc-dfca [2451432])
Mar 13 21:08:21 linbit1 kernel: [506492.897746] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.897755] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:21 linbit1 kernel: [506492.897763] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Restarting receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.897768] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:21 linbit1 kernel: [506492.900777] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: sock was shut down by peer
Mar 13 21:08:21 linbit1 kernel: [506492.900789] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:21 linbit1 kernel: [506492.900864] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Terminating sender thread
Mar 13 21:08:21 linbit1 kernel: [506492.900904] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Starting sender thread (from drbd_r_pvc-f35d [2448570])
Mar 13 21:08:21 linbit1 kernel: [506492.945785] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Connection closed
Mar 13 21:08:21 linbit1 kernel: [506492.945794] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:21 linbit1 kernel: [506492.945802] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Restarting receiver thread
Mar 13 21:08:21 linbit1 kernel: [506492.945808] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:22 linbit1 kernel: [506493.428715] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 21:08:22 linbit1 kernel: [506493.428726] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:08:22 linbit1 kernel: [506493.428869] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 21:08:22 linbit1 kernel: [506493.757698] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee: State change failed: Need a connection to start verify or resync
Mar 13 21:08:22 linbit1 kernel: [506493.768911] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Failed: conn( StandAlone -> Connecting )
Mar 13 21:08:22 linbit1 kernel: [506493.768930] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Terminating receiver thread
Mar 13 21:08:22 linbit1 kernel: [506493.769019] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit2: Terminating sender thread
Mar 13 21:08:22 linbit1 kernel: [506493.769371] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:22 linbit1 kernel: [506493.801759] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Terminating sender thread
Mar 13 21:08:22 linbit1 kernel: [506493.801770] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Starting sender thread (from drbd_r_pvc-dfca [2451432])
Mar 13 21:08:22 linbit1 kernel: [506493.841849] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Connection closed
Mar 13 21:08:22 linbit1 kernel: [506493.841868] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:22 linbit1 kernel: [506493.841873] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Terminating receiver thread
Mar 13 21:08:22 linbit1 kernel: [506493.842031] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee linbit3: Terminating sender thread
Mar 13 21:08:22 linbit1 kernel: [506493.850681] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:22 linbit1 kernel: [506493.850757] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Terminating sender thread
Mar 13 21:08:22 linbit1 kernel: [506493.850767] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Starting sender thread (from drbd_r_pvc-f35d [2448570])
Mar 13 21:08:22 linbit1 kernel: [506493.850909] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Connection closed
Mar 13 21:08:22 linbit1 kernel: [506493.850920] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:22 linbit1 kernel: [506493.850925] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Terminating receiver thread
Mar 13 21:08:22 linbit1 kernel: [506493.850987] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Terminating sender thread
Mar 13 21:08:22 linbit1 kernel: [506493.865213] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Starting sender thread (from drbdsetup [2485808])
Mar 13 21:08:22 linbit1 kernel: [506493.868476] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:08:22 linbit1 kernel: [506493.868512] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Starting receiver thread (from drbd_w_pvc-f35d [2446599])
Mar 13 21:08:22 linbit1 kernel: [506493.868637] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:22 linbit1 kernel: [506493.885709] drbd pvc-dfcae309-771d-4765-b4a3-521a43e863ee: Terminating worker thread
Mar 13 21:08:23 linbit1 kernel: [506494.506074] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:23 linbit1 kernel: [506494.506210] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.506223] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Starting sender thread (from drbd_r_pvc-1f71 [2485608])
Mar 13 21:08:23 linbit1 kernel: [506494.506466] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Connection closed
Mar 13 21:08:23 linbit1 kernel: [506494.506479] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:23 linbit1 kernel: [506494.506485] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Terminating receiver thread
Mar 13 21:08:23 linbit1 kernel: [506494.506544] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.852132] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:23 linbit1 kernel: [506494.852213] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.852229] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Starting sender thread (from drbd_r_pvc-20f0 [2485650])
Mar 13 21:08:23 linbit1 kernel: [506494.852457] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Connection closed
Mar 13 21:08:23 linbit1 kernel: [506494.852472] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:23 linbit1 kernel: [506494.852480] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Terminating receiver thread
Mar 13 21:08:23 linbit1 kernel: [506494.852583] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.877841] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:23 linbit1 kernel: [506494.877967] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.877979] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Starting sender thread (from drbd_r_pvc-837c [2485677])
Mar 13 21:08:23 linbit1 kernel: [506494.878171] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Connection closed
Mar 13 21:08:23 linbit1 kernel: [506494.878185] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:23 linbit1 kernel: [506494.878193] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Terminating receiver thread
Mar 13 21:08:23 linbit1 kernel: [506494.878254] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.904424] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:23 linbit1 kernel: [506494.904518] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.904531] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Starting sender thread (from drbd_r_pvc-986e [2485696])
Mar 13 21:08:23 linbit1 kernel: [506494.904743] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Connection closed
Mar 13 21:08:23 linbit1 kernel: [506494.904756] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:23 linbit1 kernel: [506494.904761] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Terminating receiver thread
Mar 13 21:08:23 linbit1 kernel: [506494.904872] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.926906] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:23 linbit1 kernel: [506494.926974] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.927002] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Starting sender thread (from drbd_r_pvc-d174 [2485715])
Mar 13 21:08:23 linbit1 kernel: [506494.927216] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Connection closed
Mar 13 21:08:23 linbit1 kernel: [506494.927228] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:23 linbit1 kernel: [506494.927233] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Terminating receiver thread
Mar 13 21:08:23 linbit1 kernel: [506494.927256] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit3: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.950809] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:23 linbit1 kernel: [506494.950876] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Terminating sender thread
Mar 13 21:08:23 linbit1 kernel: [506494.950894] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Starting sender thread (from drbd_r_pvc-f35d [2485812])
Mar 13 21:08:23 linbit1 kernel: [506494.951089] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Connection closed
Mar 13 21:08:23 linbit1 kernel: [506494.951101] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:23 linbit1 kernel: [506494.951107] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Terminating receiver thread
Mar 13 21:08:23 linbit1 kernel: [506494.951138] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit2: Terminating sender thread
Mar 13 21:08:24 linbit1 kernel: [506495.068806] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: sock was shut down by peer
Mar 13 21:08:24 linbit1 kernel: [506495.068818] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:24 linbit1 kernel: [506495.069013] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Terminating sender thread
Mar 13 21:08:24 linbit1 kernel: [506495.069059] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Starting sender thread (from drbd_r_pvc-1f71 [2445285])
Mar 13 21:08:24 linbit1 kernel: [506495.109975] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Connection closed
Mar 13 21:08:24 linbit1 kernel: [506495.109987] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:24 linbit1 kernel: [506495.109996] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Restarting receiver thread
Mar 13 21:08:24 linbit1 kernel: [506495.110001] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:24 linbit1 kernel: [506495.677899] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 21:08:24 linbit1 kernel: [506495.677909] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:08:24 linbit1 kernel: [506495.678092] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 21:08:24 linbit1 kernel: [506495.721701] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Preparing cluster-wide state change 2816451278 (0->1 499/146)
Mar 13 21:08:24 linbit1 kernel: [506495.721918] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: State change 2816451278: primary_nodes=0, weak_nodes=0
Mar 13 21:08:24 linbit1 kernel: [506495.721925] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Committing cluster-wide state change 2816451278 (0ms)
Mar 13 21:08:24 linbit1 kernel: [506495.721947] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:08:24 linbit1 kernel: [506495.996799] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: sock was shut down by peer
Mar 13 21:08:24 linbit1 kernel: [506495.996811] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:24 linbit1 kernel: [506495.996908] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Terminating sender thread
Mar 13 21:08:24 linbit1 kernel: [506495.996950] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Starting sender thread (from drbd_r_pvc-d174 [2445823])
Mar 13 21:08:25 linbit1 kernel: [506496.037856] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Connection closed
Mar 13 21:08:25 linbit1 kernel: [506496.037864] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:25 linbit1 kernel: [506496.037872] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Restarting receiver thread
Mar 13 21:08:25 linbit1 kernel: [506496.037878] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:25 linbit1 kernel: [506496.241555] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:25 linbit1 kernel: [506496.241602] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: sock_recvmsg returned -4
Mar 13 21:08:25 linbit1 kernel: [506496.241723] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.241732] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Starting sender thread (from drbd_r_pvc-1f71 [2445285])
Mar 13 21:08:25 linbit1 kernel: [506496.297859] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Connection closed
Mar 13 21:08:25 linbit1 kernel: [506496.297877] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:25 linbit1 kernel: [506496.297882] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Terminating receiver thread
Mar 13 21:08:25 linbit1 kernel: [506496.298042] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0 linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.317704] drbd pvc-1f7111f9-abe0-4526-abf4-8bb246304fd0: Terminating worker thread
Mar 13 21:08:25 linbit1 kernel: [506496.323969] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:25 linbit1 kernel: [506496.324052] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.324087] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Starting sender thread (from drbd_r_pvc-20f0 [2445705])
Mar 13 21:08:25 linbit1 kernel: [506496.376530] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: sock was shut down by peer
Mar 13 21:08:25 linbit1 kernel: [506496.376542] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:25 linbit1 kernel: [506496.376617] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.376658] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Starting sender thread (from drbd_r_pvc-f35d [2448572])
Mar 13 21:08:25 linbit1 kernel: [506496.421930] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Connection closed
Mar 13 21:08:25 linbit1 kernel: [506496.421949] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:25 linbit1 kernel: [506496.421955] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Terminating receiver thread
Mar 13 21:08:25 linbit1 kernel: [506496.422031] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.425862] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:25 linbit1 kernel: [506496.425932] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.425963] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Starting sender thread (from drbd_r_pvc-837c [2445412])
Mar 13 21:08:25 linbit1 kernel: [506496.449726] drbd pvc-20f0b4c5-4f16-4b65-a52c-fb6c96b56cbc: Terminating worker thread
Mar 13 21:08:25 linbit1 kernel: [506496.449976] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Connection closed
Mar 13 21:08:25 linbit1 kernel: [506496.449987] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:25 linbit1 kernel: [506496.449997] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Restarting receiver thread
Mar 13 21:08:25 linbit1 kernel: [506496.450002] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:25 linbit1 kernel: [506496.477922] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Connection closed
Mar 13 21:08:25 linbit1 kernel: [506496.477941] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:25 linbit1 kernel: [506496.477946] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Terminating receiver thread
Mar 13 21:08:25 linbit1 kernel: [506496.477994] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.481345] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:25 linbit1 kernel: [506496.481429] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.481464] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Starting sender thread (from drbd_r_pvc-986e [2445770])
Mar 13 21:08:25 linbit1 kernel: [506496.505698] drbd pvc-837cb078-6fac-4237-8c03-6184acb314ae: Terminating worker thread
Mar 13 21:08:25 linbit1 kernel: [506496.533896] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Connection closed
Mar 13 21:08:25 linbit1 kernel: [506496.533913] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:25 linbit1 kernel: [506496.533924] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Terminating receiver thread
Mar 13 21:08:25 linbit1 kernel: [506496.533960] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.550460] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:25 linbit1 kernel: [506496.550643] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Terminating sender thread
Mar 13 21:08:25 linbit1 kernel: [506496.550720] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Starting sender thread (from drbd_r_pvc-d174 [2445823])
Mar 13 21:08:25 linbit1 kernel: [506496.561695] drbd pvc-986e7e35-ae6d-4e62-b90d-191e128bd24a: Terminating worker thread
Mar 13 21:08:25 linbit1 kernel: [506496.609936] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Connection closed
Mar 13 21:08:25 linbit1 kernel: [506496.609954] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:25 linbit1 kernel: [506496.980675] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Handshake to peer 1 successful: Agreed network protocol version 121
Mar 13 21:08:25 linbit1 kernel: [506496.980686] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:08:25 linbit1 kernel: [506496.980824] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 21:08:26 linbit1 kernel: [506497.073698] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Preparing cluster-wide state change 2086706100 (0->1 499/146)
Mar 13 21:08:26 linbit1 kernel: [506497.073991] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: State change 2086706100: primary_nodes=0, weak_nodes=0
Mar 13 21:08:26 linbit1 kernel: [506497.073998] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Committing cluster-wide state change 2086706100 (0ms)
Mar 13 21:08:26 linbit1 kernel: [506497.074021] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:08:26 linbit1 kernel: [506497.165322] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Preparing remote state change 64291981
Mar 13 21:08:26 linbit1 kernel: [506497.165548] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Committing remote state change 64291981 (primary_nodes=0)
Mar 13 21:08:26 linbit1 kernel: [506497.165562] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:08:26 linbit1 kernel: [506497.165790] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:08:26 linbit1 kernel: [506497.629679] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: State change failed: Need a connection to start verify or resync
Mar 13 21:08:26 linbit1 kernel: [506497.640890] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Failed: conn( StandAlone -> Connecting )
Mar 13 21:08:26 linbit1 kernel: [506497.640908] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Terminating receiver thread
Mar 13 21:08:26 linbit1 kernel: [506497.640966] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b linbit2: Terminating sender thread
Mar 13 21:08:26 linbit1 kernel: [506497.644957] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:26 linbit1 kernel: [506497.644997] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: sock_recvmsg returned -4
Mar 13 21:08:26 linbit1 kernel: [506497.645047] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Terminating sender thread
Mar 13 21:08:26 linbit1 kernel: [506497.645054] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Starting sender thread (from drbd_r_pvc-f35d [2448572])
Mar 13 21:08:26 linbit1 kernel: [506497.657701] drbd pvc-d1746b46-d2be-4657-aaf4-05198959329b: Terminating worker thread
Mar 13 21:08:26 linbit1 kernel: [506497.697745] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Connection closed
Mar 13 21:08:26 linbit1 kernel: [506497.697761] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:26 linbit1 kernel: [506497.697766] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Terminating receiver thread
Mar 13 21:08:26 linbit1 kernel: [506497.697838] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3 linbit3: Terminating sender thread
Mar 13 21:08:26 linbit1 kernel: [506497.737691] drbd pvc-f35dc493-2853-439d-84a0-6d25a62830f3: Terminating worker thread
Mar 13 21:08:26 linbit1 kernel: [506497.938816] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: Preparing cluster-wide state change 3806549646 (1->-1 7680/1024)
Mar 13 21:08:26 linbit1 kernel: [506497.939107] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: State change 3806549646: primary_nodes=0, weak_nodes=0
Mar 13 21:08:26 linbit1 kernel: [506497.939113] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: Committing cluster-wide state change 3806549646 (0ms)
Mar 13 21:08:26 linbit1 kernel: [506497.939145] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: disk( UpToDate -> Detaching )
Mar 13 21:08:26 linbit1 kernel: [506497.939270] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:26 linbit1 kernel: [506497.939280] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: disk( Detaching -> Diskless )
Mar 13 21:08:26 linbit1 kernel: [506497.940278] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: drbd_bm_resize called with capacity == 0
Mar 13 21:08:27 linbit1 kernel: [506498.222938] change_cluster_wide_state: 10 callbacks suppressed
Mar 13 21:08:27 linbit1 kernel: [506498.222943] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:27 linbit1 kernel: [506498.235108] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: State change failed: State change was refused by peer node
Mar 13 21:08:27 linbit1 kernel: [506498.245792] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: Failed: quorum( yes -> no )
Mar 13 21:08:27 linbit1 kernel: [506498.245798] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:27 linbit1 kernel: [506498.245838] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:27 linbit1 kernel: [506498.257998] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: State change failed: State change was refused by peer node
Mar 13 21:08:27 linbit1 kernel: [506498.268680] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027: Failed: quorum( yes -> no )
Mar 13 21:08:27 linbit1 kernel: [506498.268684] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288/0 drbd1027 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:29 linbit1 kernel: [506500.076780] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: sock was shut down by peer
Mar 13 21:08:29 linbit1 kernel: [506500.076794] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:29 linbit1 kernel: [506500.076858] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Terminating sender thread
Mar 13 21:08:29 linbit1 kernel: [506500.076905] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Starting sender thread (from drbd_r_pvc-a2d2 [2452532])
Mar 13 21:08:29 linbit1 kernel: [506500.141747] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Connection closed
Mar 13 21:08:29 linbit1 kernel: [506500.141756] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:29 linbit1 kernel: [506500.141765] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Restarting receiver thread
Mar 13 21:08:29 linbit1 kernel: [506500.141771] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:30 linbit1 kernel: [506501.159155] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:30 linbit1 kernel: [506501.159237] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Terminating sender thread
Mar 13 21:08:30 linbit1 kernel: [506501.159247] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Starting sender thread (from drbd_r_pvc-a2d2 [2452532])
Mar 13 21:08:30 linbit1 kernel: [506501.159451] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Connection closed
Mar 13 21:08:30 linbit1 kernel: [506501.159463] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:30 linbit1 kernel: [506501.159468] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Terminating receiver thread
Mar 13 21:08:30 linbit1 kernel: [506501.159527] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Terminating sender thread
Mar 13 21:08:30 linbit1 kernel: [506501.168514] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Starting sender thread (from drbdsetup [2486036])
Mar 13 21:08:30 linbit1 kernel: [506501.170215] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:08:30 linbit1 kernel: [506501.170242] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Starting receiver thread (from drbd_w_pvc-a2d2 [2452499])
Mar 13 21:08:30 linbit1 kernel: [506501.170333] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:30 linbit1 kernel: [506501.370098] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Preparing remote state change 1499262407
Mar 13 21:08:30 linbit1 kernel: [506501.370358] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Committing remote state change 1499262407 (primary_nodes=0)
Mar 13 21:08:30 linbit1 kernel: [506501.370373] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( UpToDate -> Detaching )
Mar 13 21:08:30 linbit1 kernel: [506501.370669] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: pdsk( Detaching -> Diskless )
Mar 13 21:08:30 linbit1 kernel: [506501.582661] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Preparing cluster-wide state change 373001487 (0->-1 7680/1024)
Mar 13 21:08:30 linbit1 kernel: [506501.582947] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change 373001487: primary_nodes=0, weak_nodes=0
Mar 13 21:08:30 linbit1 kernel: [506501.582952] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Committing cluster-wide state change 373001487 (0ms)
Mar 13 21:08:30 linbit1 kernel: [506501.582984] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: disk( UpToDate -> Detaching )
Mar 13 21:08:30 linbit1 kernel: [506501.583116] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:30 linbit1 kernel: [506501.583128] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: disk( Detaching -> Diskless )
Mar 13 21:08:30 linbit1 kernel: [506501.584103] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: drbd_bm_resize called with capacity == 0
Mar 13 21:08:30 linbit1 kernel: [506501.896075] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:30 linbit1 kernel: [506501.908241] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change failed: State change was refused by peer node
Mar 13 21:08:30 linbit1 kernel: [506501.918926] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: Failed: quorum( yes -> no )
Mar 13 21:08:30 linbit1 kernel: [506501.918931] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:30 linbit1 kernel: [506501.918971] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:30 linbit1 kernel: [506501.931130] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: State change failed: State change was refused by peer node
Mar 13 21:08:30 linbit1 kernel: [506501.941814] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005: Failed: quorum( yes -> no )
Mar 13 21:08:30 linbit1 kernel: [506501.941818] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac/0 drbd1005 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:30 linbit1 kernel: [506501.980572] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Preparing cluster-wide state change 1022675209 (0->-1 7680/1024)
Mar 13 21:08:30 linbit1 kernel: [506501.980851] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: State change 1022675209: primary_nodes=0, weak_nodes=0
Mar 13 21:08:30 linbit1 kernel: [506501.980854] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Committing cluster-wide state change 1022675209 (0ms)
Mar 13 21:08:30 linbit1 kernel: [506501.980871] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: disk( UpToDate -> Detaching )
Mar 13 21:08:30 linbit1 kernel: [506501.981003] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: disk( Detaching -> Diskless )
Mar 13 21:08:30 linbit1 kernel: [506501.981970] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: drbd_bm_resize called with capacity == 0
Mar 13 21:08:31 linbit1 kernel: [506502.065089] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Preparing remote state change 4095396326
Mar 13 21:08:31 linbit1 kernel: [506502.065317] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Committing remote state change 4095396326 (primary_nodes=0)
Mar 13 21:08:31 linbit1 kernel: [506502.065329] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:08:31 linbit1 kernel: [506502.065545] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:08:31 linbit1 kernel: [506502.318260] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:31 linbit1 kernel: [506502.330427] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: State change failed: State change was refused by peer node
Mar 13 21:08:31 linbit1 kernel: [506502.341113] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:31 linbit1 kernel: [506502.341149] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:31 linbit1 kernel: [506502.353309] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: State change failed: State change was refused by peer node
Mar 13 21:08:31 linbit1 kernel: [506502.363992] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028: Failed: quorum( yes -> no )
Mar 13 21:08:31 linbit1 kernel: [506502.363996] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338/0 drbd1028 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:31 linbit1 kernel: [506502.952929] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Preparing remote state change 3679649704
Mar 13 21:08:31 linbit1 kernel: [506502.953136] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Committing remote state change 3679649704 (primary_nodes=0)
Mar 13 21:08:31 linbit1 kernel: [506502.953151] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit2: pdsk( UpToDate -> Detaching )
Mar 13 21:08:31 linbit1 kernel: [506502.953361] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit2: pdsk( Detaching -> Diskless )
Mar 13 21:08:32 linbit1 kernel: [506503.315759] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Preparing remote state change 4281793962
Mar 13 21:08:32 linbit1 kernel: [506503.315985] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Committing remote state change 4281793962 (primary_nodes=0)
Mar 13 21:08:32 linbit1 kernel: [506503.316096] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:32 linbit1 kernel: [506503.316205] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:32 linbit1 kernel: [506503.573787] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Preparing remote state change 792279083
Mar 13 21:08:32 linbit1 kernel: [506503.574005] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Committing remote state change 792279083 (primary_nodes=0)
Mar 13 21:08:32 linbit1 kernel: [506503.574019] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:08:32 linbit1 kernel: [506503.574314] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:08:32 linbit1 kernel: [506503.736926] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Preparing remote state change 4157597699
Mar 13 21:08:32 linbit1 kernel: [506503.737103] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Committing remote state change 4157597699 (primary_nodes=0)
Mar 13 21:08:32 linbit1 kernel: [506503.737116] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: pdsk( UpToDate -> Detaching )
Mar 13 21:08:32 linbit1 kernel: [506503.737316] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: pdsk( Detaching -> Diskless )
Mar 13 21:08:32 linbit1 kernel: [506503.974228] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Preparing cluster-wide state change 3658586866 (1->-1 7680/1024)
Mar 13 21:08:32 linbit1 kernel: [506503.974499] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: State change 3658586866: primary_nodes=0, weak_nodes=0
Mar 13 21:08:32 linbit1 kernel: [506503.974504] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Committing cluster-wide state change 3658586866 (0ms)
Mar 13 21:08:32 linbit1 kernel: [506503.974534] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: disk( UpToDate -> Detaching )
Mar 13 21:08:32 linbit1 kernel: [506503.974650] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:32 linbit1 kernel: [506503.974659] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: disk( Detaching -> Diskless )
Mar 13 21:08:32 linbit1 kernel: [506503.975689] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: drbd_bm_resize called with capacity == 0
Mar 13 21:08:32 linbit1 kernel: [506503.976205] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Preparing remote state change 1510423439
Mar 13 21:08:32 linbit1 kernel: [506503.976421] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Committing remote state change 1510423439 (primary_nodes=0)
Mar 13 21:08:32 linbit1 kernel: [506503.976434] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:08:32 linbit1 kernel: [506503.976629] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:08:33 linbit1 kernel: [506504.262155] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:33 linbit1 kernel: [506504.274321] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: State change failed: State change was refused by peer node
Mar 13 21:08:33 linbit1 kernel: [506504.285005] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: Failed: quorum( yes -> no )
Mar 13 21:08:33 linbit1 kernel: [506504.285011] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:33 linbit1 kernel: [506504.285059] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:33 linbit1 kernel: [506504.297219] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: State change failed: State change was refused by peer node
Mar 13 21:08:33 linbit1 kernel: [506504.307902] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011: Failed: quorum( yes -> no )
Mar 13 21:08:33 linbit1 kernel: [506504.307906] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a/0 drbd1011 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:33 linbit1 kernel: [506504.351347] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:33 linbit1 kernel: [506504.351359] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:33 linbit1 kernel: [506504.363523] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2: State change failed: State change was refused by peer node
Mar 13 21:08:33 linbit1 kernel: [506504.374208] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:33 linbit1 kernel: [506504.374248] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:33 linbit1 kernel: [506504.386407] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2: State change failed: State change was refused by peer node
Mar 13 21:08:33 linbit1 kernel: [506504.397091] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2/0 drbd1026 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:33 linbit1 kernel: [506504.431240] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:33 linbit1 kernel: [506504.431313] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Terminating sender thread
Mar 13 21:08:33 linbit1 kernel: [506504.431352] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Starting sender thread (from drbd_r_pvc-a2d2 [2486040])
Mar 13 21:08:33 linbit1 kernel: [506504.431589] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Connection closed
Mar 13 21:08:33 linbit1 kernel: [506504.431602] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:33 linbit1 kernel: [506504.431608] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Terminating receiver thread
Mar 13 21:08:33 linbit1 kernel: [506504.431691] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit2: Terminating sender thread
Mar 13 21:08:33 linbit1 kernel: [506504.462172] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:33 linbit1 kernel: [506504.462183] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:33 linbit1 kernel: [506504.474344] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148: State change failed: State change was refused by peer node
Mar 13 21:08:33 linbit1 kernel: [506504.485030] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:33 linbit1 kernel: [506504.485073] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:33 linbit1 kernel: [506504.497232] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148: State change failed: State change was refused by peer node
Mar 13 21:08:33 linbit1 kernel: [506504.507916] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148/0 drbd1016 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:34 linbit1 kernel: [506505.172523] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: sock was shut down by peer
Mar 13 21:08:34 linbit1 kernel: [506505.172539] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:34 linbit1 kernel: [506505.172607] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Terminating sender thread
Mar 13 21:08:34 linbit1 kernel: [506505.172644] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Starting sender thread (from drbd_r_pvc-01ec [2445634])
Mar 13 21:08:34 linbit1 kernel: [506505.217718] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Connection closed
Mar 13 21:08:34 linbit1 kernel: [506505.217727] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:34 linbit1 kernel: [506505.217735] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Restarting receiver thread
Mar 13 21:08:34 linbit1 kernel: [506505.217741] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:34 linbit1 kernel: [506505.251496] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:34 linbit1 kernel: [506505.251560] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Terminating sender thread
Mar 13 21:08:34 linbit1 kernel: [506505.251567] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Starting sender thread (from drbd_r_pvc-01ec [2445634])
Mar 13 21:08:34 linbit1 kernel: [506505.251706] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Connection closed
Mar 13 21:08:34 linbit1 kernel: [506505.251718] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:34 linbit1 kernel: [506505.251722] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Terminating receiver thread
Mar 13 21:08:34 linbit1 kernel: [506505.251776] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Terminating sender thread
Mar 13 21:08:34 linbit1 kernel: [506505.260848] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Starting sender thread (from drbdsetup [2486350])
Mar 13 21:08:34 linbit1 kernel: [506505.263877] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:34 linbit1 kernel: [506505.263909] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Starting receiver thread (from drbd_w_pvc-01ec [2444652])
Mar 13 21:08:34 linbit1 kernel: [506505.264035] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:35 linbit1 kernel: [506506.554578] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:35 linbit1 kernel: [506506.554666] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Terminating sender thread
Mar 13 21:08:35 linbit1 kernel: [506506.554703] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Starting sender thread (from drbd_r_pvc-8f12 [2452407])
Mar 13 21:08:35 linbit1 kernel: [506506.621710] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Connection closed
Mar 13 21:08:35 linbit1 kernel: [506506.621727] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:35 linbit1 kernel: [506506.621732] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Terminating receiver thread
Mar 13 21:08:35 linbit1 kernel: [506506.621805] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Terminating sender thread
Mar 13 21:08:35 linbit1 kernel: [506506.635206] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Starting sender thread (from drbdsetup [2486369])
Mar 13 21:08:35 linbit1 kernel: [506506.638218] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:35 linbit1 kernel: [506506.638250] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Starting receiver thread (from drbd_w_pvc-8f12 [2450906])
Mar 13 21:08:35 linbit1 kernel: [506506.638375] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:35 linbit1 kernel: [506506.998082] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:35 linbit1 kernel: [506506.998165] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Terminating sender thread
Mar 13 21:08:35 linbit1 kernel: [506506.998193] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Starting sender thread (from drbd_r_pvc-a2d2 [2452534])
Mar 13 21:08:36 linbit1 kernel: [506507.073872] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Connection closed
Mar 13 21:08:36 linbit1 kernel: [506507.073890] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:36 linbit1 kernel: [506507.073895] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Terminating receiver thread
Mar 13 21:08:36 linbit1 kernel: [506507.074008] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288 linbit3: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.092794] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:36 linbit1 kernel: [506507.092874] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.092922] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Starting sender thread (from drbd_r_pvc-7e6b [2451349])
Mar 13 21:08:36 linbit1 kernel: [506507.113682] drbd pvc-a2d229af-1d7a-41cf-9c7c-8f2d32348288: Terminating worker thread
Mar 13 21:08:36 linbit1 kernel: [506507.120500] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: sock was shut down by peer
Mar 13 21:08:36 linbit1 kernel: [506507.120509] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:36 linbit1 kernel: [506507.120567] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.120597] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Starting sender thread (from drbd_r_pvc-7e6b [2451351])
Mar 13 21:08:36 linbit1 kernel: [506507.153739] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Connection closed
Mar 13 21:08:36 linbit1 kernel: [506507.153754] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:36 linbit1 kernel: [506507.153759] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Terminating receiver thread
Mar 13 21:08:36 linbit1 kernel: [506507.153827] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit2: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.154133] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: conn( BrokenPipe -> Disconnecting )
Mar 13 21:08:36 linbit1 kernel: [506507.185895] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Connection closed
Mar 13 21:08:36 linbit1 kernel: [506507.185912] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:36 linbit1 kernel: [506507.185917] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Terminating receiver thread
Mar 13 21:08:36 linbit1 kernel: [506507.186074] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2 linbit3: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.189773] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:36 linbit1 kernel: [506507.189938] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.189978] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Starting sender thread (from drbd_r_pvc-e2bb [2449117])
Mar 13 21:08:36 linbit1 kernel: [506507.213677] drbd pvc-7e6bced2-b830-4867-8c7c-ab7e434f32f2: Terminating worker thread
Mar 13 21:08:36 linbit1 kernel: [506507.220488] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: sock was shut down by peer
Mar 13 21:08:36 linbit1 kernel: [506507.220499] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:36 linbit1 kernel: [506507.220633] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.220675] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Starting sender thread (from drbd_r_pvc-e2bb [2449119])
Mar 13 21:08:36 linbit1 kernel: [506507.269816] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Connection closed
Mar 13 21:08:36 linbit1 kernel: [506507.269833] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:36 linbit1 kernel: [506507.269838] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Terminating receiver thread
Mar 13 21:08:36 linbit1 kernel: [506507.269949] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit2: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.270280] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: conn( BrokenPipe -> Disconnecting )
Mar 13 21:08:36 linbit1 kernel: [506507.317838] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Connection closed
Mar 13 21:08:36 linbit1 kernel: [506507.317855] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:36 linbit1 kernel: [506507.317860] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Terminating receiver thread
Mar 13 21:08:36 linbit1 kernel: [506507.318015] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148 linbit3: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.333713] drbd pvc-e2bb94d4-e865-462d-812c-a95130b94148: Terminating worker thread
Mar 13 21:08:36 linbit1 kernel: [506507.560327] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:36 linbit1 kernel: [506507.560408] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.560445] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Starting sender thread (from drbd_r_pvc-01ec [2486354])
Mar 13 21:08:36 linbit1 kernel: [506507.560672] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Connection closed
Mar 13 21:08:36 linbit1 kernel: [506507.560687] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:36 linbit1 kernel: [506507.560694] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Terminating receiver thread
Mar 13 21:08:36 linbit1 kernel: [506507.560719] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit3: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.782261] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:36 linbit1 kernel: [506507.782337] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.782368] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Starting sender thread (from drbd_r_pvc-654a [2446928])
Mar 13 21:08:36 linbit1 kernel: [506507.853717] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Connection closed
Mar 13 21:08:36 linbit1 kernel: [506507.853737] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:36 linbit1 kernel: [506507.853743] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Terminating receiver thread
Mar 13 21:08:36 linbit1 kernel: [506507.853828] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Terminating sender thread
Mar 13 21:08:36 linbit1 kernel: [506507.862745] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Starting sender thread (from drbdsetup [2486434])
Mar 13 21:08:36 linbit1 kernel: [506507.865213] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:08:36 linbit1 kernel: [506507.865243] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Starting receiver thread (from drbd_w_pvc-654a [2446909])
Mar 13 21:08:36 linbit1 kernel: [506507.865373] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:37 linbit1 kernel: [506508.223663] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:37 linbit1 kernel: [506508.223751] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Terminating sender thread
Mar 13 21:08:37 linbit1 kernel: [506508.223772] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Starting sender thread (from drbd_r_pvc-8f12 [2486373])
Mar 13 21:08:37 linbit1 kernel: [506508.224016] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Connection closed
Mar 13 21:08:37 linbit1 kernel: [506508.224033] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:37 linbit1 kernel: [506508.224043] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Terminating receiver thread
Mar 13 21:08:37 linbit1 kernel: [506508.224162] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit3: Terminating sender thread
Mar 13 21:08:37 linbit1 kernel: [506508.413842] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 21:08:37 linbit1 kernel: [506508.413853] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:08:37 linbit1 kernel: [506508.414000] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 21:08:37 linbit1 kernel: [506508.508491] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Preparing remote state change 2202231886
Mar 13 21:08:37 linbit1 kernel: [506508.508718] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Committing remote state change 2202231886 (primary_nodes=0)
Mar 13 21:08:37 linbit1 kernel: [506508.509680] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Preparing cluster-wide state change 3770200141 (1->2 499/146)
Mar 13 21:08:37 linbit1 kernel: [506508.509855] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: State change 3770200141: primary_nodes=0, weak_nodes=0
Mar 13 21:08:37 linbit1 kernel: [506508.509862] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Committing cluster-wide state change 3770200141 (0ms)
Mar 13 21:08:37 linbit1 kernel: [506508.509894] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:08:38 linbit1 kernel: [506509.480713] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: sock was shut down by peer
Mar 13 21:08:38 linbit1 kernel: [506509.480728] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:38 linbit1 kernel: [506509.480937] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Terminating sender thread
Mar 13 21:08:38 linbit1 kernel: [506509.480971] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Starting sender thread (from drbd_r_pvc-654a [2486438])
Mar 13 21:08:38 linbit1 kernel: [506509.525802] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Connection closed
Mar 13 21:08:38 linbit1 kernel: [506509.525813] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:38 linbit1 kernel: [506509.525821] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Restarting receiver thread
Mar 13 21:08:38 linbit1 kernel: [506509.525825] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:38 linbit1 kernel: [506509.594514] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Preparing cluster-wide state change 765558445 (0->-1 7680/1024)
Mar 13 21:08:38 linbit1 kernel: [506509.594838] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: State change 765558445: primary_nodes=0, weak_nodes=0
Mar 13 21:08:38 linbit1 kernel: [506509.594844] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Committing cluster-wide state change 765558445 (0ms)
Mar 13 21:08:38 linbit1 kernel: [506509.594875] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: disk( UpToDate -> Detaching )
Mar 13 21:08:38 linbit1 kernel: [506509.595018] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: disk( Detaching -> Diskless )
Mar 13 21:08:38 linbit1 kernel: [506509.596149] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: drbd_bm_resize called with capacity == 0
Mar 13 21:08:38 linbit1 kernel: [506509.893683] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:38 linbit1 kernel: [506509.905850] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: State change failed: State change was refused by peer node
Mar 13 21:08:38 linbit1 kernel: [506509.916535] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:38 linbit1 kernel: [506509.916566] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:38 linbit1 kernel: [506509.928725] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: State change failed: State change was refused by peer node
Mar 13 21:08:38 linbit1 kernel: [506509.939408] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004: Failed: quorum( yes -> no )
Mar 13 21:08:38 linbit1 kernel: [506509.939412] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694/0 drbd1004 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:39 linbit1 kernel: [506510.316266] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Preparing remote state change 2051974952
Mar 13 21:08:39 linbit1 kernel: [506510.316524] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Committing remote state change 2051974952 (primary_nodes=0)
Mar 13 21:08:39 linbit1 kernel: [506510.316538] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:08:39 linbit1 kernel: [506510.316902] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:08:39 linbit1 kernel: [506510.851146] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Preparing remote state change 2518662455
Mar 13 21:08:39 linbit1 kernel: [506510.851383] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Committing remote state change 2518662455 (primary_nodes=0)
Mar 13 21:08:39 linbit1 kernel: [506510.851502] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:39 linbit1 kernel: [506510.851609] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:40 linbit1 kernel: [506511.055250] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:40 linbit1 kernel: [506511.055357] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Terminating sender thread
Mar 13 21:08:40 linbit1 kernel: [506511.055404] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Starting sender thread (from drbd_r_pvc-01ec [2445633])
Mar 13 21:08:40 linbit1 kernel: [506511.121966] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Connection closed
Mar 13 21:08:40 linbit1 kernel: [506511.121986] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:40 linbit1 kernel: [506511.121992] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Terminating receiver thread
Mar 13 21:08:40 linbit1 kernel: [506511.122119] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac linbit2: Terminating sender thread
Mar 13 21:08:40 linbit1 kernel: [506511.125985] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Preparing cluster-wide state change 1834080464 (1->-1 7680/1024)
Mar 13 21:08:40 linbit1 kernel: [506511.126319] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: State change 1834080464: primary_nodes=0, weak_nodes=0
Mar 13 21:08:40 linbit1 kernel: [506511.126324] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Committing cluster-wide state change 1834080464 (0ms)
Mar 13 21:08:40 linbit1 kernel: [506511.126359] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: disk( UpToDate -> Detaching )
Mar 13 21:08:40 linbit1 kernel: [506511.126527] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: disk( Detaching -> Diskless )
Mar 13 21:08:40 linbit1 kernel: [506511.127579] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: drbd_bm_resize called with capacity == 0
Mar 13 21:08:40 linbit1 kernel: [506511.145674] drbd pvc-01ec89c0-c4ad-45be-9350-cf49e39860ac: Terminating worker thread
Mar 13 21:08:40 linbit1 kernel: [506511.410188] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:40 linbit1 kernel: [506511.422356] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: State change failed: State change was refused by peer node
Mar 13 21:08:40 linbit1 kernel: [506511.433041] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:40 linbit1 kernel: [506511.433096] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:40 linbit1 kernel: [506511.445255] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: State change failed: State change was refused by peer node
Mar 13 21:08:40 linbit1 kernel: [506511.455938] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013: Failed: quorum( yes -> no )
Mar 13 21:08:40 linbit1 kernel: [506511.455942] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591/0 drbd1013 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:40 linbit1 kernel: [506511.484843] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 2077502836 (0->-1 7680/1024)
Mar 13 21:08:40 linbit1 kernel: [506511.485198] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: State change 2077502836: primary_nodes=0, weak_nodes=0
Mar 13 21:08:40 linbit1 kernel: [506511.485203] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 2077502836 (0ms)
Mar 13 21:08:40 linbit1 kernel: [506511.485233] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: disk( UpToDate -> Detaching )
Mar 13 21:08:40 linbit1 kernel: [506511.485391] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: disk( Detaching -> Diskless )
Mar 13 21:08:40 linbit1 kernel: [506511.486356] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: drbd_bm_resize called with capacity == 0
Mar 13 21:08:40 linbit1 kernel: [506511.691730] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Preparing remote state change 3356852509
Mar 13 21:08:40 linbit1 kernel: [506511.691747] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:40 linbit1 kernel: [506511.691999] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Committing remote state change 3356852509 (primary_nodes=0)
Mar 13 21:08:40 linbit1 kernel: [506511.692014] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:08:40 linbit1 kernel: [506511.692221] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:08:40 linbit1 kernel: [506511.766208] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:40 linbit1 kernel: [506511.778376] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: State change failed: State change was refused by peer node
Mar 13 21:08:40 linbit1 kernel: [506511.789061] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: Failed: quorum( yes -> no )
Mar 13 21:08:40 linbit1 kernel: [506511.789067] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:40 linbit1 kernel: [506511.789118] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:40 linbit1 kernel: [506511.801278] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: State change failed: State change was refused by peer node
Mar 13 21:08:40 linbit1 kernel: [506511.811961] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001: Failed: quorum( yes -> no )
Mar 13 21:08:40 linbit1 kernel: [506511.811965] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b/0 drbd1001 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:40 linbit1 kernel: [506511.852640] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:40 linbit1 kernel: [506511.852731] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Terminating sender thread
Mar 13 21:08:40 linbit1 kernel: [506511.852765] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Starting sender thread (from drbd_r_pvc-654a [2486438])
Mar 13 21:08:40 linbit1 kernel: [506511.852949] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Connection closed
Mar 13 21:08:40 linbit1 kernel: [506511.852964] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:40 linbit1 kernel: [506511.852970] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Terminating receiver thread
Mar 13 21:08:40 linbit1 kernel: [506511.853017] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit2: Terminating sender thread
Mar 13 21:08:40 linbit1 kernel: [506511.883414] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:40 linbit1 kernel: [506511.895581] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc: State change failed: State change was refused by peer node
Mar 13 21:08:40 linbit1 kernel: [506511.906267] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:40 linbit1 kernel: [506511.906312] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:40 linbit1 kernel: [506511.918472] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc: State change failed: State change was refused by peer node
Mar 13 21:08:40 linbit1 kernel: [506511.929156] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc/0 drbd1010 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:40 linbit1 kernel: [506511.949502] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Preparing cluster-wide state change 1900651487 (1->-1 7680/1024)
Mar 13 21:08:40 linbit1 kernel: [506511.949835] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: State change 1900651487: primary_nodes=0, weak_nodes=0
Mar 13 21:08:40 linbit1 kernel: [506511.949841] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Committing cluster-wide state change 1900651487 (4ms)
Mar 13 21:08:40 linbit1 kernel: [506511.949873] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: disk( UpToDate -> Detaching )
Mar 13 21:08:40 linbit1 kernel: [506511.950028] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:40 linbit1 kernel: [506511.950039] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: disk( Detaching -> Diskless )
Mar 13 21:08:40 linbit1 kernel: [506511.951040] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: drbd_bm_resize called with capacity == 0
Mar 13 21:08:41 linbit1 kernel: [506512.131687] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Preparing remote state change 4027745479
Mar 13 21:08:41 linbit1 kernel: [506512.131987] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Committing remote state change 4027745479 (primary_nodes=0)
Mar 13 21:08:41 linbit1 kernel: [506512.132061] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:41 linbit1 kernel: [506512.132126] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:41 linbit1 kernel: [506512.297092] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:41 linbit1 kernel: [506512.309259] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: State change failed: State change was refused by peer node
Mar 13 21:08:41 linbit1 kernel: [506512.319944] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: Failed: quorum( yes -> no )
Mar 13 21:08:41 linbit1 kernel: [506512.319949] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:41 linbit1 kernel: [506512.320004] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:41 linbit1 kernel: [506512.332163] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: State change failed: State change was refused by peer node
Mar 13 21:08:41 linbit1 kernel: [506512.342846] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014: Failed: quorum( yes -> no )
Mar 13 21:08:41 linbit1 kernel: [506512.342849] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676/0 drbd1014 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:42 linbit1 kernel: [506513.100310] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Preparing remote state change 273992538
Mar 13 21:08:42 linbit1 kernel: [506513.100556] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Committing remote state change 273992538 (primary_nodes=0)
Mar 13 21:08:42 linbit1 kernel: [506513.100685] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:42 linbit1 kernel: [506513.100792] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:42 linbit1 kernel: [506513.592632] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Preparing remote state change 1467269056
Mar 13 21:08:42 linbit1 kernel: [506513.592844] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Committing remote state change 1467269056 (primary_nodes=0)
Mar 13 21:08:42 linbit1 kernel: [506513.592920] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:42 linbit1 kernel: [506513.592994] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:43 linbit1 kernel: [506514.114954] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:43 linbit1 kernel: [506514.115046] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Terminating sender thread
Mar 13 21:08:43 linbit1 kernel: [506514.115080] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Starting sender thread (from drbd_r_pvc-8f12 [2452405])
Mar 13 21:08:43 linbit1 kernel: [506514.169872] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Connection closed
Mar 13 21:08:43 linbit1 kernel: [506514.169887] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:43 linbit1 kernel: [506514.169893] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Terminating receiver thread
Mar 13 21:08:43 linbit1 kernel: [506514.170014] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338 linbit2: Terminating sender thread
Mar 13 21:08:43 linbit1 kernel: [506514.180827] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:43 linbit1 kernel: [506514.180913] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Terminating sender thread
Mar 13 21:08:43 linbit1 kernel: [506514.180945] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Starting sender thread (from drbd_r_pvc-c419 [2445801])
Mar 13 21:08:43 linbit1 kernel: [506514.209649] drbd pvc-8f12a6ad-b16c-4206-91d3-c5ec07b55338: Terminating worker thread
Mar 13 21:08:43 linbit1 kernel: [506514.241772] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Connection closed
Mar 13 21:08:43 linbit1 kernel: [506514.241791] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:43 linbit1 kernel: [506514.241797] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Terminating receiver thread
Mar 13 21:08:43 linbit1 kernel: [506514.241919] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Terminating sender thread
Mar 13 21:08:43 linbit1 kernel: [506514.254348] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Starting sender thread (from drbdsetup [2486798])
Mar 13 21:08:43 linbit1 kernel: [506514.257346] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:43 linbit1 kernel: [506514.257373] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Starting receiver thread (from drbd_w_pvc-c419 [2444932])
Mar 13 21:08:43 linbit1 kernel: [506514.257488] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:43 linbit1 kernel: [506514.268413] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:43 linbit1 kernel: [506514.268507] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Terminating sender thread
Mar 13 21:08:43 linbit1 kernel: [506514.268547] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Starting sender thread (from drbd_r_pvc-f893 [2448650])
Mar 13 21:08:43 linbit1 kernel: [506514.309775] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Connection closed
Mar 13 21:08:43 linbit1 kernel: [506514.309794] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:43 linbit1 kernel: [506514.309800] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Terminating receiver thread
Mar 13 21:08:43 linbit1 kernel: [506514.309918] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Terminating sender thread
Mar 13 21:08:43 linbit1 kernel: [506514.321313] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Starting sender thread (from drbdsetup [2486817])
Mar 13 21:08:43 linbit1 kernel: [506514.324363] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:08:43 linbit1 kernel: [506514.324398] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Starting receiver thread (from drbd_w_pvc-f893 [2448629])
Mar 13 21:08:43 linbit1 kernel: [506514.324503] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:43 linbit1 kernel: [506514.892780] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 21:08:43 linbit1 kernel: [506514.892789] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:08:43 linbit1 kernel: [506514.892925] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 21:08:43 linbit1 kernel: [506514.941638] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Preparing cluster-wide state change 1684275363 (1->2 499/146)
Mar 13 21:08:43 linbit1 kernel: [506514.941918] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: State change 1684275363: primary_nodes=0, weak_nodes=0
Mar 13 21:08:43 linbit1 kernel: [506514.941925] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Committing cluster-wide state change 1684275363 (0ms)
Mar 13 21:08:43 linbit1 kernel: [506514.941957] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:08:44 linbit1 kernel: [506515.048477] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Preparing remote state change 2068024339
Mar 13 21:08:44 linbit1 kernel: [506515.048739] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Committing remote state change 2068024339 (primary_nodes=0)
Mar 13 21:08:45 linbit1 kernel: [506516.487911] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:45 linbit1 kernel: [506516.488080] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.488112] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Starting sender thread (from drbd_r_pvc-6361 [2445735])
Mar 13 21:08:45 linbit1 kernel: [506516.565694] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Connection closed
Mar 13 21:08:45 linbit1 kernel: [506516.565711] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:45 linbit1 kernel: [506516.565718] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Terminating receiver thread
Mar 13 21:08:45 linbit1 kernel: [506516.565788] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.577526] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Starting sender thread (from drbdsetup [2486837])
Mar 13 21:08:45 linbit1 kernel: [506516.580614] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:08:45 linbit1 kernel: [506516.580652] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Starting receiver thread (from drbd_w_pvc-6361 [2444502])
Mar 13 21:08:45 linbit1 kernel: [506516.580782] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:45 linbit1 kernel: [506516.652927] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:45 linbit1 kernel: [506516.653017] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.653053] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Starting sender thread (from drbd_r_pvc-4ab0 [2448424])
Mar 13 21:08:45 linbit1 kernel: [506516.705710] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Connection closed
Mar 13 21:08:45 linbit1 kernel: [506516.705726] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:45 linbit1 kernel: [506516.705731] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Terminating receiver thread
Mar 13 21:08:45 linbit1 kernel: [506516.705783] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.717940] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Starting sender thread (from drbdsetup [2486856])
Mar 13 21:08:45 linbit1 kernel: [506516.721155] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:45 linbit1 kernel: [506516.721189] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Starting receiver thread (from drbd_w_pvc-4ab0 [2448402])
Mar 13 21:08:45 linbit1 kernel: [506516.721348] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:45 linbit1 kernel: [506516.728062] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:45 linbit1 kernel: [506516.728142] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.728175] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Starting sender thread (from drbd_r_pvc-654a [2446930])
Mar 13 21:08:45 linbit1 kernel: [506516.801847] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Connection closed
Mar 13 21:08:45 linbit1 kernel: [506516.801864] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:45 linbit1 kernel: [506516.801869] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Terminating receiver thread
Mar 13 21:08:45 linbit1 kernel: [506516.801908] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a linbit3: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.805460] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:45 linbit1 kernel: [506516.805527] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.805561] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Starting sender thread (from drbd_r_pvc-b5e1 [2448489])
Mar 13 21:08:45 linbit1 kernel: [506516.841632] drbd pvc-654a630d-e5c6-45e7-8370-8844b4cd486a: Terminating worker thread
Mar 13 21:08:45 linbit1 kernel: [506516.860744] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: sock was shut down by peer
Mar 13 21:08:45 linbit1 kernel: [506516.860754] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:45 linbit1 kernel: [506516.860849] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.860881] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Starting sender thread (from drbd_r_pvc-f893 [2486821])
Mar 13 21:08:45 linbit1 kernel: [506516.877696] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Connection closed
Mar 13 21:08:45 linbit1 kernel: [506516.877714] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:45 linbit1 kernel: [506516.877720] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Terminating receiver thread
Mar 13 21:08:45 linbit1 kernel: [506516.877795] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit2: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.878147] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:45 linbit1 kernel: [506516.878266] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506516.878299] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Starting sender thread (from drbd_r_pvc-b5e1 [2448491])
Mar 13 21:08:45 linbit1 kernel: [506516.937755] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Connection closed
Mar 13 21:08:45 linbit1 kernel: [506516.937766] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:45 linbit1 kernel: [506516.937775] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Restarting receiver thread
Mar 13 21:08:45 linbit1 kernel: [506516.937838] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:45 linbit1 kernel: [506516.981688] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Connection closed
Mar 13 21:08:45 linbit1 kernel: [506516.981704] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:45 linbit1 kernel: [506516.981709] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Terminating receiver thread
Mar 13 21:08:45 linbit1 kernel: [506516.981784] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc linbit3: Terminating sender thread
Mar 13 21:08:45 linbit1 kernel: [506517.005631] drbd pvc-b5e14190-d2e3-49fa-a417-0effd3c1f9cc: Terminating worker thread
Mar 13 21:08:46 linbit1 kernel: [506517.117777] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 21:08:46 linbit1 kernel: [506517.117786] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:08:46 linbit1 kernel: [506517.117919] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 21:08:46 linbit1 kernel: [506517.157620] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Preparing cluster-wide state change 3555933917 (0->2 499/146)
Mar 13 21:08:46 linbit1 kernel: [506517.171330] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:46 linbit1 kernel: [506517.171409] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Terminating sender thread
Mar 13 21:08:46 linbit1 kernel: [506517.171488] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Starting sender thread (from drbd_r_pvc-f893 [2486821])
Mar 13 21:08:46 linbit1 kernel: [506517.171642] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Connection closed
Mar 13 21:08:46 linbit1 kernel: [506517.171655] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:46 linbit1 kernel: [506517.171659] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Terminating receiver thread
Mar 13 21:08:46 linbit1 kernel: [506517.171690] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit2: Terminating sender thread
Mar 13 21:08:46 linbit1 kernel: [506517.172724] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: State change 3555933917: primary_nodes=0, weak_nodes=0
Mar 13 21:08:46 linbit1 kernel: [506517.172733] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Committing cluster-wide state change 3555933917 (12ms)
Mar 13 21:08:46 linbit1 kernel: [506517.172767] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:08:46 linbit1 kernel: [506517.197904] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:46 linbit1 kernel: [506517.197971] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Terminating sender thread
Mar 13 21:08:46 linbit1 kernel: [506517.197997] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Starting sender thread (from drbd_r_pvc-c419 [2486802])
Mar 13 21:08:46 linbit1 kernel: [506517.198246] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Connection closed
Mar 13 21:08:46 linbit1 kernel: [506517.198260] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:46 linbit1 kernel: [506517.198265] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Terminating receiver thread
Mar 13 21:08:46 linbit1 kernel: [506517.198294] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit3: Terminating sender thread
Mar 13 21:08:46 linbit1 kernel: [506517.204455] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Preparing remote state change 1915929300
Mar 13 21:08:46 linbit1 kernel: [506517.204688] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Committing remote state change 1915929300 (primary_nodes=0)
Mar 13 21:08:46 linbit1 kernel: [506517.272697] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: sock was shut down by peer
Mar 13 21:08:46 linbit1 kernel: [506517.272706] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:46 linbit1 kernel: [506517.272884] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Terminating sender thread
Mar 13 21:08:46 linbit1 kernel: [506517.272918] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Starting sender thread (from drbd_r_pvc-6361 [2486841])
Mar 13 21:08:46 linbit1 kernel: [506517.337781] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Connection closed
Mar 13 21:08:46 linbit1 kernel: [506517.337791] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:46 linbit1 kernel: [506517.337801] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Restarting receiver thread
Mar 13 21:08:46 linbit1 kernel: [506517.337806] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:46 linbit1 kernel: [506517.737042] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:46 linbit1 kernel: [506517.737120] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Terminating sender thread
Mar 13 21:08:46 linbit1 kernel: [506517.737153] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Starting sender thread (from drbd_r_pvc-4ab0 [2486860])
Mar 13 21:08:46 linbit1 kernel: [506517.737377] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Connection closed
Mar 13 21:08:46 linbit1 kernel: [506517.737390] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:46 linbit1 kernel: [506517.737396] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Terminating receiver thread
Mar 13 21:08:46 linbit1 kernel: [506517.737410] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit3: Terminating sender thread
Mar 13 21:08:46 linbit1 kernel: [506517.940195] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:46 linbit1 kernel: [506517.940292] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Terminating sender thread
Mar 13 21:08:46 linbit1 kernel: [506517.940329] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Starting sender thread (from drbd_r_pvc-6361 [2486841])
Mar 13 21:08:46 linbit1 kernel: [506517.940489] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Connection closed
Mar 13 21:08:46 linbit1 kernel: [506517.940497] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:46 linbit1 kernel: [506517.940499] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Terminating receiver thread
Mar 13 21:08:46 linbit1 kernel: [506517.940522] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit2: Terminating sender thread
Mar 13 21:08:48 linbit1 kernel: [506519.121133] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:48 linbit1 kernel: [506519.133294] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd: State change failed: State change was refused by peer node
Mar 13 21:08:48 linbit1 kernel: [506519.143977] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:48 linbit1 kernel: [506519.144000] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:48 linbit1 kernel: [506519.156157] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd: State change failed: State change was refused by peer node
Mar 13 21:08:48 linbit1 kernel: [506519.166837] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd/0 drbd1008 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:48 linbit1 kernel: [506519.297339] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Preparing remote state change 2217984950
Mar 13 21:08:48 linbit1 kernel: [506519.297575] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Committing remote state change 2217984950 (primary_nodes=0)
Mar 13 21:08:48 linbit1 kernel: [506519.297687] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:48 linbit1 kernel: [506519.297786] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:48 linbit1 kernel: [506519.562270] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:48 linbit1 kernel: [506519.562355] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Terminating sender thread
Mar 13 21:08:48 linbit1 kernel: [506519.562391] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Starting sender thread (from drbd_r_pvc-c419 [2445799])
Mar 13 21:08:48 linbit1 kernel: [506519.621849] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Connection closed
Mar 13 21:08:48 linbit1 kernel: [506519.621867] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:48 linbit1 kernel: [506519.621872] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Terminating receiver thread
Mar 13 21:08:48 linbit1 kernel: [506519.621993] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694 linbit2: Terminating sender thread
Mar 13 21:08:48 linbit1 kernel: [506519.628534] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:48 linbit1 kernel: [506519.628612] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Terminating sender thread
Mar 13 21:08:48 linbit1 kernel: [506519.628645] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Starting sender thread (from drbd_r_pvc-f893 [2448652])
Mar 13 21:08:48 linbit1 kernel: [506519.649669] drbd pvc-c41969dd-54f4-413c-a3c5-a272b9c80694: Terminating worker thread
Mar 13 21:08:48 linbit1 kernel: [506519.689847] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Connection closed
Mar 13 21:08:48 linbit1 kernel: [506519.689863] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:48 linbit1 kernel: [506519.689868] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Terminating receiver thread
Mar 13 21:08:48 linbit1 kernel: [506519.689950] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676 linbit3: Terminating sender thread
Mar 13 21:08:48 linbit1 kernel: [506519.729629] drbd pvc-f8932809-bd63-4def-b16a-68888d7cd676: Terminating worker thread
Mar 13 21:08:49 linbit1 kernel: [506520.220025] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:49 linbit1 kernel: [506520.220107] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Terminating sender thread
Mar 13 21:08:49 linbit1 kernel: [506520.220143] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Starting sender thread (from drbd_r_pvc-4ab0 [2448422])
Mar 13 21:08:49 linbit1 kernel: [506520.281848] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Connection closed
Mar 13 21:08:49 linbit1 kernel: [506520.281864] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:49 linbit1 kernel: [506520.281870] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Terminating receiver thread
Mar 13 21:08:49 linbit1 kernel: [506520.281986] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591 linbit2: Terminating sender thread
Mar 13 21:08:49 linbit1 kernel: [506520.321620] drbd pvc-4ab00870-4b36-4d50-a172-8d3b35d24591: Terminating worker thread
Mar 13 21:08:49 linbit1 kernel: [506520.415027] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:49 linbit1 kernel: [506520.415115] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Terminating sender thread
Mar 13 21:08:49 linbit1 kernel: [506520.415151] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Starting sender thread (from drbd_r_pvc-6361 [2445737])
Mar 13 21:08:49 linbit1 kernel: [506520.473880] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Connection closed
Mar 13 21:08:49 linbit1 kernel: [506520.473898] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:49 linbit1 kernel: [506520.473904] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Terminating receiver thread
Mar 13 21:08:49 linbit1 kernel: [506520.473938] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b linbit3: Terminating sender thread
Mar 13 21:08:49 linbit1 kernel: [506520.509621] drbd pvc-63619ac2-d122-438b-adb1-8d387095e32b: Terminating worker thread
Mar 13 21:08:49 linbit1 kernel: [506520.632891] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Preparing remote state change 2833685725
Mar 13 21:08:49 linbit1 kernel: [506520.633123] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Committing remote state change 2833685725 (primary_nodes=0)
Mar 13 21:08:49 linbit1 kernel: [506520.633219] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:49 linbit1 kernel: [506520.633300] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:50 linbit1 kernel: [506521.448892] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:50 linbit1 kernel: [506521.461059] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578: State change failed: State change was refused by peer node
Mar 13 21:08:50 linbit1 kernel: [506521.471745] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:50 linbit1 kernel: [506521.471786] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:50 linbit1 kernel: [506521.483946] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578: State change failed: State change was refused by peer node
Mar 13 21:08:50 linbit1 kernel: [506521.494629] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578/0 drbd1015 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:50 linbit1 kernel: [506521.981541] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Preparing remote state change 1974479056
Mar 13 21:08:50 linbit1 kernel: [506521.981879] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Committing remote state change 1974479056 (primary_nodes=0)
Mar 13 21:08:50 linbit1 kernel: [506521.981963] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:50 linbit1 kernel: [506521.982057] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:51 linbit1 kernel: [506522.830430] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: Preparing cluster-wide state change 2304505634 (1->-1 7680/1024)
Mar 13 21:08:51 linbit1 kernel: [506522.833227] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: State change 2304505634: primary_nodes=0, weak_nodes=0
Mar 13 21:08:51 linbit1 kernel: [506522.833234] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: Committing cluster-wide state change 2304505634 (0ms)
Mar 13 21:08:51 linbit1 kernel: [506522.833271] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: disk( UpToDate -> Detaching )
Mar 13 21:08:51 linbit1 kernel: [506522.833442] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: disk( Detaching -> Diskless )
Mar 13 21:08:51 linbit1 kernel: [506522.834688] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: drbd_bm_resize called with capacity == 0
Mar 13 21:08:51 linbit1 kernel: [506522.841042] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Preparing remote state change 523244014
Mar 13 21:08:51 linbit1 kernel: [506522.841256] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Committing remote state change 523244014 (primary_nodes=0)
Mar 13 21:08:51 linbit1 kernel: [506522.841329] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:51 linbit1 kernel: [506522.841418] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:52 linbit1 kernel: [506523.135080] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:52 linbit1 kernel: [506523.147246] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: State change failed: State change was refused by peer node
Mar 13 21:08:52 linbit1 kernel: [506523.157931] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029: Failed: quorum( yes -> no )
Mar 13 21:08:52 linbit1 kernel: [506523.157936] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:52 linbit1 kernel: [506523.157990] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:52 linbit1 kernel: [506523.170149] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: State change failed: State change was refused by peer node
Mar 13 21:08:52 linbit1 kernel: [506523.180833] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef/0 drbd1029 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:52 linbit1 kernel: [506523.500940] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Preparing remote state change 4024714016
Mar 13 21:08:52 linbit1 kernel: [506523.501169] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Committing remote state change 4024714016 (primary_nodes=0)
Mar 13 21:08:52 linbit1 kernel: [506523.501290] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:52 linbit1 kernel: [506523.501415] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:52 linbit1 kernel: [506523.920483] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: sock was shut down by peer
Mar 13 21:08:52 linbit1 kernel: [506523.920498] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:52 linbit1 kernel: [506523.920568] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Terminating sender thread
Mar 13 21:08:52 linbit1 kernel: [506523.920604] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Starting sender thread (from drbd_r_pvc-b352 [2445472])
Mar 13 21:08:52 linbit1 kernel: [506523.989739] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Connection closed
Mar 13 21:08:52 linbit1 kernel: [506523.989750] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:52 linbit1 kernel: [506523.989759] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Restarting receiver thread
Mar 13 21:08:52 linbit1 kernel: [506523.989765] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:53 linbit1 kernel: [506524.238749] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:53 linbit1 kernel: [506524.238831] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Terminating sender thread
Mar 13 21:08:53 linbit1 kernel: [506524.238881] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Starting sender thread (from drbd_r_pvc-b352 [2445469])
Mar 13 21:08:53 linbit1 kernel: [506524.289655] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Connection closed
Mar 13 21:08:53 linbit1 kernel: [506524.289672] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:53 linbit1 kernel: [506524.289678] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Terminating receiver thread
Mar 13 21:08:53 linbit1 kernel: [506524.289745] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit2: Terminating sender thread
Mar 13 21:08:53 linbit1 kernel: [506524.290103] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:08:53 linbit1 kernel: [506524.290160] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Terminating sender thread
Mar 13 21:08:53 linbit1 kernel: [506524.290166] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Starting sender thread (from drbd_r_pvc-b352 [2445472])
Mar 13 21:08:53 linbit1 kernel: [506524.345731] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Connection closed
Mar 13 21:08:53 linbit1 kernel: [506524.345749] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:54 linbit1 kernel: [506525.373593] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd: State change failed: Need a connection to start verify or resync
Mar 13 21:08:54 linbit1 kernel: [506525.384807] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Failed: conn( StandAlone -> Connecting )
Mar 13 21:08:54 linbit1 kernel: [506525.384825] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Terminating receiver thread
Mar 13 21:08:54 linbit1 kernel: [506525.384895] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd linbit3: Terminating sender thread
Mar 13 21:08:54 linbit1 kernel: [506525.409636] drbd pvc-b352f35e-70af-4675-a246-d6ac1f09f8cd: Terminating worker thread
Mar 13 21:08:54 linbit1 kernel: [506525.800427] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: sock was shut down by peer
Mar 13 21:08:54 linbit1 kernel: [506525.800440] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:54 linbit1 kernel: [506525.800573] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Terminating sender thread
Mar 13 21:08:54 linbit1 kernel: [506525.800608] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Starting sender thread (from drbd_r_pvc-b502 [2448940])
Mar 13 21:08:54 linbit1 kernel: [506525.816711] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: sock was shut down by peer
Mar 13 21:08:54 linbit1 kernel: [506525.816718] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:54 linbit1 kernel: [506525.816807] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Terminating sender thread
Mar 13 21:08:54 linbit1 kernel: [506525.816840] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Starting sender thread (from drbd_r_pvc-b502 [2448938])
Mar 13 21:08:54 linbit1 kernel: [506525.855419] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Connection closed
Mar 13 21:08:54 linbit1 kernel: [506525.855427] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:54 linbit1 kernel: [506525.855433] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Restarting receiver thread
Mar 13 21:08:54 linbit1 kernel: [506525.855436] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:54 linbit1 kernel: [506525.869789] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Connection closed
Mar 13 21:08:54 linbit1 kernel: [506525.869802] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:54 linbit1 kernel: [506525.869813] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Restarting receiver thread
Mar 13 21:08:54 linbit1 kernel: [506525.869819] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:54 linbit1 kernel: [506525.966411] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Preparing cluster-wide state change 146674792 (0->-1 7680/1024)
Mar 13 21:08:54 linbit1 kernel: [506525.966694] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: State change 146674792: primary_nodes=0, weak_nodes=0
Mar 13 21:08:54 linbit1 kernel: [506525.966700] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Committing cluster-wide state change 146674792 (0ms)
Mar 13 21:08:54 linbit1 kernel: [506525.966732] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: disk( UpToDate -> Detaching )
Mar 13 21:08:54 linbit1 kernel: [506525.966873] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: disk( Detaching -> Diskless )
Mar 13 21:08:54 linbit1 kernel: [506525.967914] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: drbd_bm_resize called with capacity == 0
Mar 13 21:08:55 linbit1 kernel: [506526.269041] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:55 linbit1 kernel: [506526.281207] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: State change failed: State change was refused by peer node
Mar 13 21:08:55 linbit1 kernel: [506526.291893] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:55 linbit1 kernel: [506526.291956] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:55 linbit1 kernel: [506526.304116] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: State change failed: State change was refused by peer node
Mar 13 21:08:55 linbit1 kernel: [506526.314799] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017: Failed: quorum( yes -> no )
Mar 13 21:08:55 linbit1 kernel: [506526.314802] drbd pvc-400778d2-ad89-410f-90fd-625de046658b/0 drbd1017 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:55 linbit1 kernel: [506526.360971] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:55 linbit1 kernel: [506526.361036] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Terminating sender thread
Mar 13 21:08:55 linbit1 kernel: [506526.361155] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Starting sender thread (from drbd_r_pvc-b502 [2448938])
Mar 13 21:08:55 linbit1 kernel: [506526.369652] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 21:08:55 linbit1 kernel: [506526.369659] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:08:55 linbit1 kernel: [506526.385827] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 21:08:55 linbit1 kernel: [506526.445725] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Connection closed
Mar 13 21:08:55 linbit1 kernel: [506526.445744] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:55 linbit1 kernel: [506526.464464] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Preparing remote state change 3890061757
Mar 13 21:08:55 linbit1 kernel: [506526.464684] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Committing remote state change 3890061757 (primary_nodes=0)
Mar 13 21:08:55 linbit1 kernel: [506526.464693] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:08:55 linbit1 kernel: [506526.757533] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Preparing remote state change 428357351
Mar 13 21:08:55 linbit1 kernel: [506526.757810] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Committing remote state change 428357351 (primary_nodes=0)
Mar 13 21:08:55 linbit1 kernel: [506526.757905] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:55 linbit1 kernel: [506526.758002] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:08:56 linbit1 kernel: [506527.196692] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: sock was shut down by peer
Mar 13 21:08:56 linbit1 kernel: [506527.196708] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:56 linbit1 kernel: [506527.196776] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Terminating sender thread
Mar 13 21:08:56 linbit1 kernel: [506527.196817] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Starting sender thread (from drbd_r_pvc-7803 [2451907])
Mar 13 21:08:56 linbit1 kernel: [506527.236469] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Preparing remote state change 651208366
Mar 13 21:08:56 linbit1 kernel: [506527.245647] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Connection closed
Mar 13 21:08:56 linbit1 kernel: [506527.245656] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:08:56 linbit1 kernel: [506527.245665] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Restarting receiver thread
Mar 13 21:08:56 linbit1 kernel: [506527.245671] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:56 linbit1 kernel: [506527.276996] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Aborting remote state change 651208366
Mar 13 21:08:56 linbit1 kernel: [506527.453590] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578: State change failed: Need a connection to start verify or resync
Mar 13 21:08:56 linbit1 kernel: [506527.464804] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Failed: conn( StandAlone -> Connecting )
Mar 13 21:08:56 linbit1 kernel: [506527.464823] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Terminating receiver thread
Mar 13 21:08:56 linbit1 kernel: [506527.464873] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit2: Terminating sender thread
Mar 13 21:08:56 linbit1 kernel: [506527.465226] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:08:56 linbit1 kernel: [506527.465273] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: sock_recvmsg returned -4
Mar 13 21:08:56 linbit1 kernel: [506527.465401] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Terminating sender thread
Mar 13 21:08:56 linbit1 kernel: [506527.465442] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Starting sender thread (from drbd_r_pvc-b502 [2448940])
Mar 13 21:08:56 linbit1 kernel: [506527.513751] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Connection closed
Mar 13 21:08:56 linbit1 kernel: [506527.513769] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:56 linbit1 kernel: [506527.513775] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Terminating receiver thread
Mar 13 21:08:56 linbit1 kernel: [506527.513911] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578 linbit3: Terminating sender thread
Mar 13 21:08:56 linbit1 kernel: [506527.541610] drbd pvc-b502c895-62ea-4b2a-a97f-4f42a6ee1578: Terminating worker thread
Mar 13 21:08:56 linbit1 kernel: [506527.753474] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Preparing remote state change 2161782773
Mar 13 21:08:56 linbit1 kernel: [506527.753811] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Committing remote state change 2161782773 (primary_nodes=0)
Mar 13 21:08:56 linbit1 kernel: [506527.753827] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:08:56 linbit1 kernel: [506527.754103] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:08:57 linbit1 kernel: [506528.329827] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Preparing remote state change 749360501
Mar 13 21:08:57 linbit1 kernel: [506528.330048] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Committing remote state change 749360501 (primary_nodes=0)
Mar 13 21:08:57 linbit1 kernel: [506528.330062] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: pdsk( UpToDate -> Detaching )
Mar 13 21:08:57 linbit1 kernel: [506528.330386] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: pdsk( Detaching -> Diskless )
Mar 13 21:08:57 linbit1 kernel: [506528.656476] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:08:57 linbit1 kernel: [506528.656550] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Terminating sender thread
Mar 13 21:08:57 linbit1 kernel: [506528.656560] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Starting sender thread (from drbd_r_pvc-7803 [2451907])
Mar 13 21:08:57 linbit1 kernel: [506528.656715] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Connection closed
Mar 13 21:08:57 linbit1 kernel: [506528.656728] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:08:57 linbit1 kernel: [506528.656732] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Terminating receiver thread
Mar 13 21:08:57 linbit1 kernel: [506528.656799] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Terminating sender thread
Mar 13 21:08:57 linbit1 kernel: [506528.666961] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Starting sender thread (from drbdsetup [2487247])
Mar 13 21:08:57 linbit1 kernel: [506528.670081] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:08:57 linbit1 kernel: [506528.670113] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Starting receiver thread (from drbd_w_pvc-7803 [2451888])
Mar 13 21:08:57 linbit1 kernel: [506528.670232] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Unconnected -> Connecting )
Mar 13 21:08:57 linbit1 kernel: [506528.828143] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Preparing remote state change 2813553434
Mar 13 21:08:57 linbit1 kernel: [506528.828338] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Committing remote state change 2813553434 (primary_nodes=0)
Mar 13 21:08:57 linbit1 kernel: [506528.828352] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit2: pdsk( UpToDate -> Detaching )
Mar 13 21:08:57 linbit1 kernel: [506528.828553] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit2: pdsk( Detaching -> Diskless )
Mar 13 21:08:58 linbit1 kernel: [506529.070189] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Preparing cluster-wide state change 857035143 (1->-1 7680/1024)
Mar 13 21:08:58 linbit1 kernel: [506529.070542] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: State change 857035143: primary_nodes=0, weak_nodes=0
Mar 13 21:08:58 linbit1 kernel: [506529.070548] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Committing cluster-wide state change 857035143 (0ms)
Mar 13 21:08:58 linbit1 kernel: [506529.070578] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( UpToDate -> Detaching )
Mar 13 21:08:58 linbit1 kernel: [506529.070688] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:58 linbit1 kernel: [506529.070697] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: disk( Detaching -> Diskless )
Mar 13 21:08:58 linbit1 kernel: [506529.071625] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: drbd_bm_resize called with capacity == 0
Mar 13 21:08:58 linbit1 kernel: [506529.333452] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:58 linbit1 kernel: [506529.345616] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: State change failed: State change was refused by peer node
Mar 13 21:08:58 linbit1 kernel: [506529.356300] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: Failed: quorum( yes -> no )
Mar 13 21:08:58 linbit1 kernel: [506529.356304] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:58 linbit1 kernel: [506529.356420] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:58 linbit1 kernel: [506529.368579] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: State change failed: State change was refused by peer node
Mar 13 21:08:58 linbit1 kernel: [506529.376424] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: sock was shut down by peer
Mar 13 21:08:58 linbit1 kernel: [506529.379261] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033: Failed: quorum( yes -> no )
Mar 13 21:08:58 linbit1 kernel: [506529.379265] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be/0 drbd1033 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:58 linbit1 kernel: [506529.379279] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:08:58 linbit1 kernel: [506529.379350] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Terminating sender thread
Mar 13 21:08:58 linbit1 kernel: [506529.379388] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Starting sender thread (from drbd_r_pvc-4007 [2448911])
Mar 13 21:08:58 linbit1 kernel: [506529.406798] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( BrokenPipe -> Disconnecting )
Mar 13 21:08:58 linbit1 kernel: [506529.425733] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Connection closed
Mar 13 21:08:58 linbit1 kernel: [506529.425752] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:08:58 linbit1 kernel: [506529.425758] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Terminating receiver thread
Mar 13 21:08:58 linbit1 kernel: [506529.425876] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Terminating sender thread
Mar 13 21:08:58 linbit1 kernel: [506529.434600] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Starting sender thread (from drbdsetup [2487303])
Mar 13 21:08:58 linbit1 kernel: [506529.437211] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:08:58 linbit1 kernel: [506529.437244] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Starting receiver thread (from drbd_w_pvc-4007 [2446824])
Mar 13 21:08:58 linbit1 kernel: [506529.437360] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( Unconnected -> Connecting )
Mar 13 21:08:58 linbit1 kernel: [506529.439970] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Preparing cluster-wide state change 901134870 (0->-1 7680/1024)
Mar 13 21:08:58 linbit1 kernel: [506529.440234] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change 901134870: primary_nodes=0, weak_nodes=0
Mar 13 21:08:58 linbit1 kernel: [506529.440239] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Committing cluster-wide state change 901134870 (0ms)
Mar 13 21:08:58 linbit1 kernel: [506529.440266] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: disk( UpToDate -> Detaching )
Mar 13 21:08:58 linbit1 kernel: [506529.440378] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:58 linbit1 kernel: [506529.440386] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: disk( Detaching -> Diskless )
Mar 13 21:08:58 linbit1 kernel: [506529.441343] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: drbd_bm_resize called with capacity == 0
Mar 13 21:08:58 linbit1 kernel: [506529.619197] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Preparing remote state change 2872251531
Mar 13 21:08:58 linbit1 kernel: [506529.619435] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Committing remote state change 2872251531 (primary_nodes=0)
Mar 13 21:08:58 linbit1 kernel: [506529.619449] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:08:58 linbit1 kernel: [506529.619625] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:08:58 linbit1 kernel: [506529.689510] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:58 linbit1 kernel: [506529.701676] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change failed: State change was refused by peer node
Mar 13 21:08:58 linbit1 kernel: [506529.712360] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: Failed: quorum( yes -> no )
Mar 13 21:08:58 linbit1 kernel: [506529.712366] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:58 linbit1 kernel: [506529.712423] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:58 linbit1 kernel: [506529.724582] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change failed: State change was refused by peer node
Mar 13 21:08:58 linbit1 kernel: [506529.735265] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019: Failed: quorum( yes -> no )
Mar 13 21:08:58 linbit1 kernel: [506529.735269] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588/0 drbd1019 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:58 linbit1 kernel: [506529.782920] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:08:58 linbit1 kernel: [506529.782926] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:58 linbit1 kernel: [506529.795084] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7: State change failed: State change was refused by peer node
Mar 13 21:08:58 linbit1 kernel: [506529.805766] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:08:58 linbit1 kernel: [506529.805793] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:08:58 linbit1 kernel: [506529.817949] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7: State change failed: State change was refused by peer node
Mar 13 21:08:58 linbit1 kernel: [506529.828630] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7/0 drbd1032 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.604992    2448 scope.go:115] "RemoveContainer" containerID="d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.606384922Z" level=info msg="RemoveContainer for \"d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.608864850Z" level=info msg="RemoveContainer for \"d4b6055d7afaaec2f3249d13044fa4c00e342a0651c2d535075ed3a4da5df65e\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.609073    2448 scope.go:115] "RemoveContainer" containerID="b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.610424855Z" level=info msg="RemoveContainer for \"b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.612303981Z" level=info msg="RemoveContainer for \"b7d832e45cb6c9c55f3f1dd608ab6351940e297b496315401ddc21c774e37b54\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.612508    2448 scope.go:115] "RemoveContainer" containerID="dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.613532625Z" level=info msg="RemoveContainer for \"dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.614699428Z" level=info msg="RemoveContainer for \"dee55430ccd3631fba10d55b65554c31d873f4bc78b158346125870ce1327b65\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.614857    2448 scope.go:115] "RemoveContainer" containerID="e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.616223233Z" level=info msg="RemoveContainer for \"e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.618713201Z" level=info msg="RemoveContainer for \"e74e0200bca7216ed6d24c794d09ccc3f9e89adbb7d4d776e4196b5b8a265fa1\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.618923    2448 scope.go:115] "RemoveContainer" containerID="4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.620016686Z" level=info msg="RemoveContainer for \"4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.621263050Z" level=info msg="RemoveContainer for \"4d6df716532afcd5251b268f4fa9340e5f869be6546c20d0bc88aa2af1af1b99\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.621417    2448 scope.go:115] "RemoveContainer" containerID="2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.622614054Z" level=info msg="RemoveContainer for \"2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.623973818Z" level=info msg="RemoveContainer for \"2d59d2d9ffd9f50325d0a71cd1564caa56f97fa23485b93a7171aee4b8ec41bd\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.624137    2448 scope.go:115] "RemoveContainer" containerID="6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.625027902Z" level=info msg="RemoveContainer for \"6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.626384746Z" level=info msg="RemoveContainer for \"6533f1a46c3e50e285067585e673097948777dfeea59d08f82a84d1d650cc4e7\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.626542    2448 scope.go:115] "RemoveContainer" containerID="a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.627696830Z" level=info msg="RemoveContainer for \"a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.629057075Z" level=info msg="RemoveContainer for \"a6ce137faab9a5a98cdffa940d0333df5987b1012b1d324df12ca953a68bdb29\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.629255    2448 scope.go:115] "RemoveContainer" containerID="8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.630333199Z" level=info msg="RemoveContainer for \"8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.631441282Z" level=info msg="RemoveContainer for \"8c8397679abf5ae0b7044ec62d7909823526b54802a8df4c8a35f97499433291\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.631601    2448 scope.go:115] "RemoveContainer" containerID="1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.632692207Z" level=info msg="RemoveContainer for \"1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.634441692Z" level=info msg="RemoveContainer for \"1781f721b24c481f144ee9ab07c25522c562b69993006c3ac078214565dac78c\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.634640    2448 scope.go:115] "RemoveContainer" containerID="e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.635584536Z" level=info msg="RemoveContainer for \"e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.637559302Z" level=info msg="RemoveContainer for \"e70001731f25ab88660424efa736ba98b97e5e4b445618cc844c82c994cb5dfe\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.637770    2448 scope.go:115] "RemoveContainer" containerID="8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.638816266Z" level=info msg="RemoveContainer for \"8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.640024470Z" level=info msg="RemoveContainer for \"8bf4cb440046425788b298768671178b456cbdc00ad4bab4a5f5c8add64c74a1\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.640186    2448 scope.go:115] "RemoveContainer" containerID="d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.641173714Z" level=info msg="RemoveContainer for \"d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.642681799Z" level=info msg="RemoveContainer for \"d46e93dfa863fba6459410da48d12b866cbd8575d86a40292aa2114c5214710c\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.642848    2448 scope.go:115] "RemoveContainer" containerID="dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.643885003Z" level=info msg="RemoveContainer for \"dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.647108613Z" level=info msg="RemoveContainer for \"dcb0013c0f13683e0f04baaef2422c2562c435b4369c794a3b14e722b36c8e7a\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.647409    2448 scope.go:115] "RemoveContainer" containerID="9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.650412224Z" level=info msg="RemoveContainer for \"9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.653929075Z" level=info msg="RemoveContainer for \"9bca696ccbffe23ed7e53f91b5c425e1181057ab2ce701d06fa6d4bb1d2e4ecf\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.654200    2448 scope.go:115] "RemoveContainer" containerID="a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.655786041Z" level=info msg="RemoveContainer for \"a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.657886288Z" level=info msg="RemoveContainer for \"a6ee4f3fdfb3e156a4ac45bc90bac480ba2155ff41de69ff4d1438fbc341f3fe\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.658129    2448 scope.go:115] "RemoveContainer" containerID="7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.659677774Z" level=info msg="RemoveContainer for \"7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.660979858Z" level=info msg="RemoveContainer for \"7616afb13db67c62eb5553d8330e488dd0c9bf9de3db662ae96b71f787b9ff23\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.661143    2448 scope.go:115] "RemoveContainer" containerID="b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.661900261Z" level=info msg="RemoveContainer for \"b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.663031505Z" level=info msg="RemoveContainer for \"b0d0990bbc0a36a9ea041f36823ee4062918cea8d6af35b8cb4f9366e9ba83ba\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.663164    2448 scope.go:115] "RemoveContainer" containerID="914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.664169388Z" level=info msg="RemoveContainer for \"914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.665638673Z" level=info msg="RemoveContainer for \"914137831aebfa1ef58029efc8c6fbc893fe4a46d7d8e2647d22f7d6f2dab0e0\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.665816    2448 scope.go:115] "RemoveContainer" containerID="04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.667028757Z" level=info msg="RemoveContainer for \"04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086\""
Mar 13 21:09:00 linbit1 kernel: [506531.676341] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:09:00 linbit1 kernel: [506531.676412] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Terminating sender thread
Mar 13 21:09:00 linbit1 kernel: [506531.676425] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Starting sender thread (from drbd_r_pvc-7803 [2487251])
Mar 13 21:09:00 linbit1 kernel: [506531.676680] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Connection closed
Mar 13 21:09:00 linbit1 kernel: [506531.676693] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:00 linbit1 kernel: [506531.676699] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Terminating receiver thread
Mar 13 21:09:00 linbit1 kernel: [506531.676721] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit2: Terminating sender thread
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.669132604Z" level=info msg="RemoveContainer for \"04d80bf77c073b4beeb63aee4867746710ede7d87c065890741d214291c68086\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.669357    2448 scope.go:115] "RemoveContainer" containerID="93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.670371408Z" level=info msg="RemoveContainer for \"93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.671865493Z" level=info msg="RemoveContainer for \"93e944da7d38bf1ad52717458e19a565f5efa255f0c93c2db0cee36af997780e\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.672037    2448 scope.go:115] "RemoveContainer" containerID="a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.673127857Z" level=info msg="RemoveContainer for \"a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.678123753Z" level=info msg="RemoveContainer for \"a7d913486befa810f1393de4d1bfdfe0793f93412e711753886d93c8900173d1\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.678369    2448 scope.go:115] "RemoveContainer" containerID="4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.679621038Z" level=info msg="RemoveContainer for \"4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.681668685Z" level=info msg="RemoveContainer for \"4fd956158829e8a5afcc55d211cf941310134922922eb4cfc1acb303f656a4f1\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.681895    2448 scope.go:115] "RemoveContainer" containerID="9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.682976009Z" level=info msg="RemoveContainer for \"9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.684147773Z" level=info msg="RemoveContainer for \"9756e4a48589526ca46a51db4dbbc18076389705a163ee02fca5e91cb59351bc\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.684292    2448 scope.go:115] "RemoveContainer" containerID="516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.685266856Z" level=info msg="RemoveContainer for \"516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.686396460Z" level=info msg="RemoveContainer for \"516f988cc467a2474d22179df699950a5c3f6b54466fe99746f3a0951372d011\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.686571    2448 scope.go:115] "RemoveContainer" containerID="13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.687577704Z" level=info msg="RemoveContainer for \"13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.689622190Z" level=info msg="RemoveContainer for \"13998de857fbcefa755f2a5cd034227b34e61aa6cc42cb9e7c290e479b34690f\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.689819    2448 scope.go:115] "RemoveContainer" containerID="08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.691106195Z" level=info msg="RemoveContainer for \"08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.692595040Z" level=info msg="RemoveContainer for \"08f8b91aa371f5d941eee1910c4bbfa8490acb18e82fdfa8312304ea6f38450a\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.692810    2448 scope.go:115] "RemoveContainer" containerID="795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.693774284Z" level=info msg="RemoveContainer for \"795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.695067368Z" level=info msg="RemoveContainer for \"795072c36869141313286cea28e920e4445444aebf9876733983e981a79ade5b\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.695238    2448 scope.go:115] "RemoveContainer" containerID="5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.696370332Z" level=info msg="RemoveContainer for \"5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.697642136Z" level=info msg="RemoveContainer for \"5ed161a4cc833ba8fcad0ec5b1ff70c33495198d80500523659438c5443049b7\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.697828    2448 scope.go:115] "RemoveContainer" containerID="738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.698906020Z" level=info msg="RemoveContainer for \"738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.700289705Z" level=info msg="RemoveContainer for \"738a8db0426e3dda7385615892db4d7413e0a8cc2cf0c5bcfcddca1e46ceae6c\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.700465    2448 scope.go:115] "RemoveContainer" containerID="11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.701601349Z" level=info msg="RemoveContainer for \"11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.703414635Z" level=info msg="RemoveContainer for \"11549779703299120f83a89950b9dab0a2e96502e5f421e5079d2fbbc097e3ec\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-kubelite[2448]: I0313 21:09:00.703603    2448 scope.go:115] "RemoveContainer" containerID="716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.704730959Z" level=info msg="RemoveContainer for \"716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.706167004Z" level=info msg="RemoveContainer for \"716fc5b7c51829c74e1e219220af214f2919a2b8506a6e6170574281af85b928\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.707528608Z" level=info msg="StopPodSandbox for \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.780 [WARNING][2487415] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.781 [INFO][2487415] k8s.go 576: Cleaning up netns ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.781 [INFO][2487415] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" iface="eth0" netns=""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.781 [INFO][2487415] k8s.go 583: Releasing IP address(es) ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.781 [INFO][2487415] utils.go 196: Calico CNI releasing IP address ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.811 [INFO][2487437] ipam_plugin.go 411: Releasing address using handleID ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.832 [WARNING][2487437] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.832 [INFO][2487437] ipam_plugin.go 439: Releasing address using workloadID ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.840 [INFO][2487415] k8s.go 589: Teardown processing complete. ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.844788332Z" level=info msg="TearDown network for sandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.844851532Z" level=info msg="StopPodSandbox for \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.845377694Z" level=info msg="RemovePodSandbox for \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.845449334Z" level=info msg="Forcibly stopping sandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.917 [WARNING][2487474] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.918 [INFO][2487474] k8s.go 576: Cleaning up netns ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.918 [INFO][2487474] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" iface="eth0" netns=""
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.918 [INFO][2487474] k8s.go 583: Releasing IP address(es) ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.918 [INFO][2487474] utils.go 196: Calico CNI releasing IP address ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.951 [INFO][2487496] ipam_plugin.go 411: Releasing address using handleID ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.972 [WARNING][2487496] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.972 [INFO][2487496] ipam_plugin.go 439: Releasing address using workloadID ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" HandleID="k8s-pod-network.3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61" Workload="linbit1-k8s-fio--bench--r2--n7--3--mfqd4-eth0"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:00.987 [INFO][2487474] k8s.go 589: Teardown processing complete. ContainerID="3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.991048324Z" level=info msg="TearDown network for sandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.993154131Z" level=info msg="RemovePodSandbox \"3a6c6cb0301264bdb8af3ee8ba1c5568ef861cdd0fccd0a502c5737e56d09b61\" returns successfully"
Mar 13 21:09:00 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:00.993556252Z" level=info msg="StopPodSandbox for \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.070 [WARNING][2487533] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.071 [INFO][2487533] k8s.go 576: Cleaning up netns ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.071 [INFO][2487533] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" iface="eth0" netns=""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.071 [INFO][2487533] k8s.go 583: Releasing IP address(es) ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.071 [INFO][2487533] utils.go 196: Calico CNI releasing IP address ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.101 [INFO][2487553] ipam_plugin.go 411: Releasing address using handleID ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.124 [WARNING][2487553] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.125 [INFO][2487553] ipam_plugin.go 439: Releasing address using workloadID ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.136 [INFO][2487533] k8s.go 589: Teardown processing complete. ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.140620887Z" level=info msg="TearDown network for sandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.140678528Z" level=info msg="StopPodSandbox for \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" returns successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.141232689Z" level=info msg="RemovePodSandbox for \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.141302450Z" level=info msg="Forcibly stopping sandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\""
Mar 13 21:09:01 linbit1 kernel: [506532.176443] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: sock was shut down by peer
Mar 13 21:09:01 linbit1 kernel: [506532.176461] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:09:01 linbit1 kernel: [506532.176552] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Terminating sender thread
Mar 13 21:09:01 linbit1 kernel: [506532.176604] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Starting sender thread (from drbd_r_pvc-329d [2454394])
Mar 13 21:09:01 linbit1 kernel: [506532.233767] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Connection closed
Mar 13 21:09:01 linbit1 kernel: [506532.233780] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:09:01 linbit1 kernel: [506532.233791] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Restarting receiver thread
Mar 13 21:09:01 linbit1 kernel: [506532.233798] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.216 [WARNING][2487588] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.216 [INFO][2487588] k8s.go 576: Cleaning up netns ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.216 [INFO][2487588] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" iface="eth0" netns=""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.216 [INFO][2487588] k8s.go 583: Releasing IP address(es) ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.216 [INFO][2487588] utils.go 196: Calico CNI releasing IP address ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.248 [INFO][2487610] ipam_plugin.go 411: Releasing address using handleID ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.277 [WARNING][2487610] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.277 [INFO][2487610] ipam_plugin.go 439: Releasing address using workloadID ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" HandleID="k8s-pod-network.4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672" Workload="linbit1-k8s-fio--bench--r2--n0--4--9lr25-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.289 [INFO][2487588] k8s.go 589: Teardown processing complete. ContainerID="4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.292925179Z" level=info msg="TearDown network for sandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.295572388Z" level=info msg="RemovePodSandbox \"4d954604f3e0276df72fbae2db371f494c5401bccabd4b79ab476bb3d6e87672\" returns successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.296169390Z" level=info msg="StopPodSandbox for \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.370 [WARNING][2487645] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.371 [INFO][2487645] k8s.go 576: Cleaning up netns ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.371 [INFO][2487645] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" iface="eth0" netns=""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.371 [INFO][2487645] k8s.go 583: Releasing IP address(es) ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.371 [INFO][2487645] utils.go 196: Calico CNI releasing IP address ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.404 [INFO][2487667] ipam_plugin.go 411: Releasing address using handleID ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.426 [WARNING][2487667] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.426 [INFO][2487667] ipam_plugin.go 439: Releasing address using workloadID ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.439 [INFO][2487645] k8s.go 589: Teardown processing complete. ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.443394985Z" level=info msg="TearDown network for sandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.443453586Z" level=info msg="StopPodSandbox for \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" returns successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.444031907Z" level=info msg="RemovePodSandbox for \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.444100748Z" level=info msg="Forcibly stopping sandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.561 [WARNING][2487704] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.562 [INFO][2487704] k8s.go 576: Cleaning up netns ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.562 [INFO][2487704] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" iface="eth0" netns=""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.562 [INFO][2487704] k8s.go 583: Releasing IP address(es) ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.562 [INFO][2487704] utils.go 196: Calico CNI releasing IP address ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.594 [INFO][2487726] ipam_plugin.go 411: Releasing address using handleID ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.617 [WARNING][2487726] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.618 [INFO][2487726] ipam_plugin.go 439: Releasing address using workloadID ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" HandleID="k8s-pod-network.173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7" Workload="linbit1-k8s-fio--bench--r2--n8--1--5nnlm-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.633 [INFO][2487704] k8s.go 589: Teardown processing complete. ContainerID="173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.638045974Z" level=info msg="TearDown network for sandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.640449302Z" level=info msg="RemovePodSandbox \"173a4660c744edafc05df26fe480f0530ff8c522e60101c86ab456caaf454bd7\" returns successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.641008184Z" level=info msg="StopPodSandbox for \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.714 [WARNING][2487761] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.714 [INFO][2487761] k8s.go 576: Cleaning up netns ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.715 [INFO][2487761] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" iface="eth0" netns=""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.715 [INFO][2487761] k8s.go 583: Releasing IP address(es) ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.715 [INFO][2487761] utils.go 196: Calico CNI releasing IP address ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.750 [INFO][2487783] ipam_plugin.go 411: Releasing address using handleID ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.778 [WARNING][2487783] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.778 [INFO][2487783] ipam_plugin.go 439: Releasing address using workloadID ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.786 [INFO][2487761] k8s.go 589: Teardown processing complete. ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.790454787Z" level=info msg="TearDown network for sandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.790526427Z" level=info msg="StopPodSandbox for \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" returns successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.791127189Z" level=info msg="RemovePodSandbox for \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.791238189Z" level=info msg="Forcibly stopping sandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.866 [WARNING][2487814] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.867 [INFO][2487814] k8s.go 576: Cleaning up netns ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.867 [INFO][2487814] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" iface="eth0" netns=""
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.867 [INFO][2487814] k8s.go 583: Releasing IP address(es) ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.867 [INFO][2487814] utils.go 196: Calico CNI releasing IP address ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.900 [INFO][2487835] ipam_plugin.go 411: Releasing address using handleID ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.924 [WARNING][2487835] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.924 [INFO][2487835] ipam_plugin.go 439: Releasing address using workloadID ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" HandleID="k8s-pod-network.a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6" Workload="linbit1-k8s-fio--bench--r2--n3--0--zq6bl-eth0"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:01.933 [INFO][2487814] k8s.go 589: Teardown processing complete. ContainerID="a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.937423061Z" level=info msg="TearDown network for sandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.940055470Z" level=info msg="RemovePodSandbox \"a26d53ca0341be7eddece0de8848af6409ce554402fd1144bb84543c2f9ed5a6\" returns successfully"
Mar 13 21:09:01 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:01.940611832Z" level=info msg="StopPodSandbox for \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\""
Mar 13 21:09:01 linbit1 kernel: [506532.984090] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:09:01 linbit1 kernel: [506532.984182] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Terminating sender thread
Mar 13 21:09:01 linbit1 kernel: [506532.984287] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Starting sender thread (from drbd_r_pvc-329d [2454394])
Mar 13 21:09:01 linbit1 kernel: [506532.984445] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Connection closed
Mar 13 21:09:01 linbit1 kernel: [506532.984458] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:01 linbit1 kernel: [506532.984463] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Terminating receiver thread
Mar 13 21:09:01 linbit1 kernel: [506532.984563] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Terminating sender thread
Mar 13 21:09:01 linbit1 kernel: [506532.995417] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Starting sender thread (from drbdsetup [2487899])
Mar 13 21:09:01 linbit1 kernel: [506532.998323] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:09:01 linbit1 kernel: [506532.998355] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Starting receiver thread (from drbd_w_pvc-329d [2454364])
Mar 13 21:09:01 linbit1 kernel: [506532.998461] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.020 [WARNING][2487870] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.020 [INFO][2487870] k8s.go 576: Cleaning up netns ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.020 [INFO][2487870] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" iface="eth0" netns=""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.020 [INFO][2487870] k8s.go 583: Releasing IP address(es) ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.020 [INFO][2487870] utils.go 196: Calico CNI releasing IP address ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.051 [INFO][2487913] ipam_plugin.go 411: Releasing address using handleID ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.076 [WARNING][2487913] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.076 [INFO][2487913] ipam_plugin.go 439: Releasing address using workloadID ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.085 [INFO][2487870] k8s.go 589: Teardown processing complete. ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.089614873Z" level=info msg="TearDown network for sandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.089738673Z" level=info msg="StopPodSandbox for \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" returns successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.090290315Z" level=info msg="RemovePodSandbox for \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.090345475Z" level=info msg="Forcibly stopping sandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\""
Mar 13 21:09:02 linbit1 kernel: [506533.188401] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: sock was shut down by peer
Mar 13 21:09:02 linbit1 kernel: [506533.188414] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:09:02 linbit1 kernel: [506533.188480] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Terminating sender thread
Mar 13 21:09:02 linbit1 kernel: [506533.188517] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Starting sender thread (from drbd_r_pvc-d2f1 [2454632])
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.171 [WARNING][2487952] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.172 [INFO][2487952] k8s.go 576: Cleaning up netns ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.172 [INFO][2487952] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" iface="eth0" netns=""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.172 [INFO][2487952] k8s.go 583: Releasing IP address(es) ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.172 [INFO][2487952] utils.go 196: Calico CNI releasing IP address ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.202 [INFO][2487973] ipam_plugin.go 411: Releasing address using handleID ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.226 [WARNING][2487973] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.226 [INFO][2487973] ipam_plugin.go 439: Releasing address using workloadID ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" HandleID="k8s-pod-network.a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078" Workload="linbit1-k8s-fio--bench--r2--n0--1--559g2-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.240 [INFO][2487952] k8s.go 589: Teardown processing complete. ContainerID="a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.244316613Z" level=info msg="TearDown network for sandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.246721100Z" level=info msg="RemovePodSandbox \"a158f70ca591b9630ccaeaa3615cbd451f735d9d1f24b252b1377404e6712078\" returns successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.247251622Z" level=info msg="StopPodSandbox for \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\""
Mar 13 21:09:02 linbit1 kernel: [506533.261636] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Connection closed
Mar 13 21:09:02 linbit1 kernel: [506533.261649] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:09:02 linbit1 kernel: [506533.261658] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Restarting receiver thread
Mar 13 21:09:02 linbit1 kernel: [506533.261663] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:02 linbit1 kernel: [506533.362288] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Preparing cluster-wide state change 1343895500 (0->-1 7680/1024)
Mar 13 21:09:02 linbit1 kernel: [506533.362632] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: State change 1343895500: primary_nodes=0, weak_nodes=0
Mar 13 21:09:02 linbit1 kernel: [506533.362637] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Committing cluster-wide state change 1343895500 (0ms)
Mar 13 21:09:02 linbit1 kernel: [506533.362670] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: disk( UpToDate -> Detaching )
Mar 13 21:09:02 linbit1 kernel: [506533.362808] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: disk( Detaching -> Diskless )
Mar 13 21:09:02 linbit1 kernel: [506533.364141] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: drbd_bm_resize called with capacity == 0
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.312 [WARNING][2488041] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.312 [INFO][2488041] k8s.go 576: Cleaning up netns ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.312 [INFO][2488041] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" iface="eth0" netns=""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.312 [INFO][2488041] k8s.go 583: Releasing IP address(es) ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.312 [INFO][2488041] utils.go 196: Calico CNI releasing IP address ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.345 [INFO][2488079] ipam_plugin.go 411: Releasing address using handleID ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.367 [WARNING][2488079] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.367 [INFO][2488079] ipam_plugin.go 439: Releasing address using workloadID ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.380 [INFO][2488041] k8s.go 589: Teardown processing complete. ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.385243508Z" level=info msg="TearDown network for sandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.385343628Z" level=info msg="StopPodSandbox for \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" returns successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.385816670Z" level=info msg="RemovePodSandbox for \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.385861150Z" level=info msg="Forcibly stopping sandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.461 [WARNING][2488123] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.461 [INFO][2488123] k8s.go 576: Cleaning up netns ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.461 [INFO][2488123] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" iface="eth0" netns=""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.461 [INFO][2488123] k8s.go 583: Releasing IP address(es) ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.461 [INFO][2488123] utils.go 196: Calico CNI releasing IP address ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.489 [INFO][2488148] ipam_plugin.go 411: Releasing address using handleID ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.511 [WARNING][2488148] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.511 [INFO][2488148] ipam_plugin.go 439: Releasing address using workloadID ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" HandleID="k8s-pod-network.71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2" Workload="linbit1-k8s-fio--bench--r2--n1--1--vjkzb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.519 [INFO][2488123] k8s.go 589: Teardown processing complete. ContainerID="71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.522977913Z" level=info msg="TearDown network for sandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.525326600Z" level=info msg="RemovePodSandbox \"71676ffafabcdf6c040a7bd13918744938cfb1ff0f9015b618caabdc9b9794b2\" returns successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.525923122Z" level=info msg="StopPodSandbox for \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\""
Mar 13 21:09:02 linbit1 kernel: [506533.692406] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: sock was shut down by peer
Mar 13 21:09:02 linbit1 kernel: [506533.692420] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:09:02 linbit1 kernel: [506533.692489] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Terminating sender thread
Mar 13 21:09:02 linbit1 kernel: [506533.692529] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Starting sender thread (from drbd_r_pvc-7803 [2451910])
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.597 [WARNING][2488191] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.597 [INFO][2488191] k8s.go 576: Cleaning up netns ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.597 [INFO][2488191] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" iface="eth0" netns=""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.597 [INFO][2488191] k8s.go 583: Releasing IP address(es) ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.597 [INFO][2488191] utils.go 196: Calico CNI releasing IP address ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.657 [INFO][2488216] ipam_plugin.go 411: Releasing address using handleID ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.682 [WARNING][2488216] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.682 [INFO][2488216] ipam_plugin.go 439: Releasing address using workloadID ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.698 [INFO][2488191] k8s.go 589: Teardown processing complete. ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.701753770Z" level=info msg="TearDown network for sandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.701817490Z" level=info msg="StopPodSandbox for \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" returns successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.702421612Z" level=info msg="RemovePodSandbox for \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.702501613Z" level=info msg="Forcibly stopping sandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\""
Mar 13 21:09:02 linbit1 kernel: [506533.697911] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:02 linbit1 kernel: [506533.710078] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: State change failed: State change was refused by peer node
Mar 13 21:09:02 linbit1 kernel: [506533.720765] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:02 linbit1 kernel: [506533.720818] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:02 linbit1 kernel: [506533.732978] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: State change failed: State change was refused by peer node
Mar 13 21:09:02 linbit1 kernel: [506533.743662] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034: Failed: quorum( yes -> no )
Mar 13 21:09:02 linbit1 kernel: [506533.743666] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7/0 drbd1034 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:02 linbit1 kernel: [506533.748986] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Preparing remote state change 3556062803
Mar 13 21:09:02 linbit1 kernel: [506533.749146] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Committing remote state change 3556062803 (primary_nodes=0)
Mar 13 21:09:02 linbit1 kernel: [506533.749261] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:09:02 linbit1 kernel: [506533.749377] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:09:02 linbit1 kernel: [506533.782002] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Connection closed
Mar 13 21:09:02 linbit1 kernel: [506533.782012] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:09:02 linbit1 kernel: [506533.782020] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Restarting receiver thread
Mar 13 21:09:02 linbit1 kernel: [506533.782024] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:02 linbit1 kernel: [506533.789794] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:02 linbit1 kernel: [506533.789952] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Terminating sender thread
Mar 13 21:09:02 linbit1 kernel: [506533.789983] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Starting sender thread (from drbd_r_pvc-b01e [2449062])
Mar 13 21:09:02 linbit1 kernel: [506533.793629] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 21:09:02 linbit1 kernel: [506533.793638] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:09:02 linbit1 kernel: [506533.793870] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.781 [WARNING][2488297] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.781 [INFO][2488297] k8s.go 576: Cleaning up netns ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.781 [INFO][2488297] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" iface="eth0" netns=""
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.781 [INFO][2488297] k8s.go 583: Releasing IP address(es) ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.782 [INFO][2488297] utils.go 196: Calico CNI releasing IP address ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.811 [INFO][2488326] ipam_plugin.go 411: Releasing address using handleID ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.833 [WARNING][2488326] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.833 [INFO][2488326] ipam_plugin.go 439: Releasing address using workloadID ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" HandleID="k8s-pod-network.d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf" Workload="linbit1-k8s-fio--bench--r2--n1--3--hntjb-eth0"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.848 [INFO][2488297] k8s.go 589: Teardown processing complete. ContainerID="d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.852440937Z" level=info msg="TearDown network for sandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.854813705Z" level=info msg="RemovePodSandbox \"d89bed88b5c27ebf9fda626ff3eaedfaad3ee471598fc421ecd3068d7bb93ccf\" returns successfully"
Mar 13 21:09:02 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02.855283786Z" level=info msg="StopPodSandbox for \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\""
Mar 13 21:09:02 linbit1 kernel: [506533.864464] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Preparing remote state change 2926493379
Mar 13 21:09:02 linbit1 kernel: [506533.864647] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Committing remote state change 2926493379 (primary_nodes=0)
Mar 13 21:09:02 linbit1 kernel: [506533.881693] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Connection closed
Mar 13 21:09:02 linbit1 kernel: [506533.881712] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:02 linbit1 kernel: [506533.881718] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Terminating receiver thread
Mar 13 21:09:02 linbit1 kernel: [506533.881804] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Terminating sender thread
Mar 13 21:09:02 linbit1 kernel: [506533.892787] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Starting sender thread (from drbdsetup [2488375])
Mar 13 21:09:02 linbit1 kernel: [506533.895840] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:09:02 linbit1 kernel: [506533.895879] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Starting receiver thread (from drbd_w_pvc-b01e [2447053])
Mar 13 21:09:02 linbit1 kernel: [506533.896045] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:09:02 linbit1 kernel: [506533.899476] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:02 linbit1 kernel: [506533.899558] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Terminating sender thread
Mar 13 21:09:02 linbit1 kernel: [506533.899598] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Starting sender thread (from drbd_r_pvc-d2f1 [2454630])
Mar 13 21:09:02 linbit1 kernel: [506533.940515] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Preparing remote state change 146084826
Mar 13 21:09:02 linbit1 kernel: [506533.940690] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Committing remote state change 146084826 (primary_nodes=0)
Mar 13 21:09:02 linbit1 kernel: [506533.940702] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:09:02 linbit1 kernel: [506533.989746] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Connection closed
Mar 13 21:09:02 linbit1 kernel: [506533.989765] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:02 linbit1 kernel: [506533.989772] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Terminating receiver thread
Mar 13 21:09:02 linbit1 kernel: [506533.989947] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit2: Terminating sender thread
Mar 13 21:09:02 linbit1 kernel: [506533.990290] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:02 linbit1 kernel: [506533.990343] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: sock_recvmsg returned -4
Mar 13 21:09:02 linbit1 kernel: [506533.990389] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Terminating sender thread
Mar 13 21:09:02 linbit1 kernel: [506533.990545] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Starting sender thread (from drbd_r_pvc-d2f1 [2454632])
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.933 [WARNING][2488362] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.934 [INFO][2488362] k8s.go 576: Cleaning up netns ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.934 [INFO][2488362] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" iface="eth0" netns=""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.934 [INFO][2488362] k8s.go 583: Releasing IP address(es) ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.934 [INFO][2488362] utils.go 196: Calico CNI releasing IP address ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.976 [INFO][2488395] ipam_plugin.go 411: Releasing address using handleID ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:02Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.998 [WARNING][2488395] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:02.998 [INFO][2488395] ipam_plugin.go 439: Releasing address using workloadID ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.011 [INFO][2488362] k8s.go 589: Teardown processing complete. ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.015853985Z" level=info msg="TearDown network for sandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.015914145Z" level=info msg="StopPodSandbox for \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" returns successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.016503027Z" level=info msg="RemovePodSandbox for \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.016568627Z" level=info msg="Forcibly stopping sandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\""
Mar 13 21:09:03 linbit1 kernel: [506534.040973] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Preparing remote state change 1039933202
Mar 13 21:09:03 linbit1 kernel: [506534.063325] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Committing remote state change 1039933202 (primary_nodes=0)
Mar 13 21:09:03 linbit1 kernel: [506534.063342] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:09:03 linbit1 kernel: [506534.063599] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:09:03 linbit1 kernel: [506534.065731] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Connection closed
Mar 13 21:09:03 linbit1 kernel: [506534.065751] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:03 linbit1 kernel: [506534.065756] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Terminating receiver thread
Mar 13 21:09:03 linbit1 kernel: [506534.065886] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7 linbit3: Terminating sender thread
Mar 13 21:09:03 linbit1 kernel: [506534.101593] drbd pvc-d2f1933e-7894-43b8-a470-d1ce0b87daf7: Terminating worker thread
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.091 [WARNING][2488433] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.092 [INFO][2488433] k8s.go 576: Cleaning up netns ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.092 [INFO][2488433] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" iface="eth0" netns=""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.092 [INFO][2488433] k8s.go 583: Releasing IP address(es) ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.092 [INFO][2488433] utils.go 196: Calico CNI releasing IP address ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.122 [INFO][2488454] ipam_plugin.go 411: Releasing address using handleID ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.144 [WARNING][2488454] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.144 [INFO][2488454] ipam_plugin.go 439: Releasing address using workloadID ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" HandleID="k8s-pod-network.41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba" Workload="linbit1-k8s-fio--bench--r2--n0--3--bh94n-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.156 [INFO][2488433] k8s.go 589: Teardown processing complete. ContainerID="41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.159940010Z" level=info msg="TearDown network for sandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.161949657Z" level=info msg="RemovePodSandbox \"41319c482e2842781699552c285bf74efea84025aa59f1f1733009edfd2b02ba\" returns successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.162743219Z" level=info msg="StopPodSandbox for \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\""
Mar 13 21:09:03 linbit1 kernel: [506534.301644] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Handshake to peer 0 successful: Agreed network protocol version 121
Mar 13 21:09:03 linbit1 kernel: [506534.301654] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:09:03 linbit1 kernel: [506534.301812] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Peer authenticated using 20 bytes HMAC
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.234 [WARNING][2488494] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.235 [INFO][2488494] k8s.go 576: Cleaning up netns ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.235 [INFO][2488494] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" iface="eth0" netns=""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.235 [INFO][2488494] k8s.go 583: Releasing IP address(es) ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.235 [INFO][2488494] utils.go 196: Calico CNI releasing IP address ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.263 [INFO][2488512] ipam_plugin.go 411: Releasing address using handleID ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.284 [WARNING][2488512] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.284 [INFO][2488512] ipam_plugin.go 439: Releasing address using workloadID ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.295 [INFO][2488494] k8s.go 589: Teardown processing complete. ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.299083340Z" level=info msg="TearDown network for sandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.299126300Z" level=info msg="StopPodSandbox for \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" returns successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.299830902Z" level=info msg="RemovePodSandbox for \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.299929302Z" level=info msg="Forcibly stopping sandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\""
Mar 13 21:09:03 linbit1 kernel: [506534.388442] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Preparing remote state change 4073505270
Mar 13 21:09:03 linbit1 kernel: [506534.388697] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Committing remote state change 4073505270 (primary_nodes=0)
Mar 13 21:09:03 linbit1 kernel: [506534.388707] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.369 [WARNING][2488551] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.370 [INFO][2488551] k8s.go 576: Cleaning up netns ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.370 [INFO][2488551] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" iface="eth0" netns=""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.370 [INFO][2488551] k8s.go 583: Releasing IP address(es) ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.370 [INFO][2488551] utils.go 196: Calico CNI releasing IP address ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.398 [INFO][2488576] ipam_plugin.go 411: Releasing address using handleID ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.417 [WARNING][2488576] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.418 [INFO][2488576] ipam_plugin.go 439: Releasing address using workloadID ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" HandleID="k8s-pod-network.1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa" Workload="linbit1-k8s-fio--bench--r2--n4--4--mmrrc-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.426 [INFO][2488551] k8s.go 589: Teardown processing complete. ContainerID="1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.430858205Z" level=info msg="TearDown network for sandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.432886812Z" level=info msg="RemovePodSandbox \"1bec8aad5aab75f68df11e4f60c72867eeff5a3d36c4d9525fe5efa7707905fa\" returns successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.433418494Z" level=info msg="StopPodSandbox for \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.508 [WARNING][2488612] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.508 [INFO][2488612] k8s.go 576: Cleaning up netns ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.508 [INFO][2488612] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" iface="eth0" netns=""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.509 [INFO][2488612] k8s.go 583: Releasing IP address(es) ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.509 [INFO][2488612] utils.go 196: Calico CNI releasing IP address ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.541 [INFO][2488634] ipam_plugin.go 411: Releasing address using handleID ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.563 [WARNING][2488634] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.563 [INFO][2488634] ipam_plugin.go 439: Releasing address using workloadID ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.573 [INFO][2488612] k8s.go 589: Teardown processing complete. ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.577510839Z" level=info msg="TearDown network for sandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.577567479Z" level=info msg="StopPodSandbox for \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" returns successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.578154081Z" level=info msg="RemovePodSandbox for \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.578215721Z" level=info msg="Forcibly stopping sandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.652 [WARNING][2488672] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.653 [INFO][2488672] k8s.go 576: Cleaning up netns ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.653 [INFO][2488672] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" iface="eth0" netns=""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.653 [INFO][2488672] k8s.go 583: Releasing IP address(es) ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.653 [INFO][2488672] utils.go 196: Calico CNI releasing IP address ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.683 [INFO][2488694] ipam_plugin.go 411: Releasing address using handleID ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.706 [WARNING][2488694] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.706 [INFO][2488694] ipam_plugin.go 439: Releasing address using workloadID ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" HandleID="k8s-pod-network.5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0" Workload="linbit1-k8s-fio--bench--r2--n0--2--hmskv-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.716 [INFO][2488672] k8s.go 589: Teardown processing complete. ContainerID="5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.720765782Z" level=info msg="TearDown network for sandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.723068029Z" level=info msg="RemovePodSandbox \"5218271c67c60398d431722b4a6c89b7d96b8dcd845050fa19b8843e46dd5cc0\" returns successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.723723831Z" level=info msg="StopPodSandbox for \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.794 [WARNING][2488729] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.794 [INFO][2488729] k8s.go 576: Cleaning up netns ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.794 [INFO][2488729] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" iface="eth0" netns=""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.794 [INFO][2488729] k8s.go 583: Releasing IP address(es) ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.794 [INFO][2488729] utils.go 196: Calico CNI releasing IP address ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.827 [INFO][2488745] ipam_plugin.go 411: Releasing address using handleID ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.845 [WARNING][2488745] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.846 [INFO][2488745] ipam_plugin.go 439: Releasing address using workloadID ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.854 [INFO][2488729] k8s.go 589: Teardown processing complete. ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.858366266Z" level=info msg="TearDown network for sandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.858455347Z" level=info msg="StopPodSandbox for \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" returns successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.858990908Z" level=info msg="RemovePodSandbox for \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.859057589Z" level=info msg="Forcibly stopping sandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.929 [WARNING][2488783] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.929 [INFO][2488783] k8s.go 576: Cleaning up netns ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.929 [INFO][2488783] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" iface="eth0" netns=""
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.929 [INFO][2488783] k8s.go 583: Releasing IP address(es) ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.929 [INFO][2488783] utils.go 196: Calico CNI releasing IP address ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.960 [INFO][2488804] ipam_plugin.go 411: Releasing address using handleID ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.981 [WARNING][2488804] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.981 [INFO][2488804] ipam_plugin.go 439: Releasing address using workloadID ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" HandleID="k8s-pod-network.c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5" Workload="linbit1-k8s-fio--bench--r2--n7--2--bnfd5-eth0"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:03.992 [INFO][2488783] k8s.go 589: Teardown processing complete. ContainerID="c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.995611510Z" level=info msg="TearDown network for sandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.998187358Z" level=info msg="RemovePodSandbox \"c95050c284aee8bee74d0b77d3a45de32bdc863e10a4e7ce2e23aeced976aee5\" returns successfully"
Mar 13 21:09:03 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:03.998717480Z" level=info msg="StopPodSandbox for \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.068 [WARNING][2488842] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.069 [INFO][2488842] k8s.go 576: Cleaning up netns ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.069 [INFO][2488842] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" iface="eth0" netns=""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.069 [INFO][2488842] k8s.go 583: Releasing IP address(es) ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.069 [INFO][2488842] utils.go 196: Calico CNI releasing IP address ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.099 [INFO][2488867] ipam_plugin.go 411: Releasing address using handleID ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.120 [WARNING][2488867] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.120 [INFO][2488867] ipam_plugin.go 439: Releasing address using workloadID ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.130 [INFO][2488842] k8s.go 589: Teardown processing complete. ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.134374718Z" level=info msg="TearDown network for sandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.134429998Z" level=info msg="StopPodSandbox for \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" returns successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.135006080Z" level=info msg="RemovePodSandbox for \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.135062400Z" level=info msg="Forcibly stopping sandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\""
Mar 13 21:09:04 linbit1 kernel: [506535.231802] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:09:04 linbit1 kernel: [506535.231853] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Terminating sender thread
Mar 13 21:09:04 linbit1 kernel: [506535.231871] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Starting sender thread (from drbd_r_pvc-4007 [2487307])
Mar 13 21:09:04 linbit1 kernel: [506535.232016] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Connection closed
Mar 13 21:09:04 linbit1 kernel: [506535.232023] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:04 linbit1 kernel: [506535.232025] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Terminating receiver thread
Mar 13 21:09:04 linbit1 kernel: [506535.232084] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit3: Terminating sender thread
Mar 13 21:09:04 linbit1 kernel: [506535.242683] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:04 linbit1 kernel: [506535.242712] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: sock_recvmsg returned -4
Mar 13 21:09:04 linbit1 kernel: [506535.242771] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Terminating sender thread
Mar 13 21:09:04 linbit1 kernel: [506535.242801] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Starting sender thread (from drbd_r_pvc-7803 [2451910])
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.206 [WARNING][2488907] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.206 [INFO][2488907] k8s.go 576: Cleaning up netns ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.206 [INFO][2488907] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" iface="eth0" netns=""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.206 [INFO][2488907] k8s.go 583: Releasing IP address(es) ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.206 [INFO][2488907] utils.go 196: Calico CNI releasing IP address ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.235 [INFO][2488930] ipam_plugin.go 411: Releasing address using handleID ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.255 [WARNING][2488930] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.255 [INFO][2488930] ipam_plugin.go 439: Releasing address using workloadID ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" HandleID="k8s-pod-network.e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61" Workload="linbit1-k8s-fio--bench--r2--n5--3--5zdjt-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.264 [INFO][2488907] k8s.go 589: Teardown processing complete. ContainerID="e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.267767869Z" level=info msg="TearDown network for sandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.276462697Z" level=info msg="RemovePodSandbox \"e5f36e547be6bd13aaa329b1e0cbb6ac3ba011a0dd3f664e0f8b7d7f51aeea61\" returns successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.276834658Z" level=info msg="StopPodSandbox for \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\""
Mar 13 21:09:04 linbit1 kernel: [506535.306627] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Connection closed
Mar 13 21:09:04 linbit1 kernel: [506535.306643] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:04 linbit1 kernel: [506535.306648] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Terminating receiver thread
Mar 13 21:09:04 linbit1 kernel: [506535.306785] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef linbit3: Terminating sender thread
Mar 13 21:09:04 linbit1 kernel: [506535.309847] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Preparing cluster-wide state change 3578855259 (1->-1 7680/1024)
Mar 13 21:09:04 linbit1 kernel: [506535.310137] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: State change 3578855259: primary_nodes=0, weak_nodes=0
Mar 13 21:09:04 linbit1 kernel: [506535.310142] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Committing cluster-wide state change 3578855259 (0ms)
Mar 13 21:09:04 linbit1 kernel: [506535.310169] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: disk( UpToDate -> Detaching )
Mar 13 21:09:04 linbit1 kernel: [506535.310268] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: disk( Detaching -> Diskless )
Mar 13 21:09:04 linbit1 kernel: [506535.311119] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: drbd_bm_resize called with capacity == 0
Mar 13 21:09:04 linbit1 kernel: [506535.345579] drbd pvc-7803048b-5eec-4186-a436-1c249ea7a7ef: Terminating worker thread
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.349 [WARNING][2488988] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.349 [INFO][2488988] k8s.go 576: Cleaning up netns ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.349 [INFO][2488988] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" iface="eth0" netns=""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.349 [INFO][2488988] k8s.go 583: Releasing IP address(es) ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.349 [INFO][2488988] utils.go 196: Calico CNI releasing IP address ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.381 [INFO][2489018] ipam_plugin.go 411: Releasing address using handleID ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.402 [WARNING][2489018] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.402 [INFO][2489018] ipam_plugin.go 439: Releasing address using workloadID ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.411 [INFO][2488988] k8s.go 589: Teardown processing complete. ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.415719787Z" level=info msg="TearDown network for sandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.415787827Z" level=info msg="StopPodSandbox for \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" returns successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.416330989Z" level=info msg="RemovePodSandbox for \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.416394949Z" level=info msg="Forcibly stopping sandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.488 [WARNING][2489057] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.488 [INFO][2489057] k8s.go 576: Cleaning up netns ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.488 [INFO][2489057] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" iface="eth0" netns=""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.488 [INFO][2489057] k8s.go 583: Releasing IP address(es) ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.488 [INFO][2489057] utils.go 196: Calico CNI releasing IP address ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.524 [INFO][2489083] ipam_plugin.go 411: Releasing address using handleID ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.543 [WARNING][2489083] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.543 [INFO][2489083] ipam_plugin.go 439: Releasing address using workloadID ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" HandleID="k8s-pod-network.470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc" Workload="linbit1-k8s-fio--bench--r2--n0--0--pml7f-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.551 [INFO][2489057] k8s.go 589: Teardown processing complete. ContainerID="470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.555941520Z" level=info msg="TearDown network for sandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.558235087Z" level=info msg="RemovePodSandbox \"470f28bc567205babc2d32030ab211e3bbc2cd6c8b722b9306a2a7e7226909cc\" returns successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.558776169Z" level=info msg="StopPodSandbox for \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\""
Mar 13 21:09:04 linbit1 kernel: [506535.644934] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:04 linbit1 kernel: [506535.657102] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: State change failed: State change was refused by peer node
Mar 13 21:09:04 linbit1 kernel: [506535.667788] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:04 linbit1 kernel: [506535.667839] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:04 linbit1 kernel: [506535.679999] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: State change failed: State change was refused by peer node
Mar 13 21:09:04 linbit1 kernel: [506535.690683] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021: Failed: quorum( yes -> no )
Mar 13 21:09:04 linbit1 kernel: [506535.690687] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03/0 drbd1021 linbit2: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.639 [WARNING][2489119] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.639 [INFO][2489119] k8s.go 576: Cleaning up netns ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.639 [INFO][2489119] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" iface="eth0" netns=""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.639 [INFO][2489119] k8s.go 583: Releasing IP address(es) ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.639 [INFO][2489119] utils.go 196: Calico CNI releasing IP address ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.674 [INFO][2489148] ipam_plugin.go 411: Releasing address using handleID ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.697 [WARNING][2489148] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.698 [INFO][2489148] ipam_plugin.go 439: Releasing address using workloadID ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.709 [INFO][2489119] k8s.go 589: Teardown processing complete. ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.712989747Z" level=info msg="TearDown network for sandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.713038827Z" level=info msg="StopPodSandbox for \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" returns successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.713568869Z" level=info msg="RemovePodSandbox for \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.713615669Z" level=info msg="Forcibly stopping sandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\""
Mar 13 21:09:04 linbit1 kernel: [506535.724771] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Preparing cluster-wide state change 496728353 (0->-1 7680/1024)
Mar 13 21:09:04 linbit1 kernel: [506535.725012] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: State change 496728353: primary_nodes=0, weak_nodes=0
Mar 13 21:09:04 linbit1 kernel: [506535.725017] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Committing cluster-wide state change 496728353 (0ms)
Mar 13 21:09:04 linbit1 kernel: [506535.725042] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: disk( UpToDate -> Detaching )
Mar 13 21:09:04 linbit1 kernel: [506535.725169] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:09:04 linbit1 kernel: [506535.725180] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: disk( Detaching -> Diskless )
Mar 13 21:09:04 linbit1 kernel: [506535.726145] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: drbd_bm_resize called with capacity == 0
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.781 [WARNING][2489194] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.781 [INFO][2489194] k8s.go 576: Cleaning up netns ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.781 [INFO][2489194] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" iface="eth0" netns=""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.781 [INFO][2489194] k8s.go 583: Releasing IP address(es) ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.781 [INFO][2489194] utils.go 196: Calico CNI releasing IP address ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.812 [INFO][2489217] ipam_plugin.go 411: Releasing address using handleID ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.832 [WARNING][2489217] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.832 [INFO][2489217] ipam_plugin.go 439: Releasing address using workloadID ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" HandleID="k8s-pod-network.e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc" Workload="linbit1-k8s-fio--bench--r2--n8--4--vfll8-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.841 [INFO][2489194] k8s.go 589: Teardown processing complete. ContainerID="e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.844893773Z" level=info msg="TearDown network for sandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.846888779Z" level=info msg="RemovePodSandbox \"e06c889c956de73cd61d67ab056643ccf15216fc6cea7a20be29645e42270dfc\" returns successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.847384381Z" level=info msg="StopPodSandbox for \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.920 [WARNING][2489252] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.920 [INFO][2489252] k8s.go 576: Cleaning up netns ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.920 [INFO][2489252] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" iface="eth0" netns=""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.920 [INFO][2489252] k8s.go 583: Releasing IP address(es) ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.921 [INFO][2489252] utils.go 196: Calico CNI releasing IP address ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.957 [INFO][2489282] ipam_plugin.go 411: Releasing address using handleID ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.975 [WARNING][2489282] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.975 [INFO][2489282] ipam_plugin.go 439: Releasing address using workloadID ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:04.985 [INFO][2489252] k8s.go 589: Teardown processing complete. ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.989635160Z" level=info msg="TearDown network for sandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.989692881Z" level=info msg="StopPodSandbox for \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" returns successfully"
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.990316243Z" level=info msg="RemovePodSandbox for \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\""
Mar 13 21:09:04 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:04.990366803Z" level=info msg="Forcibly stopping sandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\""
Mar 13 21:09:05 linbit1 kernel: [506536.068786] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:05 linbit1 kernel: [506536.080953] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: State change failed: State change was refused by peer node
Mar 13 21:09:05 linbit1 kernel: [506536.091638] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: Failed: quorum( yes -> no )
Mar 13 21:09:05 linbit1 kernel: [506536.091643] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:05 linbit1 kernel: [506536.091717] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:05 linbit1 kernel: [506536.103878] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: State change failed: State change was refused by peer node
Mar 13 21:09:05 linbit1 kernel: [506536.114560] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018: Failed: quorum( yes -> no )
Mar 13 21:09:05 linbit1 kernel: [506536.114564] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa/0 drbd1018 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.065 [WARNING][2489323] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.065 [INFO][2489323] k8s.go 576: Cleaning up netns ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.065 [INFO][2489323] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" iface="eth0" netns=""
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.065 [INFO][2489323] k8s.go 583: Releasing IP address(es) ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.066 [INFO][2489323] utils.go 196: Calico CNI releasing IP address ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.097 [INFO][2489349] ipam_plugin.go 411: Releasing address using handleID ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.115 [WARNING][2489349] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.115 [INFO][2489349] ipam_plugin.go 439: Releasing address using workloadID ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" HandleID="k8s-pod-network.f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557" Workload="linbit1-k8s-fio--bench--r2--n7--4--ckbx6-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.125 [INFO][2489323] k8s.go 589: Teardown processing complete. ContainerID="f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.129600372Z" level=info msg="TearDown network for sandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" successfully"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.131827820Z" level=info msg="RemovePodSandbox \"f10f052257c651027c0dfe7660a8896fd63e114c9f2fa12d7be075a1b3ff4557\" returns successfully"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.132470622Z" level=info msg="StopPodSandbox for \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\""
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.207 [WARNING][2489390] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.208 [INFO][2489390] k8s.go 576: Cleaning up netns ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.208 [INFO][2489390] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" iface="eth0" netns=""
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.208 [INFO][2489390] k8s.go 583: Releasing IP address(es) ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.208 [INFO][2489390] utils.go 196: Calico CNI releasing IP address ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.251 [INFO][2489411] ipam_plugin.go 411: Releasing address using handleID ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.275 [WARNING][2489411] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.275 [INFO][2489411] ipam_plugin.go 439: Releasing address using workloadID ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.284 [INFO][2489390] k8s.go 589: Teardown processing complete. ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.287108601Z" level=info msg="TearDown network for sandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" successfully"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.287178361Z" level=info msg="StopPodSandbox for \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" returns successfully"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.287605923Z" level=info msg="RemovePodSandbox for \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\""
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.287676203Z" level=info msg="Forcibly stopping sandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\""
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.360 [WARNING][2489450] k8s.go 534: WorkloadEndpoint does not exist in the datastore, moving forward with the clean up ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" WorkloadEndpoint="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.360 [INFO][2489450] k8s.go 576: Cleaning up netns ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.360 [INFO][2489450] dataplane_linux.go 500: CleanUpNamespace called with no netns name, ignoring. ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" iface="eth0" netns=""
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.360 [INFO][2489450] k8s.go 583: Releasing IP address(es) ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.361 [INFO][2489450] utils.go 196: Calico CNI releasing IP address ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.390 [INFO][2489471] ipam_plugin.go 411: Releasing address using handleID ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="About to acquire host-wide IPAM lock." source="ipam_plugin.go:352"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="Acquired host-wide IPAM lock." source="ipam_plugin.go:367"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.417 [WARNING][2489471] ipam_plugin.go 428: Asked to release address but it doesn't exist. Ignoring ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.417 [INFO][2489471] ipam_plugin.go 439: Releasing address using workloadID ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" HandleID="k8s-pod-network.585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba" Workload="linbit1-k8s-fio--bench--r2--n8--3--8hwhw-eth0"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05Z" level=info msg="Released host-wide IPAM lock." source="ipam_plugin.go:373"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: 2023-03-13 21:09:05.427 [INFO][2489450] k8s.go 589: Teardown processing complete. ContainerID="585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.430594745Z" level=info msg="TearDown network for sandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" successfully"
Mar 13 21:09:05 linbit1 microk8s.daemon-containerd[2400]: time="2023-03-13T21:09:05.433255153Z" level=info msg="RemovePodSandbox \"585072ee15c2992f0f7d62dbe949363a9d52db206191218d3956d15df326e5ba\" returns successfully"
Mar 13 21:09:06 linbit1 kernel: [506537.048413] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: sock was shut down by peer
Mar 13 21:09:06 linbit1 kernel: [506537.048430] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:09:06 linbit1 kernel: [506537.048510] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Terminating sender thread
Mar 13 21:09:06 linbit1 kernel: [506537.048560] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Starting sender thread (from drbd_r_pvc-0845 [2454250])
Mar 13 21:09:06 linbit1 kernel: [506537.089707] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Connection closed
Mar 13 21:09:06 linbit1 kernel: [506537.089719] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:09:06 linbit1 kernel: [506537.089729] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Restarting receiver thread
Mar 13 21:09:06 linbit1 kernel: [506537.089735] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:06 linbit1 kernel: [506537.160575] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Preparing remote state change 2895687169
Mar 13 21:09:06 linbit1 kernel: [506537.160825] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Committing remote state change 2895687169 (primary_nodes=0)
Mar 13 21:09:06 linbit1 kernel: [506537.160927] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:09:06 linbit1 kernel: [506537.161021] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:09:06 linbit1 kernel: [506537.791269] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:09:06 linbit1 kernel: [506537.791367] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Terminating sender thread
Mar 13 21:09:06 linbit1 kernel: [506537.791480] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Starting sender thread (from drbd_r_pvc-0845 [2454250])
Mar 13 21:09:06 linbit1 kernel: [506537.791646] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Connection closed
Mar 13 21:09:06 linbit1 kernel: [506537.791660] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:06 linbit1 kernel: [506537.791665] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Terminating receiver thread
Mar 13 21:09:06 linbit1 kernel: [506537.791766] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Terminating sender thread
Mar 13 21:09:06 linbit1 kernel: [506537.801594] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Starting sender thread (from drbdsetup [2489541])
Mar 13 21:09:06 linbit1 kernel: [506537.804370] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:09:06 linbit1 kernel: [506537.804407] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Starting receiver thread (from drbd_w_pvc-0845 [2453724])
Mar 13 21:09:06 linbit1 kernel: [506537.804513] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:06 linbit1 kernel: [506537.809869] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:09:06 linbit1 kernel: [506537.809946] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Terminating sender thread
Mar 13 21:09:06 linbit1 kernel: [506537.809967] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Starting sender thread (from drbd_r_pvc-329d [2487906])
Mar 13 21:09:06 linbit1 kernel: [506537.810178] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Connection closed
Mar 13 21:09:06 linbit1 kernel: [506537.810185] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:06 linbit1 kernel: [506537.810186] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Terminating receiver thread
Mar 13 21:09:06 linbit1 kernel: [506537.810278] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit3: Terminating sender thread
Mar 13 21:09:06 linbit1 kernel: [506537.826960] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:09:06 linbit1 kernel: [506537.827042] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Terminating sender thread
Mar 13 21:09:06 linbit1 kernel: [506537.827059] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Starting sender thread (from drbd_r_pvc-b01e [2488380])
Mar 13 21:09:06 linbit1 kernel: [506537.827283] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Connection closed
Mar 13 21:09:06 linbit1 kernel: [506537.827296] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:06 linbit1 kernel: [506537.827303] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Terminating receiver thread
Mar 13 21:09:06 linbit1 kernel: [506537.827388] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit2: Terminating sender thread
Mar 13 21:09:06 linbit1 kernel: [506537.945280] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Preparing remote state change 2217607130
Mar 13 21:09:06 linbit1 kernel: [506537.945480] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Committing remote state change 2217607130 (primary_nodes=0)
Mar 13 21:09:06 linbit1 kernel: [506537.945494] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:09:06 linbit1 kernel: [506537.945773] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:09:07 linbit1 kernel: [506538.014385] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:07 linbit1 kernel: [506538.014470] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Terminating sender thread
Mar 13 21:09:07 linbit1 kernel: [506538.014503] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Starting sender thread (from drbd_r_pvc-4007 [2448909])
Mar 13 21:09:07 linbit1 kernel: [506538.049792] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Connection closed
Mar 13 21:09:07 linbit1 kernel: [506538.049807] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:07 linbit1 kernel: [506538.049812] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Terminating receiver thread
Mar 13 21:09:07 linbit1 kernel: [506538.049926] drbd pvc-400778d2-ad89-410f-90fd-625de046658b linbit2: Terminating sender thread
Mar 13 21:09:07 linbit1 kernel: [506538.053708] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 3001433127 (0->-1 7680/1024)
Mar 13 21:09:07 linbit1 kernel: [506538.054006] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change 3001433127: primary_nodes=0, weak_nodes=0
Mar 13 21:09:07 linbit1 kernel: [506538.054012] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 3001433127 (0ms)
Mar 13 21:09:07 linbit1 kernel: [506538.054044] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: disk( UpToDate -> Detaching )
Mar 13 21:09:07 linbit1 kernel: [506538.054168] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:09:07 linbit1 kernel: [506538.054176] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: disk( Detaching -> Diskless )
Mar 13 21:09:07 linbit1 kernel: [506538.055230] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: drbd_bm_resize called with capacity == 0
Mar 13 21:09:07 linbit1 kernel: [506538.085564] drbd pvc-400778d2-ad89-410f-90fd-625de046658b: Terminating worker thread
Mar 13 21:09:07 linbit1 kernel: [506538.337867] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:07 linbit1 kernel: [506538.350033] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change failed: State change was refused by peer node
Mar 13 21:09:07 linbit1 kernel: [506538.360718] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: Failed: quorum( yes -> no )
Mar 13 21:09:07 linbit1 kernel: [506538.360724] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:07 linbit1 kernel: [506538.360759] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:07 linbit1 kernel: [506538.372919] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change failed: State change was refused by peer node
Mar 13 21:09:07 linbit1 kernel: [506538.383603] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022: Failed: quorum( yes -> no )
Mar 13 21:09:07 linbit1 kernel: [506538.383607] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b/0 drbd1022 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:08 linbit1 kernel: [506539.324406] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: sock was shut down by peer
Mar 13 21:09:08 linbit1 kernel: [506539.324422] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:09:08 linbit1 kernel: [506539.324506] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Terminating sender thread
Mar 13 21:09:08 linbit1 kernel: [506539.324553] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Starting sender thread (from drbd_r_pvc-a79c [2450592])
Mar 13 21:09:08 linbit1 kernel: [506539.381634] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Connection closed
Mar 13 21:09:08 linbit1 kernel: [506539.381646] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:09:08 linbit1 kernel: [506539.381657] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Restarting receiver thread
Mar 13 21:09:08 linbit1 kernel: [506539.381664] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:08 linbit1 kernel: [506539.880417] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Preparing remote state change 3015195755
Mar 13 21:09:08 linbit1 kernel: [506539.880661] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Committing remote state change 3015195755 (primary_nodes=0)
Mar 13 21:09:09 linbit1 kernel: [506540.328787] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:09:09 linbit1 kernel: [506540.328880] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.328931] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Starting sender thread (from drbd_r_pvc-a79c [2450592])
Mar 13 21:09:09 linbit1 kernel: [506540.329083] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Connection closed
Mar 13 21:09:09 linbit1 kernel: [506540.329095] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:09 linbit1 kernel: [506540.329099] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Terminating receiver thread
Mar 13 21:09:09 linbit1 kernel: [506540.329158] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.336580] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Starting sender thread (from drbdsetup [2489677])
Mar 13 21:09:09 linbit1 kernel: [506540.339687] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( StandAlone -> Unconnected )
Mar 13 21:09:09 linbit1 kernel: [506540.339722] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Starting receiver thread (from drbd_w_pvc-a79c [2450570])
Mar 13 21:09:09 linbit1 kernel: [506540.339838] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:09 linbit1 kernel: [506540.346173] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:09 linbit1 kernel: [506540.346251] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.346281] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Starting sender thread (from drbd_r_pvc-b7be [2449092])
Mar 13 21:09:09 linbit1 kernel: [506540.405597] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Connection closed
Mar 13 21:09:09 linbit1 kernel: [506540.405615] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:09 linbit1 kernel: [506540.405620] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Terminating receiver thread
Mar 13 21:09:09 linbit1 kernel: [506540.405688] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.415513] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Starting sender thread (from drbdsetup [2489696])
Mar 13 21:09:09 linbit1 kernel: [506540.418292] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:09:09 linbit1 kernel: [506540.418326] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Starting receiver thread (from drbd_w_pvc-b7be [2447224])
Mar 13 21:09:09 linbit1 kernel: [506540.418456] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( Unconnected -> Connecting )
Mar 13 21:09:09 linbit1 kernel: [506540.518592] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:09:09 linbit1 kernel: [506540.518655] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.518689] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Starting sender thread (from drbd_r_pvc-0845 [2489545])
Mar 13 21:09:09 linbit1 kernel: [506540.518916] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Connection closed
Mar 13 21:09:09 linbit1 kernel: [506540.518928] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:09 linbit1 kernel: [506540.518935] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Terminating receiver thread
Mar 13 21:09:09 linbit1 kernel: [506540.519007] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit3: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.531916] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:09 linbit1 kernel: [506540.532009] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.532045] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Starting sender thread (from drbd_r_pvc-329d [2454393])
Mar 13 21:09:09 linbit1 kernel: [506540.572410] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: sock was shut down by peer
Mar 13 21:09:09 linbit1 kernel: [506540.572424] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:09:09 linbit1 kernel: [506540.572485] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.572528] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Starting sender thread (from drbd_r_pvc-b01e [2449064])
Mar 13 21:09:09 linbit1 kernel: [506540.581811] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Connection closed
Mar 13 21:09:09 linbit1 kernel: [506540.581829] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:09 linbit1 kernel: [506540.581834] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Terminating receiver thread
Mar 13 21:09:09 linbit1 kernel: [506540.581933] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be linbit2: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.589423] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:09 linbit1 kernel: [506540.589488] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.589526] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Starting sender thread (from drbd_r_pvc-9428 [2451976])
Mar 13 21:09:09 linbit1 kernel: [506540.605571] drbd pvc-329d1432-978f-452c-b6c0-ddaf5f91f9be: Terminating worker thread
Mar 13 21:09:09 linbit1 kernel: [506540.625775] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Connection closed
Mar 13 21:09:09 linbit1 kernel: [506540.625786] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: conn( BrokenPipe -> Unconnected )
Mar 13 21:09:09 linbit1 kernel: [506540.625796] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Restarting receiver thread
Mar 13 21:09:09 linbit1 kernel: [506540.625802] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: conn( Unconnected -> Connecting )
Mar 13 21:09:09 linbit1 kernel: [506540.657613] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Connection closed
Mar 13 21:09:09 linbit1 kernel: [506540.657629] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:09 linbit1 kernel: [506540.657634] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Terminating receiver thread
Mar 13 21:09:09 linbit1 kernel: [506540.657784] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.667182] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Starting sender thread (from drbdsetup [2489735])
Mar 13 21:09:09 linbit1 kernel: [506540.670220] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:09:09 linbit1 kernel: [506540.670254] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Starting receiver thread (from drbd_w_pvc-9428 [2450001])
Mar 13 21:09:09 linbit1 kernel: [506540.670357] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Unconnected -> Connecting )
Mar 13 21:09:09 linbit1 kernel: [506540.676626] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:09:09 linbit1 kernel: [506540.676756] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Terminating sender thread
Mar 13 21:09:09 linbit1 kernel: [506540.676770] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Starting sender thread (from drbd_r_pvc-b01e [2449064])
Mar 13 21:09:09 linbit1 kernel: [506540.737829] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Connection closed
Mar 13 21:09:09 linbit1 kernel: [506540.737847] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:10 linbit1 kernel: [506541.024424] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Preparing remote state change 2391625109
Mar 13 21:09:10 linbit1 kernel: [506541.024651] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Committing remote state change 2391625109 (primary_nodes=0)
Mar 13 21:09:10 linbit1 kernel: [506541.228736] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Handshake to peer 2 successful: Agreed network protocol version 121
Mar 13 21:09:10 linbit1 kernel: [506541.228746] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Feature flags enabled on protocol level: 0x1f TRIM THIN_RESYNC WRITE_SAME WRITE_ZEROES RESYNC_DAGTAG
Mar 13 21:09:10 linbit1 kernel: [506541.228878] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Peer authenticated using 20 bytes HMAC
Mar 13 21:09:10 linbit1 kernel: [506541.373548] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Preparing cluster-wide state change 1534972129 (0->2 499/146)
Mar 13 21:09:10 linbit1 kernel: [506541.373793] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: State change 1534972129: primary_nodes=0, weak_nodes=0
Mar 13 21:09:10 linbit1 kernel: [506541.373800] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Committing cluster-wide state change 1534972129 (0ms)
Mar 13 21:09:10 linbit1 kernel: [506541.373831] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Connecting -> Connected ) peer( Unknown -> Secondary )
Mar 13 21:09:10 linbit1 kernel: [506541.757528] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: State change failed: Need a connection to start verify or resync
Mar 13 21:09:10 linbit1 kernel: [506541.768741] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Failed: conn( StandAlone -> Connecting )
Mar 13 21:09:10 linbit1 kernel: [506541.768762] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Terminating receiver thread
Mar 13 21:09:10 linbit1 kernel: [506541.768827] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588 linbit3: Terminating sender thread
Mar 13 21:09:10 linbit1 kernel: [506541.813550] drbd pvc-b01e907a-4035-4022-9701-ff7a07ac1588: Terminating worker thread
Mar 13 21:09:10 linbit1 kernel: [506541.969033] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Preparing remote state change 1697394754
Mar 13 21:09:10 linbit1 kernel: [506541.969265] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Committing remote state change 1697394754 (primary_nodes=0)
Mar 13 21:09:10 linbit1 kernel: [506541.969279] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: pdsk( UpToDate -> Detaching )
Mar 13 21:09:10 linbit1 kernel: [506541.969497] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: pdsk( Detaching -> Diskless )
Mar 13 21:09:11 linbit1 kernel: [506542.070074] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Preparing cluster-wide state change 2485003983 (0->-1 7680/1024)
Mar 13 21:09:11 linbit1 kernel: [506542.070364] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: State change 2485003983: primary_nodes=0, weak_nodes=0
Mar 13 21:09:11 linbit1 kernel: [506542.070369] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Committing cluster-wide state change 2485003983 (0ms)
Mar 13 21:09:11 linbit1 kernel: [506542.070399] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: disk( UpToDate -> Detaching )
Mar 13 21:09:11 linbit1 kernel: [506542.070537] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: disk( Detaching -> Diskless )
Mar 13 21:09:11 linbit1 kernel: [506542.071613] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: drbd_bm_resize called with capacity == 0
Mar 13 21:09:11 linbit1 kernel: [506542.084688] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: sock was shut down by peer
Mar 13 21:09:11 linbit1 kernel: [506542.084700] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:09:11 linbit1 kernel: [506542.084807] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Terminating sender thread
Mar 13 21:09:11 linbit1 kernel: [506542.084825] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Starting sender thread (from drbd_r_pvc-9428 [2489739])
Mar 13 21:09:11 linbit1 kernel: [506542.141661] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Connection closed
Mar 13 21:09:11 linbit1 kernel: [506542.141672] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( BrokenPipe -> Unconnected )
Mar 13 21:09:11 linbit1 kernel: [506542.141681] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Restarting receiver thread
Mar 13 21:09:11 linbit1 kernel: [506542.141686] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Unconnected -> Connecting )
Mar 13 21:09:11 linbit1 kernel: [506542.384034] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:11 linbit1 kernel: [506542.396200] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: State change failed: State change was refused by peer node
Mar 13 21:09:11 linbit1 kernel: [506542.406885] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023: Failed: quorum( yes -> no )
Mar 13 21:09:11 linbit1 kernel: [506542.406891] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:11 linbit1 kernel: [506542.406932] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:11 linbit1 kernel: [506542.419092] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: State change failed: State change was refused by peer node
Mar 13 21:09:11 linbit1 kernel: [506542.429776] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22/0 drbd1023 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:11 linbit1 kernel: [506542.948929] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Preparing remote state change 4070267455
Mar 13 21:09:11 linbit1 kernel: [506542.949093] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Committing remote state change 4070267455 (primary_nodes=0)
Mar 13 21:09:11 linbit1 kernel: [506542.949107] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: pdsk( UpToDate -> Detaching )
Mar 13 21:09:11 linbit1 kernel: [506542.949347] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: pdsk( Detaching -> Diskless )
Mar 13 21:09:12 linbit1 kernel: [506543.252011] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025: Would lose quorum, but using tiebreaker logic to keep
Mar 13 21:09:12 linbit1 kernel: [506543.252024] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:12 linbit1 kernel: [506543.264186] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67: State change failed: State change was refused by peer node
Mar 13 21:09:12 linbit1 kernel: [506543.274872] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit3: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:12 linbit1 kernel: [506543.274926] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:12 linbit1 kernel: [506543.287086] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67: State change failed: State change was refused by peer node
Mar 13 21:09:12 linbit1 kernel: [506543.297769] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67/0 drbd1025 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:12 linbit1 kernel: [506543.349633] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Connecting -> Disconnecting )
Mar 13 21:09:12 linbit1 kernel: [506543.349690] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Terminating sender thread
Mar 13 21:09:12 linbit1 kernel: [506543.349712] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Starting sender thread (from drbd_r_pvc-a79c [2489681])
Mar 13 21:09:12 linbit1 kernel: [506543.349904] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Connection closed
Mar 13 21:09:12 linbit1 kernel: [506543.349916] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:12 linbit1 kernel: [506543.349922] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Terminating receiver thread
Mar 13 21:09:12 linbit1 kernel: [506543.349941] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit3: Terminating sender thread
Mar 13 21:09:12 linbit1 kernel: [506543.365529] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:09:12 linbit1 kernel: [506543.365588] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Terminating sender thread
Mar 13 21:09:12 linbit1 kernel: [506543.365609] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Starting sender thread (from drbd_r_pvc-b7be [2489700])
Mar 13 21:09:12 linbit1 kernel: [506543.365799] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Connection closed
Mar 13 21:09:12 linbit1 kernel: [506543.365810] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:12 linbit1 kernel: [506543.365814] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Terminating receiver thread
Mar 13 21:09:12 linbit1 kernel: [506543.365910] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit2: Terminating sender thread
Mar 13 21:09:12 linbit1 kernel: [506543.371319] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Preparing remote state change 3649581906
Mar 13 21:09:12 linbit1 kernel: [506543.382113] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:12 linbit1 kernel: [506543.382191] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Terminating sender thread
Mar 13 21:09:12 linbit1 kernel: [506543.382224] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Starting sender thread (from drbd_r_pvc-0845 [2454248])
Mar 13 21:09:12 linbit1 kernel: [506543.387712] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Committing remote state change 3649581906 (primary_nodes=0)
Mar 13 21:09:12 linbit1 kernel: [506543.387836] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:09:12 linbit1 kernel: [506543.387941] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:09:12 linbit1 kernel: [506543.453776] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Connection closed
Mar 13 21:09:12 linbit1 kernel: [506543.453794] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:12 linbit1 kernel: [506543.453800] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Terminating receiver thread
Mar 13 21:09:12 linbit1 kernel: [506543.453834] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7 linbit2: Terminating sender thread
Mar 13 21:09:12 linbit1 kernel: [506543.461040] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:09:12 linbit1 kernel: [506543.461118] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Terminating sender thread
Mar 13 21:09:12 linbit1 kernel: [506543.461156] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Starting sender thread (from drbd_r_pvc-9428 [2489739])
Mar 13 21:09:12 linbit1 kernel: [506543.461310] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Connection closed
Mar 13 21:09:12 linbit1 kernel: [506543.461321] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:12 linbit1 kernel: [506543.461325] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Terminating receiver thread
Mar 13 21:09:12 linbit1 kernel: [506543.461367] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit2: Terminating sender thread
Mar 13 21:09:12 linbit1 kernel: [506543.485552] drbd pvc-0845260b-d8f2-4d2d-9ee6-67932e95f1f7: Terminating worker thread
Mar 13 21:09:13 linbit1 kernel: [506544.704472] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:13 linbit1 kernel: [506544.704618] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Terminating sender thread
Mar 13 21:09:13 linbit1 kernel: [506544.704656] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Starting sender thread (from drbd_r_pvc-7a5e [2450809])
Mar 13 21:09:13 linbit1 kernel: [506544.773618] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Connection closed
Mar 13 21:09:13 linbit1 kernel: [506544.773638] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:13 linbit1 kernel: [506544.773644] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Terminating receiver thread
Mar 13 21:09:13 linbit1 kernel: [506544.773726] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit2: Terminating sender thread
Mar 13 21:09:13 linbit1 kernel: [506544.774068] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:13 linbit1 kernel: [506544.774181] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Terminating sender thread
Mar 13 21:09:13 linbit1 kernel: [506544.774210] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Starting sender thread (from drbd_r_pvc-7a5e [2450811])
Mar 13 21:09:13 linbit1 kernel: [506544.817675] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Connection closed
Mar 13 21:09:13 linbit1 kernel: [506544.817696] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:13 linbit1 kernel: [506544.817702] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Terminating receiver thread
Mar 13 21:09:13 linbit1 kernel: [506544.817831] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67 linbit3: Terminating sender thread
Mar 13 21:09:13 linbit1 kernel: [506544.849556] drbd pvc-7a5ed880-a08f-4cc8-bce3-b736ecffdc67: Terminating worker thread
Mar 13 21:09:14 linbit1 kernel: [506545.510880] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:14 linbit1 kernel: [506545.510977] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Terminating sender thread
Mar 13 21:09:14 linbit1 kernel: [506545.511002] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Starting sender thread (from drbd_r_pvc-a79c [2450589])
Mar 13 21:09:14 linbit1 kernel: [506545.569753] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Connection closed
Mar 13 21:09:14 linbit1 kernel: [506545.569772] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:14 linbit1 kernel: [506545.569777] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Terminating receiver thread
Mar 13 21:09:14 linbit1 kernel: [506545.569818] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03 linbit2: Terminating sender thread
Mar 13 21:09:14 linbit1 kernel: [506545.589548] drbd pvc-a79cd28c-d0cf-4e71-b2cc-5f04a64aca03: Terminating worker thread
Mar 13 21:09:14 linbit1 kernel: [506545.955302] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:14 linbit1 kernel: [506545.955403] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Terminating sender thread
Mar 13 21:09:14 linbit1 kernel: [506545.955437] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Starting sender thread (from drbd_r_pvc-b7be [2449094])
Mar 13 21:09:15 linbit1 kernel: [506546.017779] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Connection closed
Mar 13 21:09:15 linbit1 kernel: [506546.017795] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:15 linbit1 kernel: [506546.017800] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Terminating receiver thread
Mar 13 21:09:15 linbit1 kernel: [506546.017913] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa linbit3: Terminating sender thread
Mar 13 21:09:15 linbit1 kernel: [506546.049540] drbd pvc-b7bece75-44cf-4036-b1c6-bfcafce9c1aa: Terminating worker thread
Mar 13 21:09:15 linbit1 kernel: [506546.073331] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:15 linbit1 kernel: [506546.073414] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Terminating sender thread
Mar 13 21:09:15 linbit1 kernel: [506546.073450] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Starting sender thread (from drbd_r_pvc-9428 [2451978])
Mar 13 21:09:15 linbit1 kernel: [506546.145759] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Connection closed
Mar 13 21:09:15 linbit1 kernel: [506546.145777] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:15 linbit1 kernel: [506546.145784] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Terminating receiver thread
Mar 13 21:09:15 linbit1 kernel: [506546.145895] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b linbit3: Terminating sender thread
Mar 13 21:09:15 linbit1 kernel: [506546.148744] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: sock was shut down by peer
Mar 13 21:09:15 linbit1 kernel: [506546.148755] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Connected -> BrokenPipe ) peer( Secondary -> Unknown )
Mar 13 21:09:15 linbit1 kernel: [506546.148898] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Terminating sender thread
Mar 13 21:09:15 linbit1 kernel: [506546.148943] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Starting sender thread (from drbd_r_pvc-c917 [2452006])
Mar 13 21:09:15 linbit1 kernel: [506546.153214] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( BrokenPipe -> Disconnecting )
Mar 13 21:09:15 linbit1 kernel: [506546.169544] drbd pvc-942882e3-6d43-415f-8f08-18cb1792333b: Terminating worker thread
Mar 13 21:09:15 linbit1 kernel: [506546.201682] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Connection closed
Mar 13 21:09:15 linbit1 kernel: [506546.201700] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:15 linbit1 kernel: [506546.201705] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Terminating receiver thread
Mar 13 21:09:15 linbit1 kernel: [506546.201862] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Terminating sender thread
Mar 13 21:09:15 linbit1 kernel: [506546.210529] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Starting sender thread (from drbdsetup [2489992])
Mar 13 21:09:15 linbit1 kernel: [506546.213469] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:09:15 linbit1 kernel: [506546.213518] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Starting receiver thread (from drbd_w_pvc-c917 [2450090])
Mar 13 21:09:15 linbit1 kernel: [506546.213629] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:09:15 linbit1 kernel: [506546.773365] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:09:15 linbit1 kernel: [506546.773484] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Terminating sender thread
Mar 13 21:09:15 linbit1 kernel: [506546.773508] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Starting sender thread (from drbd_r_pvc-c917 [2489996])
Mar 13 21:09:15 linbit1 kernel: [506546.773662] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Connection closed
Mar 13 21:09:15 linbit1 kernel: [506546.773674] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:15 linbit1 kernel: [506546.773679] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Terminating receiver thread
Mar 13 21:09:15 linbit1 kernel: [506546.773765] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit2: Terminating sender thread
Mar 13 21:09:16 linbit1 kernel: [506547.498215] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Preparing cluster-wide state change 2297948961 (0->-1 7680/1024)
Mar 13 21:09:16 linbit1 kernel: [506547.498568] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: State change 2297948961: primary_nodes=0, weak_nodes=0
Mar 13 21:09:16 linbit1 kernel: [506547.498574] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Committing cluster-wide state change 2297948961 (0ms)
Mar 13 21:09:16 linbit1 kernel: [506547.498606] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: disk( UpToDate -> Detaching )
Mar 13 21:09:16 linbit1 kernel: [506547.498705] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: disk( Detaching -> Diskless )
Mar 13 21:09:16 linbit1 kernel: [506547.499565] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: drbd_bm_resize called with capacity == 0
Mar 13 21:09:16 linbit1 kernel: [506547.785722] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:16 linbit1 kernel: [506547.797888] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: State change failed: State change was refused by peer node
Mar 13 21:09:16 linbit1 kernel: [506547.808574] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024: Failed: quorum( yes -> no )
Mar 13 21:09:16 linbit1 kernel: [506547.808580] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit3: Failed: pdsk( UpToDate -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:16 linbit1 kernel: [506547.808627] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: ASSERTION context->flags & CS_SERIALIZE FAILED in change_cluster_wide_state
Mar 13 21:09:16 linbit1 kernel: [506547.820787] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: State change failed: State change was refused by peer node
Mar 13 21:09:16 linbit1 kernel: [506547.831470] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8/0 drbd1024 linbit2: Failed: pdsk( Diskless -> DUnknown ) repl( Established -> Off )
Mar 13 21:09:17 linbit1 kernel: [506548.847430] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:17 linbit1 kernel: [506548.847531] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Terminating sender thread
Mar 13 21:09:17 linbit1 kernel: [506548.847569] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Starting sender thread (from drbd_r_pvc-c917 [2452008])
Mar 13 21:09:17 linbit1 kernel: [506548.913770] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Connection closed
Mar 13 21:09:17 linbit1 kernel: [506548.913786] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:17 linbit1 kernel: [506548.913792] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Terminating receiver thread
Mar 13 21:09:17 linbit1 kernel: [506548.913886] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22 linbit3: Terminating sender thread
Mar 13 21:09:17 linbit1 kernel: [506548.945533] drbd pvc-c917761f-d3bb-4a7c-a219-5e270d819a22: Terminating worker thread
Mar 13 21:09:18 linbit1 kernel: [506549.034201] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Preparing remote state change 2420266668
Mar 13 21:09:18 linbit1 kernel: [506549.034478] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Committing remote state change 2420266668 (primary_nodes=0)
Mar 13 21:09:18 linbit1 kernel: [506549.034608] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:09:18 linbit1 kernel: [506549.034718] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: P_STATE packet received for volume 0, which is not configured locally
Mar 13 21:09:19 linbit1 kernel: [506550.562077] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:19 linbit1 kernel: [506550.562182] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Terminating sender thread
Mar 13 21:09:19 linbit1 kernel: [506550.562214] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Starting sender thread (from drbd_r_pvc-8385 [2451379])
Mar 13 21:09:19 linbit1 kernel: [506550.617581] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Connection closed
Mar 13 21:09:19 linbit1 kernel: [506550.617601] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:19 linbit1 kernel: [506550.617607] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Terminating receiver thread
Mar 13 21:09:19 linbit1 kernel: [506550.617775] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Terminating sender thread
Mar 13 21:09:19 linbit1 kernel: [506550.626121] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Starting sender thread (from drbdsetup [2490114])
Mar 13 21:09:19 linbit1 kernel: [506550.628429] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( StandAlone -> Unconnected )
Mar 13 21:09:19 linbit1 kernel: [506550.628462] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Starting receiver thread (from drbd_w_pvc-8385 [2449920])
Mar 13 21:09:19 linbit1 kernel: [506550.628565] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( Unconnected -> Connecting )
Mar 13 21:09:20 linbit1 kernel: [506551.905083] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( Connecting -> Disconnecting )
Mar 13 21:09:20 linbit1 kernel: [506551.905278] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Terminating sender thread
Mar 13 21:09:20 linbit1 kernel: [506551.905294] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Starting sender thread (from drbd_r_pvc-8385 [2490118])
Mar 13 21:09:20 linbit1 kernel: [506551.905603] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Connection closed
Mar 13 21:09:20 linbit1 kernel: [506551.905615] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: conn( Disconnecting -> StandAlone )
Mar 13 21:09:20 linbit1 kernel: [506551.905622] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Terminating receiver thread
Mar 13 21:09:20 linbit1 kernel: [506551.905704] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit2: Terminating sender thread
Mar 13 21:09:21 linbit1 kernel: [506552.335974] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: conn( Connected -> Disconnecting ) peer( Secondary -> Unknown )
Mar 13 21:09:21 linbit1 kernel: [506552.336077] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Terminating sender thread
Mar 13 21:09:21 linbit1 kernel: [506552.336120] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Starting sender thread (from drbd_r_pvc-8385 [2451381])
Mar 13 21:09:21 linbit1 kernel: [506552.389735] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Connection closed
Mar 13 21:09:21 linbit1 kernel: [506552.389753] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: conn( Disconnecting -> StandAlone )
Mar 13 21:09:21 linbit1 kernel: [506552.389759] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Terminating receiver thread
Mar 13 21:09:21 linbit1 kernel: [506552.389878] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8 linbit3: Terminating sender thread
Mar 13 21:09:21 linbit1 kernel: [506552.405512] drbd pvc-8385374b-abfd-40fc-ba54-1047ad8dd1a8: Terminating worker thread
Mar 13 21:10:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:10:40.334580    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:10:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:10:40.345325    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:15:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:15:40.335014    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:15:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:15:40.346187    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:17:01 linbit1 CRON[2495153]: (root) CMD (   cd / && run-parts --report /etc/cron.hourly)
Mar 13 21:17:10 linbit1 systemd[1]: Starting Daily apt download activities...
Mar 13 21:17:40 linbit1 systemd-networkd-wait-online[2495270]: Timeout occurred while waiting for network connectivity.
Mar 13 21:17:40 linbit1 apt-helper[2495267]: E: Sub-process /lib/systemd/systemd-networkd-wait-online returned an error code (1)
Mar 13 21:17:41 linbit1 systemd[1]: apt-daily.service: Deactivated successfully.
Mar 13 21:17:41 linbit1 systemd[1]: Finished Daily apt download activities.
Mar 13 21:20:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:20:40.334838    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:20:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:20:40.346263    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:25:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:25:40.334423    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:25:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:25:40.345453    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:30:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:30:40.334902    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:30:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:30:40.345074    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:33:32 linbit1 systemd[1]: run-containerd-runc-k8s.io-bcc759904c6702a8921b18aa4db1b7effdc0ebbe331b384ad882e8a59a3b73bf-runc.x0G3qJ.mount: Deactivated successfully.
Mar 13 21:35:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:35:40.335337    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:35:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:35:40.344971    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:40:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:40:40.334513    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:40:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:40:40.344289    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:45:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:45:40.334649    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:45:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:45:40.344841    2448 machine.go:65] Cannot read vendor id correctly, set empty.
Mar 13 21:50:40 linbit1 microk8s.daemon-kubelite[2448]: E0313 21:50:40.334745    2448 info.go:99] Failed to get disk map: open /sys/block/nvme0c0n1/dev: no such file or directory
Mar 13 21:50:40 linbit1 microk8s.daemon-kubelite[2448]: W0313 21:50:40.345442    2448 machine.go:65] Cannot read vendor id correctly, set empty.
